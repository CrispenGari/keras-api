### `ReLU` - Rectified Linear Unit
- Applies the rectified linear unit activation function.
- With default values, this returns the standard ReLU activation: ``max(x, 0)``, the element-wise maximum of 0 and the input tensor.
- Basically this activation introduce non-linearity in the network by making `0` to numbers inputs that are less than `0` and return the same value when the input is `+ve`.

#### Formular

<p align="center">
    <img src=""/>
</p>

