### `ReLU` - Rectified Linear Unit
- Applies the rectified linear unit activation function.
- With default values, this returns the standard ReLU activation: ``max(x, 0)``, the element-wise maximum of 0 and the input tensor.
- Basically this activation introduce non-linearity in the network by making `0` to numbers inputs that are less than `0` and return the same value when the input is `+ve`.

#### Formular

<p align="center">
    <img src="https://github.com/CrispenGari/Keras-API/blob/main/02_Activation_Functions/01_ReLU/1_ZD5kma5J-6UabfEwERv_dQ.png"/>
</p>

