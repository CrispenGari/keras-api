{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-douglas",
   "metadata": {},
   "source": [
    "### F1-score\n",
    "* The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if either the precision or the recall is zero.\n",
    "> Combines precision and recall into one metric. 1 is best, 0 is worst.\n",
    "\n",
    "```\n",
    "sklearn.metrics.f1_score()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medium-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1., 0., 1, 1, 0, 0, 1])\n",
    "y_pred = np.array([1., 1., 1., 0., 0. ,1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-devil",
   "metadata": {},
   "source": [
    "### Using `scikit-learn` to find the F1-score\n",
    "* Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "    ```\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-thanks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([0., 1])\n",
    "score = f1_score(y_true, y_pred, labels= labels,zero_division = 1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-venture",
   "metadata": {},
   "source": [
    "> `50%` f1_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-expression",
   "metadata": {},
   "source": [
    "### Creating our own `f1_score` Function\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://www.gstatic.com/education/formulas2/355397047/en/f1_score.svg\"/></p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "## taken from stackoverflow\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP, FP, TN, FN  = 0, 0, 0, 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return (TP, FP, TN, FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affecting-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    TP, FP, TN, FN = perf_measure(y_true, y_pred)\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    TP, FP, TN, FN = perf_measure(y_true, y_pred)\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def f1_score_(y_true, y_pred):\n",
    "    P = precision(y_true, y_pred)\n",
    "    R = recall(y_true, y_pred)\n",
    "    return 2 * P * R / (P + R)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "> We got the same `50%` f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
