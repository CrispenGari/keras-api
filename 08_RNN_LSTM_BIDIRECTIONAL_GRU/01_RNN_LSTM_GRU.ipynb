{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-visiting",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks `RNN`\n",
    "Recurrent Neural Networks (RNN) are designed to work with sequential data. Sequential data(can be time-series) can be in form of text, audio, video etc.\n",
    "RNN uses the previous information in the sequence to produce the current output. Let's consider the following illustration suppose we have the sentence:\n",
    "```\n",
    "\"I love deep learning.\"\n",
    "```\n",
    "* At `t0` the input will be `I`\n",
    "* At `t1` the input will be `I love`\n",
    "* At `t2` the input will be `I love deep`\n",
    "* At `t3` the input will be `I love deep learning.`\n",
    "> At the end the `NN` will have all the infomation about the last step.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/480/1*gEA0-LTj05xtESA5XoBxPw.gif\"/> </p>\n",
    "\n",
    "**Note**: In ``RNN`` weights and bias for all the nodes in the layer are same. The default activation function of `RNN` is `tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-priority",
   "metadata": {},
   "source": [
    "> Let's have a look at a `RNN` unit.\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/332/0*eRJCRsikdGGu8ffA.png\"/></p>\n",
    "\n",
    "> ``RNN’s`` face **short-term memory** problem. It is caused due to ``vanishing gradient`` problem. As ``RNN`` processes more steps it suffers from vanishing gradient more than other neural network architectures.\n",
    "\n",
    "### The vaishing gradient problem.\n",
    "In ``RNN`` to train the network you backpropagate through time, at each step the gradient is calculated. The gradient is used to update weights in the network. If the effect of the previous layer on the current layer is small then the gradient value will be small and vice-versa. If the gradient of the previous layer is smaller, then the gradient of the current layer will be even smaller. This makes the gradients exponentially shrink down as we backpropagate. Smaller gradient means it will not affect the weight updation. Due to this, the network does not learn the effect of earlier inputs. Thus, causing the ``short-term`` memory problem.\n",
    "\n",
    "To solve the problem of Vanishing gradient we use two specialised versions of RNN.\n",
    "\n",
    "#### Gated Recurrent Units `GRU`\n",
    "In this `RNN` there are two gates namely:\n",
    "1. update gate\n",
    "2. reset gate\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/700/1*RiOzdOVaaeKrUotY7-1a2A.png\"/></p>\n",
    "\n",
    "Gates are nothing but neural networks, each gate has its own weights and biases(but don’t forget that weights and bias for all nodes in one layer are same).\n",
    "\n",
    "* **Update gate** -\n",
    "Update gate decides if the cell state should be updated with the candidate state(current activation value)or not.\n",
    "* **Reset gate** -\n",
    "The reset gate is used to decide whether the previous cell state is important or not. Sometimes the reset gate is not used in simple GRU.\n",
    "* **Candidate cell** -\n",
    "It is just simply the same as the hidden state(activation) of RNN.\n",
    "**Final cell state** -\n",
    "The final cell state is dependent on the update gate. It may or may not be updated with candidate state. Remove some content from last cell state, and write some new cell content.\n",
    "\n",
    "In GRU,\n",
    "\n",
    "\n",
    "* If reset close to 0, ignore previous hidden state (allows the model to drop information that is irrelevant in the future).\n",
    "* If gamma(update gate) close to 1, then we can copy information in that unit through many steps!\n",
    "* Gamma Controls how much of past state should matter now.\n",
    "\n",
    "#### Long Short-Term Memory `LSTM`\n",
    "LSTMs are pretty much similar to GRU’s, they are also intended to solve the vanishing gradient problem. It has the following gates\n",
    "\n",
    "1. update gate\n",
    "2. reset gate\n",
    "3. forget gate\n",
    "4. output gate\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/700/1*lSDKRennQMpJFL4xxJHloQ.png\"/></p>\n",
    "\n",
    "> All 3 gates(input gate, output gate, forget gate) use sigmoid as activation function so all gate values are between 0 and 1.\n",
    "\n",
    "* **Forget gate** -\n",
    "It controls what is kept vs forgotten, from previous cell state. In laymen terms, it will decide how much information from the previous state should be kept and forget remaining.\n",
    "* **Output gate** -\n",
    "It controls which parts of the cell are output to the hidden state. It will determine what the next hidden state will be.\n",
    "\n",
    "### Ref\n",
    "\n",
    "[Article](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-disorder",
   "metadata": {},
   "source": [
    "### Practical Example\n",
    "We are going to use `RNN` with the `airline-passanger` dataset which we will load from a `csv` file. The dataset provides a record of the number of people travelling in US airlines in a particular month. we will only use the `Passengers` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-relationship",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adult-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-victor",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "packed-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Month  Passengers\n",
       "0  1949-01         112\n",
       "1  1949-02         118\n",
       "2  1949-03         132\n",
       "3  1949-04         129\n",
       "4  1949-05         121"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"airline-passengers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occupied-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "champion-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dataset = scaler.fit_transform(data[\"Passengers\"].values.reshape(-1, 1))\n",
    "train_size = int(len(dataset) * 0.75)\n",
    "test_size = len(dataset) - train_size\n",
    "train=dataset[:train_size,:]\n",
    "test=dataset[train_size:142,:]\n",
    "def getdata(data,lookback):\n",
    "    X,Y=[],[]\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        X.append(data[i:i+lookback,0])\n",
    "        Y.append(data[i+lookback,0])\n",
    "    return np.array(X),np.array(Y).reshape(-1,1)\n",
    "lookback=1\n",
    "\n",
    "X_train, y_train=getdata(train, lookback)\n",
    "X_test,y_test=getdata(test,lookback)\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-cooler",
   "metadata": {},
   "source": [
    "### ``SimpleRNN``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ranging-sucking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 1, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prescription-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 153ms/step - loss: 0.5928 - mse: 0.5928 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6040 - mse: 0.6040 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5992 - mse: 0.5992 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5815 - mse: 0.5815 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6060 - mse: 0.6060 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6001 - mse: 0.6001 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5900 - mse: 0.5900 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6034 - mse: 0.6034 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6059 - mse: 0.6059 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287adafc670>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = keras.Sequential([\n",
    "    keras.layers.SimpleRNN(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_1\")\n",
    "\n",
    "model_1.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_1.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-spouse",
   "metadata": {},
   "source": [
    "> `LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alternative-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 318ms/step - loss: 0.5990 - mse: 0.5990 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5872 - mse: 0.5872 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6126 - mse: 0.6126 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5921 - mse: 0.5921 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5943 - mse: 0.5943 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287b3551b20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = keras.Sequential([\n",
    "    keras.layers.LSTM(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_2\")\n",
    "\n",
    "model_2.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_2.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pursuant-marker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 6s 290ms/step - loss: 0.6069 - mse: 0.6069 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6142 - mse: 0.6142 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5986 - mse: 0.5986 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5898 - mse: 0.5898 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6033 - mse: 0.6033 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6130 - mse: 0.6130 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5835 - mse: 0.5835 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5692 - mse: 0.5692 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5967 - mse: 0.5967 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287bab97c40>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = keras.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_3\")\n",
    "\n",
    "model_3.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_3.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-maximum",
   "metadata": {},
   "source": [
    "> This Notebook is more focusing on the theory and how to use `RNN` not about training or testing `metrics`\n",
    "\n",
    "### Nesting `RNN's`\n",
    "To nest `RNN` we should just pass `return_sequences =True` to `RNN` layers for example:\n",
    "\n",
    "> GRU\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.GRU(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "> RNN / SimpleRNN\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.SimpleRNN(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.SimpleRNN(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "> LSTM\n",
    "\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.LSTM(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "> We can also have different `RNN` layers nested for example.\n",
    "\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.LTSM(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-boundary",
   "metadata": {},
   "source": [
    "### Bidirectional `RNN`\n",
    "Bidirectional layer is a layer that is used to wrap such as ``keras.layers.LSTM`` or ``keras.layers.GRU.`` It can also be a `keras.layes.Layer` tha meet [these](https://keras.io/api/layers/recurrent_layers/bidirectional/) condeitions.\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Bidirectional(\n",
    "    layer, merge_mode=\"concat\", weights=None, backward_layer=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "* [Docs](https://keras.io/api/layers/recurrent_layers/bidirectional/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-center",
   "metadata": {},
   "source": [
    "> Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "soviet-adelaide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 5, 20)             1680      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 105       \n",
      "=================================================================\n",
      "Total params: 4,265\n",
      "Trainable params: 4,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(10, return_sequences=True), input_shape=(5, 10)\n",
    "    ),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(10)),\n",
    "    keras.layers.Dense(5)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-argument",
   "metadata": {},
   "source": [
    "> Example With custom ``backward`` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rational-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 5, 20)             1680      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5, 5)              105       \n",
      "=================================================================\n",
      "Total params: 1,785\n",
      "Trainable params: 1,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "forward_layer = keras.layers.LSTM(10, return_sequences=True)\n",
    "backward_layer = keras.layers.LSTM(10, activation='relu', return_sequences=True,\n",
    "                       go_backwards=True)\n",
    "\n",
    "model = keras.Sequential([\n",
    "     keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer,\n",
    "                         input_shape=(5, 10)),\n",
    "      keras.layers.Dense(5)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-climb",
   "metadata": {},
   "source": [
    " > **Practical Example**: ``imdb`` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "static-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "(X_train, y_train), (X_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-margin",
   "metadata": {},
   "source": [
    "> Building the model without `Bidirectional` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "saving-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/196 [..............................] - ETA: 8:25 - loss: 0.6929 - accuracy: 0.5179"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e43e309d2ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel_4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_layer = keras.layers.Input(shape=(None, ), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(max_features, 128)(input_layer)\n",
    "x = keras.layers.LSTM(256, return_sequences=True)(x)\n",
    "x = keras.layers.LSTM(64)(x)\n",
    "output_layer = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = keras.Model(inputs=input_layer, outputs=output_layer, name=\"model_4\")\n",
    "model_4.compile(\n",
    "\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_4.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val),verbose=1, batch_size=128 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-insertion",
   "metadata": {},
   "source": [
    "> Building the model with `Bidirectional` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape=(None, ), dtype=\"int32\")\n",
    "x = keras.layers.Embedding(max_features, 128)(input_layer)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "output_layer = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = keras.Model(inputs=input_layer, outputs=output_layer, name=\"model_4\")\n",
    "model_4.compile(\n",
    "\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model_4.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val),verbose=1, batch_size=64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "> Conclusion: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
