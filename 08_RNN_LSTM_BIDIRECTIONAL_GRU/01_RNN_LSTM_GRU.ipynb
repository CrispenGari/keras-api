{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-visiting",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks `RNN`\n",
    "Recurrent Neural Networks (RNN) are designed to work with sequential data. Sequential data(can be time-series) can be in form of text, audio, video etc.\n",
    "RNN uses the previous information in the sequence to produce the current output. Let's consider the following illustration suppose we have the sentence:\n",
    "```\n",
    "\"I love deep learning.\"\n",
    "```\n",
    "* At `t0` the input will be `I`\n",
    "* At `t1` the input will be `I love`\n",
    "* At `t2` the input will be `I love deep`\n",
    "* At `t3` the input will be `I love deep learning.`\n",
    "> At the end the `NN` will have all the infomation about the last step.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/480/1*gEA0-LTj05xtESA5XoBxPw.gif\"/> </p>\n",
    "\n",
    "**Note**: In ``RNN`` weights and bias for all the nodes in the layer are same. The default activation function of `RNN` is `tanh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-priority",
   "metadata": {},
   "source": [
    "> Let's have a look at a `RNN` unit.\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/332/0*eRJCRsikdGGu8ffA.png\"/></p>\n",
    "\n",
    "> ``RNN’s`` face **short-term memory** problem. It is caused due to ``vanishing gradient`` problem. As ``RNN`` processes more steps it suffers from vanishing gradient more than other neural network architectures.\n",
    "\n",
    "### The vaishing gradient problem.\n",
    "In ``RNN`` to train the network you backpropagate through time, at each step the gradient is calculated. The gradient is used to update weights in the network. If the effect of the previous layer on the current layer is small then the gradient value will be small and vice-versa. If the gradient of the previous layer is smaller, then the gradient of the current layer will be even smaller. This makes the gradients exponentially shrink down as we backpropagate. Smaller gradient means it will not affect the weight updation. Due to this, the network does not learn the effect of earlier inputs. Thus, causing the ``short-term`` memory problem.\n",
    "\n",
    "To solve the problem of Vanishing gradient we use two specialised versions of RNN.\n",
    "\n",
    "#### Gated Recurrent Units `GRU`\n",
    "In this `RNN` there are two gates namely:\n",
    "1. update gate\n",
    "2. reset gate\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/700/1*RiOzdOVaaeKrUotY7-1a2A.png\"/></p>\n",
    "\n",
    "Gates are nothing but neural networks, each gate has its own weights and biases(but don’t forget that weights and bias for all nodes in one layer are same).\n",
    "\n",
    "* **Update gate** -\n",
    "Update gate decides if the cell state should be updated with the candidate state(current activation value)or not.\n",
    "* **Reset gate** -\n",
    "The reset gate is used to decide whether the previous cell state is important or not. Sometimes the reset gate is not used in simple GRU.\n",
    "* **Candidate cell** -\n",
    "It is just simply the same as the hidden state(activation) of RNN.\n",
    "**Final cell state** -\n",
    "The final cell state is dependent on the update gate. It may or may not be updated with candidate state. Remove some content from last cell state, and write some new cell content.\n",
    "\n",
    "In GRU,\n",
    "\n",
    "\n",
    "* If reset close to 0, ignore previous hidden state (allows the model to drop information that is irrelevant in the future).\n",
    "* If gamma(update gate) close to 1, then we can copy information in that unit through many steps!\n",
    "* Gamma Controls how much of past state should matter now.\n",
    "\n",
    "#### Long Short-Term Memory `LSTM`\n",
    "LSTMs are pretty much similar to GRU’s, they are also intended to solve the vanishing gradient problem. It has the following gates\n",
    "\n",
    "1. update gate\n",
    "2. reset gate\n",
    "3. forget gate\n",
    "4. output gate\n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/700/1*lSDKRennQMpJFL4xxJHloQ.png\"/></p>\n",
    "\n",
    "> All 3 gates(input gate, output gate, forget gate) use sigmoid as activation function so all gate values are between 0 and 1.\n",
    "\n",
    "* **Forget gate** -\n",
    "It controls what is kept vs forgotten, from previous cell state. In laymen terms, it will decide how much information from the previous state should be kept and forget remaining.\n",
    "* **Output gate** -\n",
    "It controls which parts of the cell are output to the hidden state. It will determine what the next hidden state will be.\n",
    "\n",
    "### Ref\n",
    "\n",
    "[Article](https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-disorder",
   "metadata": {},
   "source": [
    "### Practical Example\n",
    "We are going to use `RNN` with the `airline-passanger` dataset which we will load from a `csv` file. The dataset provides a record of the number of people travelling in US airlines in a particular month. we will only use the `Passengers` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-relationship",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adult-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-victor",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "packed-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Month  Passengers\n",
       "0  1949-01         112\n",
       "1  1949-02         118\n",
       "2  1949-03         132\n",
       "3  1949-04         129\n",
       "4  1949-05         121"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"airline-passengers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occupied-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "champion-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "dataset = scaler.fit_transform(data[\"Passengers\"].values.reshape(-1, 1))\n",
    "train_size = int(len(dataset) * 0.75)\n",
    "test_size = len(dataset) - train_size\n",
    "train=dataset[:train_size,:]\n",
    "test=dataset[train_size:142,:]\n",
    "def getdata(data,lookback):\n",
    "    X,Y=[],[]\n",
    "    for i in range(len(data)-lookback-1):\n",
    "        X.append(data[i:i+lookback,0])\n",
    "        Y.append(data[i+lookback,0])\n",
    "    return np.array(X),np.array(Y).reshape(-1,1)\n",
    "lookback=1\n",
    "\n",
    "X_train, y_train=getdata(train, lookback)\n",
    "X_test,y_test=getdata(test,lookback)\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-cooler",
   "metadata": {},
   "source": [
    "### ``SimpleRNN``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ranging-sucking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 1, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prescription-belly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 2s 153ms/step - loss: 0.5928 - mse: 0.5928 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6040 - mse: 0.6040 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5992 - mse: 0.5992 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.5815 - mse: 0.5815 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6060 - mse: 0.6060 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6001 - mse: 0.6001 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5900 - mse: 0.5900 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6034 - mse: 0.6034 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6059 - mse: 0.6059 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6119 - mse: 0.6119 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287adafc670>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = keras.Sequential([\n",
    "    keras.layers.SimpleRNN(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_1\")\n",
    "\n",
    "model_1.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_1.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-spouse",
   "metadata": {},
   "source": [
    "> `LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alternative-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 4s 318ms/step - loss: 0.5990 - mse: 0.5990 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.6028 - mse: 0.6028 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5872 - mse: 0.5872 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6126 - mse: 0.6126 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5921 - mse: 0.5921 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5836 - mse: 0.5836 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5894 - mse: 0.5894 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5768 - mse: 0.5768 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.5943 - mse: 0.5943 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5999 - mse: 0.5999 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287b3551b20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = keras.Sequential([\n",
    "    keras.layers.LSTM(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_2\")\n",
    "\n",
    "model_2.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_2.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pursuant-marker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 6s 290ms/step - loss: 0.6069 - mse: 0.6069 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6142 - mse: 0.6142 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5986 - mse: 0.5986 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5898 - mse: 0.5898 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.6033 - mse: 0.6033 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.6130 - mse: 0.6130 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5835 - mse: 0.5835 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5692 - mse: 0.5692 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5967 - mse: 0.5967 - val_loss: 0.1599 - val_mse: 0.1599\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.5947 - mse: 0.5947 - val_loss: 0.1599 - val_mse: 0.1599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x287bab97c40>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = keras.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1)),\n",
    "    keras.layers.Dense(1, activation='softmax')\n",
    "], name=\"model_3\")\n",
    "\n",
    "model_3.compile(\n",
    "    loss='mean_squared_error',optimizer='adam',\n",
    "    metrics=['mse']\n",
    ")\n",
    "model_3.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-maximum",
   "metadata": {},
   "source": [
    "> This Notebook is more focusing on the theory and how to use `RNN` not about training or testing `metrics`\n",
    "\n",
    "### Nesting `RNN's`\n",
    "To nest `RNN` we should just pass `return_sequences =True` to `RNN` layers for example:\n",
    "\n",
    "> GRU\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.GRU(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "> RNN / SimpleRNN\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.SimpleRNN(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.SimpleRNN(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "> LSTM\n",
    "\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.LSTM(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.LSTM(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "> We can also have different `RNN` layers nested for example.\n",
    "\n",
    "```python\n",
    "keras.layers.Sequential([\n",
    "    keras.layers.GRU(64, input_shape=(1, 1), return_sequences=True),\n",
    "    keras.layers.LTSM(64, return_sequences=True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-miracle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
