{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "essential-venue",
   "metadata": {},
   "source": [
    "### DCGAN's (Deep Convlutional Generative Adversarial Networks).\n",
    "\n",
    "Generative Adversarial Networks `(GANs)` are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A **generator** (\"the artist\") learns to create images that look real, while a **discriminator** (\"the art critic\") learns to tell real images apart from fakes.\n",
    "\n",
    "\n",
    "````\n",
    "\n",
    "       [-------------]                  [--------------]\n",
    "   --> [Discriminator] -> (fake/real)   [  Generator   ]-|\n",
    "   |   [_____________]                  [______________] |\n",
    "   |_____________________________________________________|\n",
    "                       (Generated image)(fake)\n",
    "\n",
    "````\n",
    "1. So the generator its purpose is to genarate images from random noise and pass to he discriminator to detemine if the image is real or fake.\n",
    "2. The discriminator then detemines if the image is for sure real or fake.\n",
    "3. So we have 2 agents in an environment (discriminator) and (generator)\n",
    "4. Their goal is to maximize their rewards. **But how?**.\n",
    "5. A generator gets it's reward when a discriminator fake fake from real images.\n",
    "6. A discrimnator then get's it's reward if it can not be fooled by the generator.\n",
    "7. **As we train more and more the generator model will become better and better so that it will be able to produce fake images that a discriminator model can not discriminate from real images.**\n",
    "\n",
    "### Let's implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image , image_dataset_from_directory\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-daily",
   "metadata": {},
   "source": [
    "### Renaming all the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, image in enumerate(os.listdir('./data/images/')):\n",
    "#     os.rename(f'data/images/{image}', f'data/images/img_{i}.jpg')\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-lyric",
   "metadata": {},
   "source": [
    "### Folder Structures\n",
    "\n",
    "```\n",
    "data\n",
    "    images\n",
    "```\n",
    "\n",
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = image_dataset_from_directory(\n",
    "    './data',\n",
    "    label_mode = None,# because unsupervised\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    ").map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-conspiracy",
   "metadata": {},
   "source": [
    "### Displaying some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-munich",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ds:\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0], cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-auction",
   "metadata": {},
   "source": [
    "### Create the discriminator.\n",
    "This model will maps the (`image_width` x `image_height`) in our case (`64` x `64`) to a binary class (**fake** or **real**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = keras.Sequential([\n",
    "    keras.layers.Input(shape=(64, 64, 3)),\n",
    "    keras.layers.Conv2D(64, kernel_size=(4, 4), padding=\"same\", strides=(2, 2)),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Conv2D(128, kernel_size=(4, 4), padding=\"same\", strides=(2, 2)),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Conv2D(128, kernel_size=(4, 4), padding=\"same\", strides=(2, 2)),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(.2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "], name=\"discriminator\")\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-narrative",
   "metadata": {},
   "source": [
    "### Create the generator.\n",
    "This model is the mirror of the `discriminator` model. Replacing `Conv2D` layers with `Conv2DTranspose` layers.\n",
    "\n",
    "* `Conv2DTranspose` - is just an inverse of `Conv2D`.\n",
    "* Our goal is to create an image from a vector to an image of shape `(64, 64, 3)`.\n",
    "* We are going to have `128` latent_dim.\n",
    "* `\"latent_dim\"` is the number of nodes used as input of the generator.\n",
    "\n",
    "#### Implementation\n",
    "1. We will start with the image of shape `8` x `8`.\n",
    "2. Add a reshape layer to get shape (8 x 8 x latent_dim)\n",
    "3. Our image now is in a shape that we can put in a `Conv` layer.\n",
    "4. We will then add a `Conv2DTranspose` layer with `strides =2`. Why? Remember in a normal Conv2D if we have an image of shape (16, 16)  and strides are (2, 2) the image will be reduced to (8, 8) but in a `Conv2DTranspose` the image will be increased to (32, 32) shape, which is our goal. So we want to add these `Conv2DTranspose` layers until we reach (64, 64) image shape.\n",
    "\n",
    "5. We will then add `Conv2D` layer with 3 input features, this is because we want to output 3 channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "generator = keras.Sequential([\n",
    "    keras.layers.Input(shape=(latent_dim, )),\n",
    "    keras.layers.Dense(8 * 8 * latent_dim, activation=\"relu\"),\n",
    "    keras.layers.Reshape((8, 8, latent_dim)),\n",
    "    keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    keras.layers.LeakyReLU(0.2),\n",
    "    keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    keras.layers.LeakyReLU(0.2),\n",
    "    keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
    "    keras.layers.LeakyReLU(0.2),\n",
    "    keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
    "], name=\"generator\")\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-extreme",
   "metadata": {},
   "source": [
    "### Override `train_step`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "    # .compile()\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metric(self):\n",
    "        return [\n",
    "            self.d_loss_metric,\n",
    "            self.g_loss_metric\n",
    "        ]\n",
    "    \n",
    "    # .fit()\n",
    "    def train_step(self, real_images):\n",
    "        # Sample random points in the latent space\n",
    "        \"\"\"\n",
    "        We need to create latent_vectors with shape (batch_size_of_real_images, latent_dim)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate fake images of size (batch_size)\n",
    "        \"\"\"\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        \n",
    "        \"\"\"\n",
    "        Combine images, generated and real\n",
    "        \"\"\"\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        \"\"\"\n",
    "        Assemble labels discriminating real from fake images\n",
    "        \"\"\"\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        \"\"\"\n",
    "        Add random noise to the labels - important trick!\n",
    "        \"\"\"\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the discriminator.\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "            \n",
    "        gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(gradients, self.discriminator.trainable_weights )\n",
    "        )\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "        \"\"\"\n",
    "        Train the generator (note that we should *not* update the weights\n",
    "        of the discriminator)!\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-layer",
   "metadata": {},
   "source": [
    "### Create a callback that periodically saves `generated` images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('generated'):\n",
    "    os.mkdir('generated')\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(\"generated/generated_img_%03d_%d.png\" % (epoch, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-belgium",
   "metadata": {},
   "source": [
    "### Train the end-to-end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  # In practice, use ~100 epochs\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
    ")\n",
    "gan.fit(\n",
    "    ds, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-irish",
   "metadata": {},
   "source": [
    "> If we train the model above for longer on a `GPU` we will be able to achive reasonable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-worry",
   "metadata": {},
   "source": [
    "### Reference.\n",
    "\n",
    "* [Keras DCGANs](https://keras.io/examples/generative/dcgan_overriding_train_step/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-instruction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
