{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "anonymous-awareness",
   "metadata": {},
   "source": [
    "### DCGANs `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "isolated-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from packaging.version import parse as parse_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-brand",
   "metadata": {},
   "source": [
    "### Loading the `mnist` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-posting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\crisp\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322b1f0960b44b4b01bb6e6b3f7ae42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe0172d26eb4765a8409d7f7d773ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189b1e25a0b54a0292a6aac8ada511b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-train.tfrecord...:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-test.tfrecord...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\crisp\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(ds_train, ds_test_), ds_info = tfds.load('mnist', \n",
    "                              split=['train', 'test'], \n",
    "                              shuffle_files=True,\n",
    "                              as_supervised=True,\n",
    "                              with_info=True)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.\n",
    "    return image, image\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.cache() # put dataset into memory\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "\n",
    "ds_test = ds_test_.map(preprocess).batch(batch_size).cache().prefetch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return label for testing\n",
    "def preprocess_with_label(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.math.round(image/255.)\n",
    "    return image, label\n",
    "\n",
    "ds_test_label = ds_test_.map(preprocess_with_label).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(z_dim):\n",
    "    inputs  = layers.Input(shape=[28,28,1])\n",
    "    \n",
    "    x = inputs    \n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(z_dim)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=out, name='encoder')\n",
    "\n",
    "def Decoder(z_dim):\n",
    "    inputs  = layers.Input(shape=[z_dim])\n",
    "    x = inputs    \n",
    "    x = Dense(7*7*64, activation='relu')(x)\n",
    "    x = Reshape((7,7,64))(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D((2,2))(x)    \n",
    "\n",
    "    out = Conv2D(filters=1, kernel_size=(3,3), strides=1, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    #return out          \n",
    "    return Model(inputs=inputs, outputs=out, name='decoder')\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, z_dim):\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "        \n",
    "        model_input = self.encoder.input\n",
    "        model_output = self.decoder(self.encoder.output)\n",
    "        self.model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "domestic-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(z_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-turtle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 181s 704ms/step - loss: 0.0930 - val_loss: 0.0406\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04057, saving model to ./models\\autoencoder.h5\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 154s 656ms/step - loss: 0.0363 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04057 to 0.02905, saving model to ./models\\autoencoder.h5\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 153s 651ms/step - loss: 0.0286 - val_loss: 0.0248\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.02905 to 0.02481, saving model to ./models\\autoencoder.h5\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 158s 673ms/step - loss: 0.0253 - val_loss: 0.0237\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02481 to 0.02366, saving model to ./models\\autoencoder.h5\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 151s 643ms/step - loss: 0.0234 - val_loss: 0.0230\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.02366 to 0.02303, saving model to ./models\\autoencoder.h5\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 152s 645ms/step - loss: 0.0222 - val_loss: 0.0208\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02303 to 0.02082, saving model to ./models\\autoencoder.h5\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 151s 643ms/step - loss: 0.0212 - val_loss: 0.0204\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02082 to 0.02042, saving model to ./models\\autoencoder.h5\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 150s 639ms/step - loss: 0.0204 - val_loss: 0.0198\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02042 to 0.01977, saving model to ./models\\autoencoder.h5\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 148s 630ms/step - loss: 0.0198 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01977 to 0.01938, saving model to ./models\\autoencoder.h5\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 172s 731ms/step - loss: 0.0193 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01938 to 0.01852, saving model to ./models\\autoencoder.h5\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 171s 728ms/step - loss: 0.0189 - val_loss: 0.0183\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01852 to 0.01828, saving model to ./models\\autoencoder.h5\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 154s 654ms/step - loss: 0.0185 - val_loss: 0.0185\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01828\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 154s 654ms/step - loss: 0.0181 - val_loss: 0.0182\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.01828 to 0.01823, saving model to ./models\\autoencoder.h5\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 154s 653ms/step - loss: 0.0179 - val_loss: 0.0177\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01823 to 0.01775, saving model to ./models\\autoencoder.h5\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 160s 683ms/step - loss: 0.0175 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01775 to 0.01714, saving model to ./models\\autoencoder.h5\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 153s 650ms/step - loss: 0.0173 - val_loss: 0.0169\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01714 to 0.01694, saving model to ./models\\autoencoder.h5\n",
      "Epoch 17/100\n",
      " 75/235 [========>.....................] - ETA: 1:49 - loss: 0.0169"
     ]
    }
   ],
   "source": [
    "model_path = \"./models/autoencoder.h5\"\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, \n",
    "                             monitor= \"val_loss\", \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode= \"auto\", \n",
    "                             save_weights_only = False)\n",
    "\n",
    "early = EarlyStopping(monitor= \"val_loss\", \n",
    "                      mode= \"auto\", \n",
    "                      patience = 5)\n",
    "\n",
    "callbacks_list = [checkpoint, early]\n",
    "\n",
    "autoencoder.model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=3e-4))\n",
    "    #metrics=[tf.keras.losses.BinaryCrossentropy()])\n",
    "autoencoder.model.fit(ds_train, validation_data=ds_test,\n",
    "                epochs = 100, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(ds_test))\n",
    "autoencoder.model = load_model(model_path)\n",
    "outputs = autoencoder.model.predict(images)\n",
    "\n",
    "# Display\n",
    "grid_col = 10\n",
    "grid_row = 2\n",
    "\n",
    "f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*1.1, grid_row))\n",
    "\n",
    "i = 0\n",
    "for row in range(0, grid_row, 2):\n",
    "    for col in range(grid_col):\n",
    "        axarr[row,col].imshow(images[i,:,:,0], cmap='gray')\n",
    "        axarr[row,col].axis('off')\n",
    "        axarr[row+1,col].imshow(outputs[i,:,:,0], cmap='gray')\n",
    "        axarr[row+1,col].axis('off')        \n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-angola",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
