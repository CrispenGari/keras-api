{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_Sentiment_Analysis_With_A_Closer_Look_Plus_Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjwFyZTS3VMO"
      },
      "source": [
        "### A closer look on sentiment analysis in keras and embeddings.\n",
        "\n",
        "In this notebook we are going to have a closer look on sentiment analyisis and word embeddings.\n",
        "\n",
        "\n",
        "### Exploding & Vanishing Gradients\n",
        "\n",
        "In order to train the weights for the gates inside the recurrent unit, we need to minimize some loss-function which measures the difference between the actual output of the network relative to the desired output.\n",
        "\n",
        "The reccurent units are applied recursively for each word in the input sequence. This means the recurrent gate is applied once for each time-step. The gradient-signals have to flow back from the loss-function all the way to the first time the recurrent gate is used. If the gradient of the recurrent gate is multiplicative, then we essentially have an exponential function.\n",
        "\n",
        "We will use texts that have more than 500 words. This means the RU's gate for updating its internal memory-state is applied recursively more than 500 times. If a gradient of just 1.01 is multiplied with itself 500 times then it gives a value of about 145. If a gradient of just 0.99 is multiplied with itself 500 times then it gives a value of about 0.007. These are called exploding and vanishing gradients. The only gradients that can survive recurrent multiplication are 0 and 1.\n",
        "\n",
        "To avoid these so-called exploding and vanishing gradients, care must be made when designing the recurrent unit and its gates. That is why the actual implementation of the `GRU` is more complicated, because it tries to send the gradient back through the gates without this distortion.\n",
        "\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X6UWzq_N3NKC",
        "outputId": "4eff3c57-8f7a-482a-d98e-278d1f388599"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3noo515JMp"
      },
      "source": [
        "### Data \n",
        "\n",
        "We are going to download, extract and load the data using the following helper ffunctions. The code in teh following cell was found [here](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/imdb.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6-IoNpe7CSy"
      },
      "source": [
        "import os, glob, sys\n",
        "import urllib.request\n",
        "import tarfile, zipfile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTvOZx0h72FO"
      },
      "source": [
        "### The function that download the files from internet.\n",
        "\n",
        "Again the following code was found [here](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/download.py). Note that the code is not exactly the same. It is a modified vesion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmmPIoZB77Ci"
      },
      "source": [
        "class Download:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def print_download_progress(self, count, block_size, total_size):\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "    pct_complete = min(1.0, pct_complete)\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "  def download(self, base_url, filename, download_dir):\n",
        "    save_path = os.path.join(download_dir, filename)\n",
        "    if not os.path.exists(save_path):\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "        print(\"Downloading\", filename, \"...\")\n",
        "        url = base_url + filename\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=save_path,\n",
        "                                                  reporthook=self.print_download_progress)\n",
        "        print(\" Done!\")\n",
        "\n",
        "  def maybe_download_and_extract(self, url, download_dir):\n",
        "      filename = url.split('/')[-1]\n",
        "      file_path = os.path.join(download_dir, filename)\n",
        "      if not os.path.exists(file_path):\n",
        "          if not os.path.exists(download_dir):\n",
        "              os.makedirs(download_dir)\n",
        "\n",
        "          file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                    filename=file_path,\n",
        "                                                    reporthook=self.print_download_progress)\n",
        "          print()\n",
        "          print(\"Download finished. Extracting files.\")\n",
        "\n",
        "          if file_path.endswith(\".zip\"):\n",
        "              # Unpack the zip-file.\n",
        "              zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "          elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "              # Unpack the tar-ball.\n",
        "              tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "          print(\"Done.\")\n",
        "      else:\n",
        "          print(\"Data has apparently already been downloaded and unpacked.\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScrwtLvU8nVz"
      },
      "source": [
        "download = Download()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHCcSrDH6DOy"
      },
      "source": [
        "data_dir = \"data/IMDB/\"\n",
        "data_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "def _read_text_file(path):\n",
        "    with open(path, 'rt', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "        text = \" \".join(lines)\n",
        "    return text\n",
        "\n",
        "def maybe_download_and_extract():\n",
        "    download.maybe_download_and_extract(url=data_url, download_dir=data_dir)\n",
        "\n",
        "def load_data(train=True):\n",
        "    train_test_path = \"train\" if train else \"test\"\n",
        "    dir_base = os.path.join(data_dir, \"aclImdb\", train_test_path)\n",
        "    path_pattern_pos = os.path.join(dir_base, \"pos\", \"*.txt\")\n",
        "    path_pattern_neg = os.path.join(dir_base, \"neg\", \"*.txt\")\n",
        "    paths_pos = glob.glob(path_pattern_pos)\n",
        "    paths_neg = glob.glob(path_pattern_neg)\n",
        "    data_pos = [_read_text_file(path) for path in paths_pos]\n",
        "    data_neg = [_read_text_file(path) for path in paths_neg]\n",
        "    x = data_pos + data_neg\n",
        "    y = [1.0] * len(data_pos) + [0.0] * len(data_neg)\n",
        "    return x, y\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypm84eqs7cSN",
        "outputId": "b5c1a63e-390f-4096-e67b-0627b2a26c6d"
      },
      "source": [
        "maybe_download_and_extract()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has apparently already been downloaded and unpacked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Z3XH1Y9oh0"
      },
      "source": [
        "### Loading the train and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZgiXAN47cPh"
      },
      "source": [
        "x_train_text, y_train = load_data(train=True)\n",
        "x_test_text, y_test = load_data(train=False)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq4FTYs994Pj"
      },
      "source": [
        "### Counting examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ6puPxj7cIZ",
        "outputId": "bfaf6563-20d1-4762-9cd3-be52c19a02d3"
      },
      "source": [
        "print(\"Train-set size: \", len(x_train_text))\n",
        "print(\"Test-set size:  \", len(x_test_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set size:  25000\n",
            "Test-set size:   25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN0KlOM2-mqb"
      },
      "source": [
        "### Combining features together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqWvfCzG7cAB"
      },
      "source": [
        "features = x_train_text + x_test_text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgQOj28c-2Wy"
      },
      "source": [
        "### Visualizing a single example from the tests data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "jA5kSfoJ3NFF",
        "outputId": "1093a13d-096b-45d6-f50e-4eb9bcbba780"
      },
      "source": [
        "x_test_text[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I decided I need to lengthen up my review for my all time favorite film. Unlike other war films that focus on the event, Apocalypse Now takes the viewer into a psychological head trip. The sheer surrealism makes the body uncomfortable, yet you can\\'t lay your eyes off of it. Based off of Joseph Conrad\\'s Heart Of Darkness, Apocalypse Now slowly descends its protagonist, Willard (Martin Sheen) into madness, most likely the same way Kurtz plunged into insanity. The production of this film is notorious for its delays provided by the monsoon season and for Brando\\'s unprepared performance (he read his lines from cue cards). There is a documentary titled Apocalypse Now: A filmmakers Apocalypse which shows the hell everyone went through in making this.<br /><br />The opening sequence is one of the most famous and popular in any film. As the blade of the helicopters are heard in slow motion and napalm is dropped in the trees, the song \"The End\" by the Doors can be heard. The next shot is of Willard in his bed with the fan on, so the noise of the helicopter coincides with the fan. We are informed that he does special missions for the military, mostly assassinations. When his next mission is given to him, he is baffled. \"Charging a man with murder here is like giving a speeding ticket in the Indy 500.\" The man he has to kill was a respected colonel that has gone insane and isolated himself along with tribes people. Kurtz is ordering atrocious acts that are carried out by these people and he must me stopped. Willard does not go alone however. He is carried on a boat with several soldiers and they come across several battles. Along the way, they meet Lieutenant Colonel Bill Kilgore \"Hoorah\" about the war. Willard ponders that if Kilgore is that crazy, what could Kurtz be like. There are many scenes that portray Willards plunge into insanity: The tiger attack, the slaughter of innocent Vietnamese, the nonstop rain, the piled dead bodies scattered about, and the deaths of his crew members. When he reaches the Kurtz compound, he is greeted by the village people and a hippie photojournalist (Dennis Hopper). Instead of assassinating Kurtz right away, Willard begins talking with him and his conscience begins to doubt what he should do. Kurtz, on the other hand wants to die. He is tired of the war and wants to go down as a soldier. Willard kills him with a machete while in unison, a buffalo is sacrificed with several machetes by the people. Once they realize their leader has been slain, instead of killing Willard, they hail him as their new king. Willard rejects the offer and leaves them.<br /><br />The cinematography here is absolutely breathtaking. The colors are grain free, something that is rare in older movies. I can watch it muted and admire the beauty of the scenery.<br /><br />The acting ensemble is terrific, with everyone playing their parts well. Many criticize Brando for some reason, but I think he nails his role as a depressed lunatic who is beaten up by the war.<br /><br />The soundtrack and the score are haunting, and provide the mood for the film. I am wondering what instrument they used in that guitar-like sound when the credits roll? There have been many parodies of this film, but my favorite quote comes from Marge Simpson when she explains to Homer why a character with the same name on a police show is behaving like an idiot: \"Your character provides comic relief for the show, like um, Marlon Brando in Apocalypse Now.\" Those who have seen the movie know why this is hilarious.'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1QEldLY_Bh9"
      },
      "source": [
        "Checking the label for the above review..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl4qLIyg_BI-",
        "outputId": "65344afc-a549-46b6-fb27-89dec5540024"
      },
      "source": [
        "y_test[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZXbC2X8_IVj"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "A neural network cannot work directly on text-strings so we must convert it somehow. There are two steps in this conversion, the first step is called the \"tokenizer\" which converts words to integers and is done on the data-set before it is input to the neural network. The second step is an integrated part of the neural network itself and is called the \"embedding\"-layer.\n",
        "\n",
        "WE CAN TELL THE `Tokenizer` TO USE ONLY CERTAIN EXAMPLES OF PORPULAR WORDS IN THE DATASET FOR EXAPMLE WE ARE GOING TO USE ONLY `10000` WORDS FOR THIS SIMPLE EXAMPLE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fGLTlkq3NCa"
      },
      "source": [
        "num_words = 10_000"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz1atot83M_s"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"<oov>\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXpdUe5x__US"
      },
      "source": [
        "### Fitting Our dataset into the `tokenizer`\n",
        "\n",
        "We then need to fit the data to the tokenizer using the `tokenizer.fit_on_texts()` function as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9jjmBc93M9L"
      },
      "source": [
        "tokenizer.fit_on_texts(features)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CseGqhv_AfOb"
      },
      "source": [
        "You can inspect the vocabulary of the tokenizer by calling:\n",
        "\n",
        "```\n",
        "tokenizer.word_index\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufULTpUnBVKs",
        "outputId": "8bb7a6e7-a46b-4387-ee3b-a6ee03e23378"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<oov>': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'a': 4,\n",
              " 'of': 5,\n",
              " 'to': 6,\n",
              " 'is': 7,\n",
              " 'br': 8,\n",
              " 'in': 9,\n",
              " 'it': 10,\n",
              " 'i': 11,\n",
              " 'this': 12,\n",
              " 'that': 13,\n",
              " 'was': 14,\n",
              " 'as': 15,\n",
              " 'for': 16,\n",
              " 'with': 17,\n",
              " 'movie': 18,\n",
              " 'but': 19,\n",
              " 'film': 20,\n",
              " 'on': 21,\n",
              " 'not': 22,\n",
              " 'you': 23,\n",
              " 'are': 24,\n",
              " 'his': 25,\n",
              " 'have': 26,\n",
              " 'be': 27,\n",
              " 'one': 28,\n",
              " 'he': 29,\n",
              " 'all': 30,\n",
              " 'at': 31,\n",
              " 'by': 32,\n",
              " 'an': 33,\n",
              " 'they': 34,\n",
              " 'so': 35,\n",
              " 'who': 36,\n",
              " 'from': 37,\n",
              " 'like': 38,\n",
              " 'or': 39,\n",
              " 'just': 40,\n",
              " 'her': 41,\n",
              " 'out': 42,\n",
              " 'about': 43,\n",
              " 'if': 44,\n",
              " \"it's\": 45,\n",
              " 'has': 46,\n",
              " 'there': 47,\n",
              " 'some': 48,\n",
              " 'what': 49,\n",
              " 'good': 50,\n",
              " 'when': 51,\n",
              " 'more': 52,\n",
              " 'very': 53,\n",
              " 'up': 54,\n",
              " 'no': 55,\n",
              " 'time': 56,\n",
              " 'my': 57,\n",
              " 'even': 58,\n",
              " 'would': 59,\n",
              " 'she': 60,\n",
              " 'which': 61,\n",
              " 'only': 62,\n",
              " 'really': 63,\n",
              " 'see': 64,\n",
              " 'story': 65,\n",
              " 'their': 66,\n",
              " 'had': 67,\n",
              " 'can': 68,\n",
              " 'me': 69,\n",
              " 'well': 70,\n",
              " 'were': 71,\n",
              " 'than': 72,\n",
              " 'much': 73,\n",
              " 'we': 74,\n",
              " 'bad': 75,\n",
              " 'been': 76,\n",
              " 'get': 77,\n",
              " 'do': 78,\n",
              " 'great': 79,\n",
              " 'other': 80,\n",
              " 'will': 81,\n",
              " 'also': 82,\n",
              " 'into': 83,\n",
              " 'people': 84,\n",
              " 'because': 85,\n",
              " 'how': 86,\n",
              " 'first': 87,\n",
              " 'him': 88,\n",
              " 'most': 89,\n",
              " \"don't\": 90,\n",
              " 'made': 91,\n",
              " 'then': 92,\n",
              " 'its': 93,\n",
              " 'them': 94,\n",
              " 'make': 95,\n",
              " 'way': 96,\n",
              " 'too': 97,\n",
              " 'movies': 98,\n",
              " 'could': 99,\n",
              " 'any': 100,\n",
              " 'after': 101,\n",
              " 'think': 102,\n",
              " 'characters': 103,\n",
              " 'watch': 104,\n",
              " 'films': 105,\n",
              " 'two': 106,\n",
              " 'many': 107,\n",
              " 'seen': 108,\n",
              " 'character': 109,\n",
              " 'being': 110,\n",
              " 'never': 111,\n",
              " 'plot': 112,\n",
              " 'love': 113,\n",
              " 'acting': 114,\n",
              " 'life': 115,\n",
              " 'did': 116,\n",
              " 'best': 117,\n",
              " 'where': 118,\n",
              " 'know': 119,\n",
              " 'show': 120,\n",
              " 'little': 121,\n",
              " 'over': 122,\n",
              " 'off': 123,\n",
              " 'ever': 124,\n",
              " 'does': 125,\n",
              " 'your': 126,\n",
              " 'better': 127,\n",
              " 'end': 128,\n",
              " 'man': 129,\n",
              " 'scene': 130,\n",
              " 'still': 131,\n",
              " 'say': 132,\n",
              " 'these': 133,\n",
              " 'here': 134,\n",
              " 'why': 135,\n",
              " 'scenes': 136,\n",
              " 'while': 137,\n",
              " 'something': 138,\n",
              " 'such': 139,\n",
              " 'go': 140,\n",
              " 'through': 141,\n",
              " 'back': 142,\n",
              " 'should': 143,\n",
              " 'those': 144,\n",
              " 'real': 145,\n",
              " \"i'm\": 146,\n",
              " 'now': 147,\n",
              " 'watching': 148,\n",
              " 'thing': 149,\n",
              " \"doesn't\": 150,\n",
              " 'actors': 151,\n",
              " 'though': 152,\n",
              " 'funny': 153,\n",
              " 'years': 154,\n",
              " \"didn't\": 155,\n",
              " 'old': 156,\n",
              " 'another': 157,\n",
              " '10': 158,\n",
              " 'work': 159,\n",
              " 'before': 160,\n",
              " 'actually': 161,\n",
              " 'nothing': 162,\n",
              " 'makes': 163,\n",
              " 'look': 164,\n",
              " 'director': 165,\n",
              " 'find': 166,\n",
              " 'going': 167,\n",
              " 'same': 168,\n",
              " 'new': 169,\n",
              " 'lot': 170,\n",
              " 'every': 171,\n",
              " 'few': 172,\n",
              " 'again': 173,\n",
              " 'part': 174,\n",
              " 'cast': 175,\n",
              " 'down': 176,\n",
              " 'us': 177,\n",
              " 'things': 178,\n",
              " 'want': 179,\n",
              " 'quite': 180,\n",
              " 'pretty': 181,\n",
              " 'world': 182,\n",
              " 'horror': 183,\n",
              " 'around': 184,\n",
              " 'seems': 185,\n",
              " \"can't\": 186,\n",
              " 'young': 187,\n",
              " 'take': 188,\n",
              " 'however': 189,\n",
              " 'got': 190,\n",
              " 'thought': 191,\n",
              " 'big': 192,\n",
              " 'fact': 193,\n",
              " 'enough': 194,\n",
              " 'long': 195,\n",
              " 'both': 196,\n",
              " \"that's\": 197,\n",
              " 'give': 198,\n",
              " \"i've\": 199,\n",
              " 'own': 200,\n",
              " 'may': 201,\n",
              " 'between': 202,\n",
              " 'comedy': 203,\n",
              " 'right': 204,\n",
              " 'series': 205,\n",
              " 'action': 206,\n",
              " 'must': 207,\n",
              " 'music': 208,\n",
              " 'without': 209,\n",
              " 'times': 210,\n",
              " 'saw': 211,\n",
              " 'always': 212,\n",
              " 'original': 213,\n",
              " \"isn't\": 214,\n",
              " 'role': 215,\n",
              " 'come': 216,\n",
              " 'almost': 217,\n",
              " 'gets': 218,\n",
              " 'interesting': 219,\n",
              " 'guy': 220,\n",
              " 'point': 221,\n",
              " 'done': 222,\n",
              " \"there's\": 223,\n",
              " 'whole': 224,\n",
              " 'least': 225,\n",
              " 'far': 226,\n",
              " 'bit': 227,\n",
              " 'script': 228,\n",
              " 'minutes': 229,\n",
              " 'feel': 230,\n",
              " '2': 231,\n",
              " 'anything': 232,\n",
              " 'making': 233,\n",
              " 'might': 234,\n",
              " 'since': 235,\n",
              " 'am': 236,\n",
              " 'family': 237,\n",
              " \"he's\": 238,\n",
              " 'last': 239,\n",
              " 'probably': 240,\n",
              " 'tv': 241,\n",
              " 'performance': 242,\n",
              " 'kind': 243,\n",
              " 'away': 244,\n",
              " 'yet': 245,\n",
              " 'fun': 246,\n",
              " 'worst': 247,\n",
              " 'sure': 248,\n",
              " 'rather': 249,\n",
              " 'hard': 250,\n",
              " 'girl': 251,\n",
              " 'anyone': 252,\n",
              " 'each': 253,\n",
              " 'played': 254,\n",
              " 'day': 255,\n",
              " 'found': 256,\n",
              " 'looking': 257,\n",
              " 'woman': 258,\n",
              " 'screen': 259,\n",
              " 'although': 260,\n",
              " 'our': 261,\n",
              " 'especially': 262,\n",
              " 'believe': 263,\n",
              " 'having': 264,\n",
              " 'trying': 265,\n",
              " 'course': 266,\n",
              " 'dvd': 267,\n",
              " 'everything': 268,\n",
              " 'set': 269,\n",
              " 'goes': 270,\n",
              " 'comes': 271,\n",
              " 'put': 272,\n",
              " 'ending': 273,\n",
              " 'maybe': 274,\n",
              " 'place': 275,\n",
              " 'book': 276,\n",
              " 'shows': 277,\n",
              " 'three': 278,\n",
              " 'worth': 279,\n",
              " 'different': 280,\n",
              " 'main': 281,\n",
              " 'once': 282,\n",
              " 'sense': 283,\n",
              " 'american': 284,\n",
              " 'reason': 285,\n",
              " 'looks': 286,\n",
              " 'effects': 287,\n",
              " 'watched': 288,\n",
              " 'play': 289,\n",
              " 'true': 290,\n",
              " 'money': 291,\n",
              " 'actor': 292,\n",
              " \"wasn't\": 293,\n",
              " 'job': 294,\n",
              " 'together': 295,\n",
              " 'war': 296,\n",
              " 'someone': 297,\n",
              " 'plays': 298,\n",
              " 'instead': 299,\n",
              " 'high': 300,\n",
              " 'during': 301,\n",
              " 'year': 302,\n",
              " 'said': 303,\n",
              " 'half': 304,\n",
              " 'everyone': 305,\n",
              " 'later': 306,\n",
              " 'takes': 307,\n",
              " '1': 308,\n",
              " 'seem': 309,\n",
              " 'audience': 310,\n",
              " 'special': 311,\n",
              " 'beautiful': 312,\n",
              " 'left': 313,\n",
              " 'himself': 314,\n",
              " 'seeing': 315,\n",
              " 'john': 316,\n",
              " 'night': 317,\n",
              " 'black': 318,\n",
              " 'version': 319,\n",
              " 'shot': 320,\n",
              " 'excellent': 321,\n",
              " 'idea': 322,\n",
              " 'house': 323,\n",
              " 'mind': 324,\n",
              " 'star': 325,\n",
              " 'wife': 326,\n",
              " 'fan': 327,\n",
              " 'death': 328,\n",
              " 'used': 329,\n",
              " 'else': 330,\n",
              " 'simply': 331,\n",
              " 'nice': 332,\n",
              " 'budget': 333,\n",
              " 'poor': 334,\n",
              " 'completely': 335,\n",
              " 'short': 336,\n",
              " 'second': 337,\n",
              " \"you're\": 338,\n",
              " '3': 339,\n",
              " 'read': 340,\n",
              " 'along': 341,\n",
              " 'less': 342,\n",
              " 'top': 343,\n",
              " 'help': 344,\n",
              " 'home': 345,\n",
              " 'men': 346,\n",
              " 'either': 347,\n",
              " 'line': 348,\n",
              " 'boring': 349,\n",
              " 'dead': 350,\n",
              " 'friends': 351,\n",
              " 'kids': 352,\n",
              " 'try': 353,\n",
              " 'production': 354,\n",
              " 'enjoy': 355,\n",
              " 'camera': 356,\n",
              " 'wrong': 357,\n",
              " 'use': 358,\n",
              " 'given': 359,\n",
              " 'low': 360,\n",
              " 'classic': 361,\n",
              " 'father': 362,\n",
              " 'need': 363,\n",
              " 'full': 364,\n",
              " 'stupid': 365,\n",
              " 'until': 366,\n",
              " 'next': 367,\n",
              " 'performances': 368,\n",
              " 'school': 369,\n",
              " 'hollywood': 370,\n",
              " 'rest': 371,\n",
              " 'truly': 372,\n",
              " 'awful': 373,\n",
              " 'video': 374,\n",
              " 'couple': 375,\n",
              " 'start': 376,\n",
              " 'sex': 377,\n",
              " 'recommend': 378,\n",
              " 'women': 379,\n",
              " 'let': 380,\n",
              " 'tell': 381,\n",
              " 'terrible': 382,\n",
              " 'remember': 383,\n",
              " 'mean': 384,\n",
              " 'came': 385,\n",
              " 'understand': 386,\n",
              " 'getting': 387,\n",
              " 'perhaps': 388,\n",
              " 'moments': 389,\n",
              " 'name': 390,\n",
              " 'keep': 391,\n",
              " 'face': 392,\n",
              " 'itself': 393,\n",
              " 'wonderful': 394,\n",
              " 'playing': 395,\n",
              " 'human': 396,\n",
              " 'style': 397,\n",
              " 'small': 398,\n",
              " 'episode': 399,\n",
              " 'perfect': 400,\n",
              " 'others': 401,\n",
              " 'person': 402,\n",
              " 'doing': 403,\n",
              " 'often': 404,\n",
              " 'early': 405,\n",
              " 'stars': 406,\n",
              " 'definitely': 407,\n",
              " 'written': 408,\n",
              " 'head': 409,\n",
              " 'lines': 410,\n",
              " 'dialogue': 411,\n",
              " 'gives': 412,\n",
              " 'piece': 413,\n",
              " \"couldn't\": 414,\n",
              " 'went': 415,\n",
              " 'finally': 416,\n",
              " 'mother': 417,\n",
              " 'case': 418,\n",
              " 'title': 419,\n",
              " 'absolutely': 420,\n",
              " 'live': 421,\n",
              " 'boy': 422,\n",
              " 'yes': 423,\n",
              " 'laugh': 424,\n",
              " 'certainly': 425,\n",
              " 'liked': 426,\n",
              " 'become': 427,\n",
              " 'entertaining': 428,\n",
              " 'worse': 429,\n",
              " 'oh': 430,\n",
              " 'sort': 431,\n",
              " 'loved': 432,\n",
              " 'lost': 433,\n",
              " 'hope': 434,\n",
              " 'called': 435,\n",
              " 'picture': 436,\n",
              " 'felt': 437,\n",
              " 'overall': 438,\n",
              " 'entire': 439,\n",
              " 'several': 440,\n",
              " 'mr': 441,\n",
              " 'based': 442,\n",
              " 'supposed': 443,\n",
              " 'cinema': 444,\n",
              " 'friend': 445,\n",
              " 'guys': 446,\n",
              " 'sound': 447,\n",
              " '5': 448,\n",
              " 'problem': 449,\n",
              " 'drama': 450,\n",
              " 'against': 451,\n",
              " 'waste': 452,\n",
              " 'white': 453,\n",
              " 'beginning': 454,\n",
              " '4': 455,\n",
              " 'fans': 456,\n",
              " 'totally': 457,\n",
              " 'dark': 458,\n",
              " 'care': 459,\n",
              " 'direction': 460,\n",
              " 'humor': 461,\n",
              " 'wanted': 462,\n",
              " \"she's\": 463,\n",
              " 'seemed': 464,\n",
              " 'game': 465,\n",
              " 'under': 466,\n",
              " 'children': 467,\n",
              " 'despite': 468,\n",
              " 'lives': 469,\n",
              " 'lead': 470,\n",
              " 'guess': 471,\n",
              " 'example': 472,\n",
              " 'already': 473,\n",
              " 'final': 474,\n",
              " 'throughout': 475,\n",
              " \"you'll\": 476,\n",
              " 'evil': 477,\n",
              " 'turn': 478,\n",
              " 'becomes': 479,\n",
              " 'unfortunately': 480,\n",
              " 'able': 481,\n",
              " 'quality': 482,\n",
              " \"i'd\": 483,\n",
              " 'days': 484,\n",
              " 'history': 485,\n",
              " 'fine': 486,\n",
              " 'side': 487,\n",
              " 'wants': 488,\n",
              " 'horrible': 489,\n",
              " 'heart': 490,\n",
              " 'writing': 491,\n",
              " 'amazing': 492,\n",
              " 'b': 493,\n",
              " 'flick': 494,\n",
              " 'killer': 495,\n",
              " 'run': 496,\n",
              " 'son': 497,\n",
              " '\\x96': 498,\n",
              " 'michael': 499,\n",
              " 'works': 500,\n",
              " 'close': 501,\n",
              " \"they're\": 502,\n",
              " 'act': 503,\n",
              " 'art': 504,\n",
              " 'matter': 505,\n",
              " 'kill': 506,\n",
              " 'etc': 507,\n",
              " 'tries': 508,\n",
              " \"won't\": 509,\n",
              " 'past': 510,\n",
              " 'town': 511,\n",
              " 'turns': 512,\n",
              " 'enjoyed': 513,\n",
              " 'brilliant': 514,\n",
              " 'gave': 515,\n",
              " 'behind': 516,\n",
              " 'parts': 517,\n",
              " 'stuff': 518,\n",
              " 'genre': 519,\n",
              " 'eyes': 520,\n",
              " 'car': 521,\n",
              " 'favorite': 522,\n",
              " 'directed': 523,\n",
              " 'late': 524,\n",
              " 'hand': 525,\n",
              " 'expect': 526,\n",
              " 'soon': 527,\n",
              " 'hour': 528,\n",
              " 'obviously': 529,\n",
              " 'themselves': 530,\n",
              " 'sometimes': 531,\n",
              " 'killed': 532,\n",
              " 'actress': 533,\n",
              " 'thinking': 534,\n",
              " 'child': 535,\n",
              " 'girls': 536,\n",
              " 'viewer': 537,\n",
              " 'starts': 538,\n",
              " 'city': 539,\n",
              " 'myself': 540,\n",
              " 'decent': 541,\n",
              " 'highly': 542,\n",
              " 'stop': 543,\n",
              " 'type': 544,\n",
              " 'self': 545,\n",
              " 'god': 546,\n",
              " 'says': 547,\n",
              " 'group': 548,\n",
              " 'anyway': 549,\n",
              " 'voice': 550,\n",
              " 'took': 551,\n",
              " 'known': 552,\n",
              " 'blood': 553,\n",
              " 'kid': 554,\n",
              " 'heard': 555,\n",
              " 'happens': 556,\n",
              " 'except': 557,\n",
              " 'fight': 558,\n",
              " 'feeling': 559,\n",
              " 'experience': 560,\n",
              " 'coming': 561,\n",
              " 'slow': 562,\n",
              " 'daughter': 563,\n",
              " 'writer': 564,\n",
              " 'stories': 565,\n",
              " 'moment': 566,\n",
              " 'leave': 567,\n",
              " 'told': 568,\n",
              " 'extremely': 569,\n",
              " 'score': 570,\n",
              " 'violence': 571,\n",
              " 'involved': 572,\n",
              " 'police': 573,\n",
              " 'strong': 574,\n",
              " 'lack': 575,\n",
              " 'chance': 576,\n",
              " 'cannot': 577,\n",
              " 'hit': 578,\n",
              " 'roles': 579,\n",
              " 'hilarious': 580,\n",
              " 's': 581,\n",
              " 'happen': 582,\n",
              " 'wonder': 583,\n",
              " 'particularly': 584,\n",
              " 'ok': 585,\n",
              " 'including': 586,\n",
              " 'save': 587,\n",
              " 'living': 588,\n",
              " 'looked': 589,\n",
              " \"wouldn't\": 590,\n",
              " 'crap': 591,\n",
              " 'simple': 592,\n",
              " 'please': 593,\n",
              " 'murder': 594,\n",
              " 'cool': 595,\n",
              " 'obvious': 596,\n",
              " 'happened': 597,\n",
              " 'complete': 598,\n",
              " 'cut': 599,\n",
              " 'age': 600,\n",
              " 'serious': 601,\n",
              " 'gore': 602,\n",
              " 'attempt': 603,\n",
              " 'hell': 604,\n",
              " 'ago': 605,\n",
              " 'song': 606,\n",
              " 'shown': 607,\n",
              " 'taken': 608,\n",
              " 'english': 609,\n",
              " 'james': 610,\n",
              " 'robert': 611,\n",
              " 'david': 612,\n",
              " 'seriously': 613,\n",
              " 'released': 614,\n",
              " 'reality': 615,\n",
              " 'opening': 616,\n",
              " 'interest': 617,\n",
              " 'jokes': 618,\n",
              " 'across': 619,\n",
              " 'none': 620,\n",
              " 'hero': 621,\n",
              " 'today': 622,\n",
              " 'possible': 623,\n",
              " 'exactly': 624,\n",
              " 'alone': 625,\n",
              " 'sad': 626,\n",
              " 'brother': 627,\n",
              " 'number': 628,\n",
              " 'career': 629,\n",
              " 'saying': 630,\n",
              " \"film's\": 631,\n",
              " 'usually': 632,\n",
              " 'hours': 633,\n",
              " 'cinematography': 634,\n",
              " 'talent': 635,\n",
              " 'view': 636,\n",
              " 'yourself': 637,\n",
              " 'running': 638,\n",
              " 'annoying': 639,\n",
              " 'relationship': 640,\n",
              " 'documentary': 641,\n",
              " 'wish': 642,\n",
              " 'order': 643,\n",
              " 'huge': 644,\n",
              " 'shots': 645,\n",
              " 'whose': 646,\n",
              " 'ridiculous': 647,\n",
              " 'taking': 648,\n",
              " 'important': 649,\n",
              " 'light': 650,\n",
              " 'body': 651,\n",
              " 'middle': 652,\n",
              " 'level': 653,\n",
              " 'ends': 654,\n",
              " 'female': 655,\n",
              " 'started': 656,\n",
              " 'call': 657,\n",
              " \"i'll\": 658,\n",
              " 'husband': 659,\n",
              " 'four': 660,\n",
              " 'power': 661,\n",
              " 'major': 662,\n",
              " 'word': 663,\n",
              " 'turned': 664,\n",
              " 'opinion': 665,\n",
              " 'change': 666,\n",
              " 'mostly': 667,\n",
              " 'usual': 668,\n",
              " 'silly': 669,\n",
              " 'scary': 670,\n",
              " 'rating': 671,\n",
              " 'beyond': 672,\n",
              " 'somewhat': 673,\n",
              " 'ones': 674,\n",
              " 'happy': 675,\n",
              " 'words': 676,\n",
              " 'room': 677,\n",
              " 'knew': 678,\n",
              " 'knows': 679,\n",
              " 'country': 680,\n",
              " 'disappointed': 681,\n",
              " 'talking': 682,\n",
              " 'novel': 683,\n",
              " 'apparently': 684,\n",
              " 'non': 685,\n",
              " 'strange': 686,\n",
              " 'attention': 687,\n",
              " 'upon': 688,\n",
              " 'single': 689,\n",
              " 'finds': 690,\n",
              " 'basically': 691,\n",
              " 'cheap': 692,\n",
              " 'modern': 693,\n",
              " 'due': 694,\n",
              " 'jack': 695,\n",
              " 'musical': 696,\n",
              " 'television': 697,\n",
              " 'problems': 698,\n",
              " 'miss': 699,\n",
              " 'episodes': 700,\n",
              " 'clearly': 701,\n",
              " 'local': 702,\n",
              " '7': 703,\n",
              " 'british': 704,\n",
              " 'thriller': 705,\n",
              " 'talk': 706,\n",
              " 'events': 707,\n",
              " 'five': 708,\n",
              " 'sequence': 709,\n",
              " \"aren't\": 710,\n",
              " 'class': 711,\n",
              " 'french': 712,\n",
              " 'moving': 713,\n",
              " 'ten': 714,\n",
              " 'fast': 715,\n",
              " 'earth': 716,\n",
              " 'review': 717,\n",
              " 'tells': 718,\n",
              " 'predictable': 719,\n",
              " 'songs': 720,\n",
              " 'team': 721,\n",
              " 'comic': 722,\n",
              " 'straight': 723,\n",
              " '8': 724,\n",
              " 'whether': 725,\n",
              " 'die': 726,\n",
              " 'add': 727,\n",
              " 'dialog': 728,\n",
              " 'entertainment': 729,\n",
              " 'above': 730,\n",
              " 'sets': 731,\n",
              " 'future': 732,\n",
              " 'enjoyable': 733,\n",
              " 'appears': 734,\n",
              " 'near': 735,\n",
              " 'space': 736,\n",
              " 'easily': 737,\n",
              " 'hate': 738,\n",
              " 'soundtrack': 739,\n",
              " 'bring': 740,\n",
              " 'giving': 741,\n",
              " 'lots': 742,\n",
              " 'similar': 743,\n",
              " 'romantic': 744,\n",
              " 'george': 745,\n",
              " 'supporting': 746,\n",
              " 'release': 747,\n",
              " 'mention': 748,\n",
              " 'filmed': 749,\n",
              " 'within': 750,\n",
              " 'message': 751,\n",
              " 'sequel': 752,\n",
              " 'clear': 753,\n",
              " 'falls': 754,\n",
              " 'needs': 755,\n",
              " \"haven't\": 756,\n",
              " 'dull': 757,\n",
              " 'suspense': 758,\n",
              " 'eye': 759,\n",
              " 'bunch': 760,\n",
              " 'surprised': 761,\n",
              " 'showing': 762,\n",
              " 'tried': 763,\n",
              " 'sorry': 764,\n",
              " 'certain': 765,\n",
              " 'easy': 766,\n",
              " 'working': 767,\n",
              " 'ways': 768,\n",
              " 'theme': 769,\n",
              " 'theater': 770,\n",
              " 'among': 771,\n",
              " 'named': 772,\n",
              " \"what's\": 773,\n",
              " 'storyline': 774,\n",
              " 'monster': 775,\n",
              " 'king': 776,\n",
              " 'stay': 777,\n",
              " 'effort': 778,\n",
              " 'fall': 779,\n",
              " 'stand': 780,\n",
              " 'minute': 781,\n",
              " 'gone': 782,\n",
              " 'rock': 783,\n",
              " 'using': 784,\n",
              " '9': 785,\n",
              " 'feature': 786,\n",
              " 'buy': 787,\n",
              " 'comments': 788,\n",
              " \"'\": 789,\n",
              " 'typical': 790,\n",
              " 't': 791,\n",
              " 'editing': 792,\n",
              " 'sister': 793,\n",
              " 'tale': 794,\n",
              " 'avoid': 795,\n",
              " 'deal': 796,\n",
              " 'dr': 797,\n",
              " 'mystery': 798,\n",
              " 'doubt': 799,\n",
              " 'fantastic': 800,\n",
              " 'nearly': 801,\n",
              " 'kept': 802,\n",
              " 'feels': 803,\n",
              " 'okay': 804,\n",
              " 'subject': 805,\n",
              " 'viewing': 806,\n",
              " 'elements': 807,\n",
              " 'oscar': 808,\n",
              " 'check': 809,\n",
              " 'realistic': 810,\n",
              " 'points': 811,\n",
              " 'greatest': 812,\n",
              " 'means': 813,\n",
              " 'herself': 814,\n",
              " 'parents': 815,\n",
              " 'famous': 816,\n",
              " 'imagine': 817,\n",
              " 'rent': 818,\n",
              " 'viewers': 819,\n",
              " 'crime': 820,\n",
              " 'richard': 821,\n",
              " 'form': 822,\n",
              " 'peter': 823,\n",
              " 'actual': 824,\n",
              " 'lady': 825,\n",
              " 'general': 826,\n",
              " 'dog': 827,\n",
              " 'follow': 828,\n",
              " 'believable': 829,\n",
              " 'period': 830,\n",
              " 'red': 831,\n",
              " 'move': 832,\n",
              " 'brought': 833,\n",
              " 'material': 834,\n",
              " 'forget': 835,\n",
              " 'somehow': 836,\n",
              " 'begins': 837,\n",
              " 're': 838,\n",
              " 'reviews': 839,\n",
              " 'animation': 840,\n",
              " 'paul': 841,\n",
              " \"you've\": 842,\n",
              " 'leads': 843,\n",
              " 'weak': 844,\n",
              " 'figure': 845,\n",
              " 'surprise': 846,\n",
              " 'sit': 847,\n",
              " 'hear': 848,\n",
              " 'average': 849,\n",
              " 'open': 850,\n",
              " 'sequences': 851,\n",
              " 'atmosphere': 852,\n",
              " 'killing': 853,\n",
              " 'eventually': 854,\n",
              " 'learn': 855,\n",
              " 'tom': 856,\n",
              " 'premise': 857,\n",
              " 'wait': 858,\n",
              " '20': 859,\n",
              " 'sci': 860,\n",
              " 'deep': 861,\n",
              " 'fi': 862,\n",
              " 'expected': 863,\n",
              " 'whatever': 864,\n",
              " 'indeed': 865,\n",
              " 'poorly': 866,\n",
              " 'note': 867,\n",
              " 'particular': 868,\n",
              " 'lame': 869,\n",
              " 'imdb': 870,\n",
              " 'dance': 871,\n",
              " 'situation': 872,\n",
              " 'shame': 873,\n",
              " 'third': 874,\n",
              " 'york': 875,\n",
              " 'box': 876,\n",
              " 'truth': 877,\n",
              " 'decided': 878,\n",
              " 'free': 879,\n",
              " 'hot': 880,\n",
              " \"who's\": 881,\n",
              " 'difficult': 882,\n",
              " 'needed': 883,\n",
              " 'season': 884,\n",
              " 'acted': 885,\n",
              " 'leaves': 886,\n",
              " 'unless': 887,\n",
              " 'romance': 888,\n",
              " 'emotional': 889,\n",
              " 'possibly': 890,\n",
              " 'sexual': 891,\n",
              " 'gay': 892,\n",
              " 'boys': 893,\n",
              " 'footage': 894,\n",
              " 'write': 895,\n",
              " 'western': 896,\n",
              " 'forced': 897,\n",
              " 'credits': 898,\n",
              " 'doctor': 899,\n",
              " 'became': 900,\n",
              " 'reading': 901,\n",
              " 'memorable': 902,\n",
              " 'otherwise': 903,\n",
              " 'begin': 904,\n",
              " 'crew': 905,\n",
              " 'de': 906,\n",
              " 'air': 907,\n",
              " 'question': 908,\n",
              " 'meet': 909,\n",
              " 'society': 910,\n",
              " 'male': 911,\n",
              " \"let's\": 912,\n",
              " 'meets': 913,\n",
              " 'plus': 914,\n",
              " 'cheesy': 915,\n",
              " 'hands': 916,\n",
              " 'superb': 917,\n",
              " 'screenplay': 918,\n",
              " 'interested': 919,\n",
              " 'beauty': 920,\n",
              " 'features': 921,\n",
              " 'street': 922,\n",
              " 'masterpiece': 923,\n",
              " 'perfectly': 924,\n",
              " 'whom': 925,\n",
              " 'laughs': 926,\n",
              " 'stage': 927,\n",
              " 'nature': 928,\n",
              " 'effect': 929,\n",
              " 'forward': 930,\n",
              " 'comment': 931,\n",
              " 'nor': 932,\n",
              " 'badly': 933,\n",
              " 'e': 934,\n",
              " 'previous': 935,\n",
              " 'sounds': 936,\n",
              " 'japanese': 937,\n",
              " 'weird': 938,\n",
              " 'island': 939,\n",
              " 'personal': 940,\n",
              " 'inside': 941,\n",
              " 'quickly': 942,\n",
              " 'total': 943,\n",
              " 'keeps': 944,\n",
              " 'towards': 945,\n",
              " 'result': 946,\n",
              " 'america': 947,\n",
              " 'battle': 948,\n",
              " 'crazy': 949,\n",
              " 'worked': 950,\n",
              " 'setting': 951,\n",
              " 'incredibly': 952,\n",
              " 'earlier': 953,\n",
              " 'background': 954,\n",
              " 'mess': 955,\n",
              " 'cop': 956,\n",
              " 'writers': 957,\n",
              " 'fire': 958,\n",
              " 'copy': 959,\n",
              " 'unique': 960,\n",
              " 'realize': 961,\n",
              " 'dumb': 962,\n",
              " 'powerful': 963,\n",
              " 'mark': 964,\n",
              " 'lee': 965,\n",
              " 'business': 966,\n",
              " 'rate': 967,\n",
              " 'dramatic': 968,\n",
              " 'older': 969,\n",
              " 'pay': 970,\n",
              " 'following': 971,\n",
              " 'joke': 972,\n",
              " 'directors': 973,\n",
              " 'girlfriend': 974,\n",
              " 'plenty': 975,\n",
              " 'directing': 976,\n",
              " 'various': 977,\n",
              " 'creepy': 978,\n",
              " 'baby': 979,\n",
              " 'appear': 980,\n",
              " 'development': 981,\n",
              " 'brings': 982,\n",
              " 'front': 983,\n",
              " 'ask': 984,\n",
              " 'dream': 985,\n",
              " 'water': 986,\n",
              " 'rich': 987,\n",
              " 'admit': 988,\n",
              " 'bill': 989,\n",
              " 'apart': 990,\n",
              " 'joe': 991,\n",
              " 'fairly': 992,\n",
              " 'political': 993,\n",
              " 'leading': 994,\n",
              " 'reasons': 995,\n",
              " 'spent': 996,\n",
              " 'portrayed': 997,\n",
              " 'telling': 998,\n",
              " 'cover': 999,\n",
              " 'outside': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsJLwVjA06R"
      },
      "source": [
        "### Converting tokens to sequences\n",
        "\n",
        "We can then now convert our tokes to sequences of integers using the `text_to_sequences` on the tokenizer object as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D33LTyjkAezP"
      },
      "source": [
        "train_tokens = tokenizer.texts_to_sequences(x_train_text)\n",
        "test_tokens = tokenizer.texts_to_sequences(x_test_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4ItpEMd3M6W",
        "outputId": "b4507d67-78ed-4fc6-ce75-4a6041153db4"
      },
      "source": [
        "np.array(test_tokens[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  11,  878,   11,  363,    6,    1,   54,   57,  717,   16,   57,\n",
              "         30,   56,  522,   20, 1031,   80,  296,  105,   13, 1152,   21,\n",
              "          2, 1562, 4888,  147,  307,    2,  537,   83,    4, 2007,  409,\n",
              "       1232,    2, 2025, 8272,  163,    2,  651, 3323,  245,   23,  186,\n",
              "       4199,  126,  520,  123,    5,   10,  442,  123,    5, 2447,    1,\n",
              "        490,    5, 2370, 4888,  147, 1374,    1,   93, 2077, 7719, 1522,\n",
              "       4575,   83, 2911,   89, 1280,    2,  168,   96, 6767,    1,   83,\n",
              "       5157,    2,  354,    5,   12,   20,    7, 3287,   16,   93,    1,\n",
              "       2135,   32,    2,    1,  884,    3,   16,    1,    1,  242,   29,\n",
              "        340,   25,  410,   37, 5204, 4032,   47,    7,    4,  641, 3717,\n",
              "       4888,  147,    4, 1069, 4888,   61,  277,    2,  604,  305,  415,\n",
              "        141,    9,  233,   12,    8,    8,    2,  616,  709,    7,   28,\n",
              "          5,    2,   89,  816,    3, 1088,    9,  100,   20,   15,    2,\n",
              "       3365,    5,    2,    1,   24,  555,    9,  562, 1296,    3,    1,\n",
              "          7, 3420,    9,    2, 4158,    2,  606,    2,  128,   32,    2,\n",
              "       3548,   68,   27,  555,    2,  367,  320,    7,    5, 7719,    9,\n",
              "         25, 1265,   17,    2,  327,   21,   35,    2, 3518,    5,    2,\n",
              "       4283,    1,   17,    2,  327,   74,   24, 6054,   13,   29,  125,\n",
              "        311, 8128,   16,    2, 1288,  667,    1,   51,   25,  367, 1880,\n",
              "          7,  359,    6,   88,   29,    7, 9097,    1,    4,  129,   17,\n",
              "        594,  134,    7,   38,  741,    4,    1, 3882,    9,    2,    1,\n",
              "       6532,    2,  129,   29,   46,    6,  506,   14,    4, 4781, 4160,\n",
              "         13,   46,  782, 2016,    3, 3893,  314,  341,   17,    1,   84,\n",
              "       6767,    7,    1, 2678, 1404,   13,   24, 2878,   42,   32,  133,\n",
              "         84,    3,   29,  207,   69, 2340, 7719,  125,   22,  140,  625,\n",
              "        189,   29,    7, 2878,   21,    4, 1791,   17,  440, 1247,    3,\n",
              "         34,  216,  619,  440, 3534,  341,    2,   96,   34,  909, 7454,\n",
              "       4160,  989,    1,    1,   43,    2,  296, 7719,    1,   13,   44,\n",
              "          1,    7,   13,  949,   49,   99, 6767,   27,   38,   47,   24,\n",
              "        107,  136,   13, 2066,    1,    1,   83, 5157,    2, 5314, 1357,\n",
              "          2, 5053,    5, 1325,    1,    2,    1, 2789,    2,    1,  350,\n",
              "       2191, 8359,   43,    3,    2, 2600,    5,   25,  905, 1047,   51,\n",
              "         29, 4120,    2, 6767, 9848,   29,    7,    1,   32,    2, 1997,\n",
              "         84,    3,    4, 4758,    1, 2390, 3999,  299,    5,    1, 6767,\n",
              "        204,  244, 7719,  837,  682,   17,   88,    3,   25, 5300,  837,\n",
              "          6,  799,   49,   29,  143,   78, 6767,   21,    2,   80,  525,\n",
              "        488,    6,  726,   29,    7, 1429,    5,    2,  296,    3,  488,\n",
              "          6,  140,  176,   15,    4, 1614, 7719, 1165,   88,   17,    4,\n",
              "          1,  137,    9,    1,    4, 5742,    7, 9035,   17,  440,    1,\n",
              "         32,    2,   84,  282,   34,  961,   66, 1809,   46,   76,    1,\n",
              "        299,    5,  853, 7719,   34,    1,   88,   15,   66,  169,  776,\n",
              "       7719, 7118,    2, 1455,    3,  886,   94,    8,    8,    2,  634,\n",
              "        134,    7,  420, 3101,    2, 2610,   24,    1,  879,  138,   13,\n",
              "          7, 1350,    9,  969,   98,   11,   68,  104,   10, 9161,    3,\n",
              "       3450,    2,  920,    5,    2, 1306,    8,    8,    2,  114, 3343,\n",
              "          7, 1323,   17,  305,  395,   66,  517,   70,  107, 6565, 4338,\n",
              "         16,   48,  285,   19,   11,  102,   29, 5569,   25,  215,   15,\n",
              "          4, 4125, 7302,   36,    7, 3567,   54,   32,    2,  296,    8,\n",
              "          8,    2,  739,    3,    2,  570,   24, 2417,    3, 1689,    2,\n",
              "       1246,   16,    2,   20,   11,  236, 1548,   49, 9600,   34,  329,\n",
              "          9,   13, 4713,   38,  447,   51,    2,  898, 1834,   47,   26,\n",
              "         76,  107, 7112,    5,   12,   20,   19,   57,  522, 3160,  271,\n",
              "         37,    1, 5764,   51,   60, 2688,    6, 5726,  135,    4,  109,\n",
              "         17,    2,  168,  390,   21,    4,  573,  120,    7,    1,   38,\n",
              "         33, 2466,  126,  109, 1591,  722, 2132,   16,    2,  120,   38,\n",
              "       6590, 6669, 4338,    9, 4888,  147,  144,   36,   26,  108,    2,\n",
              "         18,  119,  135,   12,    7,  580])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6te6u1xoBo_N"
      },
      "source": [
        "### Padding and Trancating\n",
        "\n",
        "What you must know is that these reviews has different length. So we need to change them to have the same length by the use of:\n",
        "\n",
        "1. Padding\n",
        "* The sequences that has less than the defined legth of tokens are then padded either `pre` or `post` with `0`.\n",
        "\n",
        "2. Truncating\n",
        "\n",
        "* The sequences that has more than the defined legth of tokens are then trancated either `pre` or `post`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXi1wUOl3M35"
      },
      "source": [
        "max_tokens = 100\n",
        "padding = truncating = \"post\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcsnseBPDA-b"
      },
      "source": [
        "Now we then need to pad our sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmQcXrTQ3M1P"
      },
      "source": [
        "train_tokens_padded = pad_sequences(\n",
        "    train_tokens,\n",
        "    maxlen=max_tokens,\n",
        "    truncating=truncating,\n",
        "    padding=padding\n",
        ")\n",
        "\n",
        "test_tokens_padded = pad_sequences(\n",
        "    test_tokens,\n",
        "    maxlen=max_tokens,\n",
        "    truncating=truncating,\n",
        "    padding=padding\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqguoOm93Myj",
        "outputId": "00ee1e57-a7ec-4aa6-9d87-7b9c6acbb21d"
      },
      "source": [
        "train_tokens_padded.shape, test_tokens_padded.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 100), (25000, 100))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wH_yq5KDsTf"
      },
      "source": [
        "### Inverse map\n",
        "\n",
        "For some strange reason, the Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sPDQ0ZW3MwL"
      },
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnfLPqNBD7sK"
      },
      "source": [
        "Helper function that converts tokens to `strings` is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqqjHFks3Mtp"
      },
      "source": [
        "def tokens_to_string(tokens):\n",
        "  return \" \".join([inverse_map[token] for token in tokens if token != 0])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "bJjOOapr3MrA",
        "outputId": "0668d957-390c-4fa6-ea08-5c6ee533c856"
      },
      "source": [
        "x_train_text[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This movie is perfect for all the romantics in the world. John Ritter has never been better and has the best line in the movie! \"Sam\" hits close to home, is lovely to look at and so much fun to play along with. Ben Gazzara was an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met Arthur in my travels somewhere. All around, an excellent choice to pick up any evening.!:-)'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "0aS4Ldq63Mok",
        "outputId": "18647c43-63e3-4932-906c-2ba64ec67e19"
      },
      "source": [
        "tokens_to_string(train_tokens_padded[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"this movie is perfect for all the <oov> in the world john ritter has never been better and has the best line in the movie sam hits close to home is lovely to look at and so much fun to play along with ben <oov> was an excellent cast and easy to fall in love with i'm sure i've met arthur in my travels somewhere all around an excellent choice to pick up any evening\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlqH1V7LEiFc"
      },
      "source": [
        "### Creating an RNN (Recurrent Neural Network)\n",
        "\n",
        "The first layer that we are going to add to our RNN is the Embedding layer.\n",
        "\n",
        "\n",
        "### Embedding Layer\n",
        "\n",
        "This layer converts each integer-token into a vector of values. This is necessary because the integer-tokens may take on values between 0 and 10000 for a vocabulary of 10000 words. The RNN cannot work on values in such a wide range. The embedding-layer is trained as a part of the RNN and will learn to map words with similar semantic meanings to similar embedding-vectors.\n",
        "\n",
        "_The values of the embedding-vector will generally fall roughly between -1.0 and 1.0, although they may exceed these values somewhat._\n",
        "\n",
        "The size of the embedding-vector is typically selected between ``100-300``, but it seems to work reasonably well with small values for Sentiment Analysis.\n",
        "\n",
        "\n",
        "The following cell defines the parameters of our embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVLwhRMh3Ml6"
      },
      "source": [
        "embbeding_size = 100\n",
        "input_dim = len(tokenizer.word_index) # the number of words in the vocabulary\n",
        "input_length = max_tokens"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Y-FK4PGKwc"
      },
      "source": [
        "### GRU\n",
        "The second layer that we will add to our RNN model is the Gated Recurrent Unit Layer. \n",
        "> Because we are going to add a `GRU` as the layer that follows the `GRU` we are going to specify `return_sequences=True` this is because the `GRU` that follows expect sequences as it's input.\n",
        "\n",
        "\n",
        "### Dense\n",
        "\n",
        "We are going to add a Dense (fully connedted) layer after our third GRU. This layer will have a `sigmoid` activation function since it we are doing a binary classification.\n",
        "\n",
        "\n",
        "### Now let's build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ87OOPK3L48",
        "outputId": "0f782ce1-c904-4c99-bcf9-3af57e6816e5"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [keras.layers.Embedding(\n",
        "        input_dim=input_dim,\n",
        "        output_dim = embbeding_size,\n",
        "        input_length = input_length,\n",
        "        name=\"embedding_layer\"\n",
        "    ),\n",
        "    keras.layers.GRU(units=128, return_sequences=True),\n",
        "    keras.layers.GRU(units=256, return_sequences=True),\n",
        "    keras.layers.GRU(units=64),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")],\n",
        "    name=\"simple_model\"\n",
        ")\n",
        "\n",
        "# combiling the model\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy,\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"acc\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"simple_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_layer (Embedding)  (None, 100, 100)          12425300  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 100, 128)          88320     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 100, 256)          296448    \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 64)                61824     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 12,871,957\n",
            "Trainable params: 12,871,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLAj98IDIoRz"
      },
      "source": [
        "### Training the RNN model\n",
        "\n",
        "We are going to use `5%` of the training data for validation. We are going to specify the validation split during the `model.fit()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io_TsZpjGHVT",
        "outputId": "cb7c7c71-ceba-414b-b8e9-6cb802cc1657"
      },
      "source": [
        "model.fit(\n",
        "        train_tokens_padded, y_train,\n",
        "          validation_split=0.05, epochs=5,\n",
        "          batch_size=128,\n",
        "          verbose=1,\n",
        "          shuffle=True,\n",
        "          validation_batch_size=64\n",
        "          )"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "186/186 [==============================] - 23s 93ms/step - loss: 0.5832 - acc: 0.6886 - val_loss: 0.4623 - val_acc: 0.7824\n",
            "Epoch 2/5\n",
            "186/186 [==============================] - 16s 86ms/step - loss: 0.3810 - acc: 0.8408 - val_loss: 0.4191 - val_acc: 0.8080\n",
            "Epoch 3/5\n",
            "186/186 [==============================] - 16s 86ms/step - loss: 0.2934 - acc: 0.8867 - val_loss: 0.3582 - val_acc: 0.8256\n",
            "Epoch 4/5\n",
            "186/186 [==============================] - 16s 86ms/step - loss: 0.2501 - acc: 0.9053 - val_loss: 0.4111 - val_acc: 0.8304\n",
            "Epoch 5/5\n",
            "186/186 [==============================] - 16s 86ms/step - loss: 0.1821 - acc: 0.9353 - val_loss: 0.5284 - val_acc: 0.8120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f00a21f13d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTn2u8j0JJSV"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "To evaluate the mode we are going to call the `model.evaluate()` on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6ZYCpRPGHQC",
        "outputId": "78a05241-078c-45f2-fba6-00e53f4a4cd3"
      },
      "source": [
        "model.evaluate(test_tokens_padded, y_test, verbose=1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 23s 30ms/step - loss: 0.5473 - acc: 0.7945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5472753643989563, 0.79448002576828]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdF8bN3nKXe6"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "_What embedding essentially does is to group the words with simmilar meaning close to each other in the vector space._\n",
        "\n",
        "\n",
        "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
        "\n",
        "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
        "\n",
        "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k3RrMvEGHOO",
        "outputId": "38b6e284-0835-4421-8f31-36d3672a8af6"
      },
      "source": [
        "embedding_layer = model.get_layer(\"embedding_layer\")\n",
        "embedding_layer"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7f00a37f0b90>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5vZrtEuK_jc"
      },
      "source": [
        "Now we can get the embedding weights of the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPQmlJNdGHMG",
        "outputId": "9c48090d-99a6-4c50-842c-41101ef1b833"
      },
      "source": [
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "embedding_weights.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124253, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTaENjnLgAj"
      },
      "source": [
        "\n",
        "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXFrp2hWGHDk",
        "outputId": "8ef128fc-526d-41dc-d88b-490bdc531af6"
      },
      "source": [
        "token_good = tokenizer.word_index['good']\n",
        "token_good"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o5nxyAxLmsc"
      },
      "source": [
        "Let us also get the integer-token for the word 'great'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxL9Iws2GHBv",
        "outputId": "74cc9ec2-e2e7-4b5b-c29d-3db117c5f438"
      },
      "source": [
        "token_great = tokenizer.word_index['great']\n",
        "token_great"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUqWvK16LrKr"
      },
      "source": [
        "> _These integertokens may be far apart and will depend on the frequency of those words in the data-set._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88asSwpbGG-D",
        "outputId": "dea4a625-ebb9-4efd-e0e1-7be7939d3057"
      },
      "source": [
        "embedding_weights[token_good]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03654959,  0.05182431,  0.00789072, -0.02546148,  0.03894342,\n",
              "       -0.00170818,  0.00561844,  0.0036592 ,  0.0351015 ,  0.03506294,\n",
              "        0.00292486, -0.03541147,  0.04214965, -0.01096839,  0.03691989,\n",
              "        0.00019001,  0.00644179,  0.05432602,  0.02255417,  0.04553063,\n",
              "       -0.01186652,  0.02498116, -0.00444889,  0.01255342,  0.01616195,\n",
              "        0.01642016, -0.04517556, -0.03839105, -0.00937105,  0.01329357,\n",
              "        0.07752912, -0.01429281,  0.03701348, -0.05405187,  0.03863146,\n",
              "       -0.02035496, -0.02392949, -0.04286262, -0.02362911,  0.04718411,\n",
              "       -0.00326303, -0.02075995, -0.03899069, -0.0362411 ,  0.00220468,\n",
              "       -0.03179303,  0.03786547,  0.03486606, -0.02811677, -0.01869663,\n",
              "        0.01828381,  0.00058268,  0.00803954, -0.0439345 ,  0.00742137,\n",
              "        0.03557568, -0.00484395,  0.03125272,  0.04664703, -0.04787948,\n",
              "        0.08408724, -0.01070623,  0.0320828 ,  0.0153351 ,  0.01114408,\n",
              "        0.03950981,  0.04133573, -0.05306445, -0.0173699 , -0.00875648,\n",
              "        0.01615299, -0.03960554, -0.03970825, -0.01341593,  0.00937491,\n",
              "       -0.00156723,  0.13301599,  0.00286135, -0.03409098, -0.03746739,\n",
              "       -0.00914763, -0.0189708 , -0.02614978,  0.02206637, -0.00615162,\n",
              "        0.0320653 , -0.04443279, -0.01863335,  0.03414012,  0.0366745 ,\n",
              "        0.01041031,  0.03763455,  0.01070177,  0.02922474,  0.05181595,\n",
              "       -0.01728479, -0.0391528 ,  0.01111066, -0.00983775, -0.01040294],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuggitDbGG7k",
        "outputId": "0ef31f72-2874-46a3-ee7b-a26ac4d9d0e2"
      },
      "source": [
        "embedding_weights[token_great]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07881194,  0.0643089 , -0.02972414,  0.09887278,  0.1089631 ,\n",
              "        0.1030881 , -0.09217654, -0.09186198, -0.05157833,  0.02069101,\n",
              "        0.00117218,  0.00072236,  0.0785922 ,  0.02382322,  0.06888454,\n",
              "        0.09756953, -0.00853645,  0.01786042,  0.00340038,  0.0597227 ,\n",
              "       -0.0633194 , -0.0812819 , -0.05079522, -0.07104529,  0.05604706,\n",
              "       -0.03787503,  0.06226208,  0.0494625 ,  0.00696902,  0.08610889,\n",
              "       -0.00151188,  0.03483196,  0.03773798, -0.04069658, -0.02446506,\n",
              "        0.05908711, -0.01301472,  0.01599188, -0.01294837, -0.04162518,\n",
              "       -0.06321004,  0.02767151, -0.01786947, -0.00384987, -0.06131248,\n",
              "       -0.05199639, -0.09441877, -0.09548356, -0.09463506,  0.00136399,\n",
              "       -0.02894701, -0.03729963, -0.05165001, -0.02650168, -0.07028391,\n",
              "        0.06605057, -0.04024285, -0.04439946,  0.07065816, -0.05921933,\n",
              "        0.06593991, -0.07370889,  0.026708  ,  0.03967481, -0.01717189,\n",
              "        0.04019506, -0.1038826 , -0.01338773,  0.03761966,  0.03544791,\n",
              "        0.03601363, -0.03393395, -0.04904008, -0.03638316, -0.08248808,\n",
              "       -0.04460688, -0.01247024,  0.06551836,  0.04110968, -0.00052493,\n",
              "        0.07052065, -0.03368018, -0.0568098 ,  0.06229902,  0.0265083 ,\n",
              "        0.0532333 , -0.02493755, -0.04196456,  0.01781576,  0.05683623,\n",
              "       -0.02623402, -0.07685123, -0.01237139, -0.0451921 ,  0.10368146,\n",
              "        0.10226143,  0.05427776,  0.03921647,  0.00265698,  0.01810365],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUXKubKL5yD"
      },
      "source": [
        "### Soted words\n",
        "\n",
        "\n",
        "We can also sort all the words in the vocabulary according to their \"similarity\" in the embedding-space. We want to see if words that have similar embedding-vectors also have similar meanings.\n",
        "\n",
        "> _Similarity of embedding-vectors can be measured by different metrics, e.g. **Euclidean distance** or cosine distance._\n",
        "\n",
        "The following helper function do the calculation these distances and printing the words in sorted order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvsUt95wGG5n"
      },
      "source": [
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = embedding_weights[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(embedding_weights, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "    \n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "    \n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "    \n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for i, (word, distance) in enumerate(zip(words, distances)):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywx_Dec7MX6e"
      },
      "source": [
        "\n",
        "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFfcRIAAGG3o",
        "outputId": "b986943a-5c80-456d-e304-20ab1e723bf0"
      },
      "source": [
        "print_sorted_words('great', metric='cosine')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.172 - masterpiece\n",
            "0.182 - archive\n",
            "0.187 - ensemble\n",
            "0.191 - communication\n",
            "0.195 - malone\n",
            "0.196 - sergio\n",
            "0.196 - georges\n",
            "0.199 - jackson\n",
            "0.202 - nowadays\n",
            "...\n",
            "1.808 - quantum\n",
            "1.809 - waste\n",
            "1.809 - prom\n",
            "1.809 - limp\n",
            "1.811 - impersonation\n",
            "1.812 - uninteresting\n",
            "1.813 - murray\n",
            "1.814 - lousy\n",
            "1.825 - pink\n",
            "1.825 - tiresome\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVpfaSIWMfgz"
      },
      "source": [
        "We can also try the word `\"worst\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdCi6YAvGG0S",
        "outputId": "418b3e24-6dc2-4466-a8ce-77049db336c4"
      },
      "source": [
        "print_sorted_words('worst', metric='cosine')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.044 - irwin\n",
            "0.044 - disappointment\n",
            "0.047 - dreck\n",
            "0.048 - downhill\n",
            "0.050 - lifeless\n",
            "0.052 - prom\n",
            "0.053 - forgettable\n",
            "0.053 - clunky\n",
            "0.054 - nauseating\n",
            "...\n",
            "1.912 - popping\n",
            "1.912 - darker\n",
            "1.914 - steamy\n",
            "1.914 - contributed\n",
            "1.916 - communication\n",
            "1.916 - article\n",
            "1.917 - superbly\n",
            "1.918 - showdown\n",
            "1.918 - malone\n",
            "1.923 - shared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6SI_t-vNNBn"
      },
      "source": [
        "### Saving word embeddings.\n",
        "\n",
        "We are then going to save our word embeddings, we are going to use the `glove.6B` formart of saving their word embeddings as a `txt` file. The file will look as follows:\n",
        "\n",
        "```txt\n",
        "the  0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658\n",
        "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.4365\n",
        "....\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P829_j6hPno0"
      },
      "source": [
        "def save_word_embeddings(path, embedding_weights=None, tokenizer=None):\n",
        "  fileW = open(path, \"w\")\n",
        "  words = [word for word in tokenizer.word_index.keys() if word not in [\"<oov>\", \"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]]\n",
        "\n",
        "  for w in words:\n",
        "    idx = tokenizer.word_index[w]\n",
        "    try:\n",
        "      weight = embedding_weights[idx]\n",
        "    except:\n",
        "      pass\n",
        "    line = f\"{w} \" + \" \".join([ f\"{round(float(f), 5)}\" for f in weight])\n",
        "    fileW.write(f\"{line}\\n\")\n",
        "  fileW.close()\n",
        "  print(\"done\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKzJJDxKRryt",
        "outputId": "6163bc2b-6364-4ce0-f760-566bc15066d0"
      },
      "source": [
        "embedding_weights.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124253, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3RIflpyMjH2",
        "outputId": "07831d51-78c0-43a0-ba34-fe2be87d91cc"
      },
      "source": [
        "%%time\n",
        "\n",
        "save_word_embeddings(\"word-embeddings.100d.txt\", embedding_weights, tokenizer)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "CPU times: user 18.4 s, sys: 310 ms, total: 18.7 s\n",
            "Wall time: 18.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDvTPOkmO3ya"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    }
  ]
}