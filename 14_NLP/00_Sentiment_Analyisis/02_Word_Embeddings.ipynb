{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Word_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gGGRO1Cko8m"
      },
      "source": [
        "### Word Embeddings\n",
        "\n",
        "In this notebook we are going to learn how to create and work with word embedding vectors. We  are going to create a simple classification model based on the IMDB dataset and have a quick view on the word embeddings.\n",
        "\n",
        "\n",
        "\n",
        "> Since this notebook is more focused on trainning our own word embeddings instead of the model achitecture and other stuff. I will explain some of the code cell, some of them will be skipped\n",
        "\n",
        "\n",
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AsSpD1DgkdiN",
        "outputId": "6bcc4912-b735-40a0-baba-8571f41cf1a1"
      },
      "source": [
        "import os, re, shutil, string\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbQJ9vFslmbO"
      },
      "source": [
        "### Dataset \n",
        "\n",
        "We are going to use the `get_file` method from keras utilsto download the [IMDB movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) and we will load it using the `text_data_from_directory` based on the [this tutorial](https://www.tensorflow.org/tutorials/keras/text_classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UziPQSU4km0H",
        "outputId": "7b7ac20b-f1ab-4b44-f39d-fc37dbb43ae9"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n",
            "84140032/84125825 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['README', 'train', 'imdb.vocab', 'test', 'imdbEr.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pan2MrXemn_W"
      },
      "source": [
        "After theb above cell executed we are going to have the following folder structure:\n",
        "\n",
        "```\n",
        "acllmdb\n",
        "  test\n",
        "    pos\n",
        "    neg\n",
        "  train\n",
        "    pos\n",
        "    neg\n",
        "    unsup\n",
        "```\n",
        "\n",
        "We wantto remove the `unsup` directory in the train folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7bIXpZDkmxF"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "\n",
        "unsup_dir =os.path.join(train_dir, \"unsup\")\n",
        "if os.path.exists(unsup_dir):\n",
        "  shutil.rmtree(unsup_dir)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJtbAlmWkmuf"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "SEED = 42\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvQksl4hkmrb"
      },
      "source": [
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ELuOBZfkmoh",
        "outputId": "14eac85a-8adf-4374-dc4a-46bb3c1f202b"
      },
      "source": [
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=BATCH_SIZE, validation_split=0.2, \n",
        "    subset='training', seed=SEED)\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=BATCH_SIZE, validation_split=0.2, \n",
        "    subset='validation', seed=SEED)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwVXbnvcn_uM"
      },
      "source": [
        "### Checking some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j5PoSSxkmlr",
        "outputId": "02b0bbf2-6fe8-466e-85a8-95ac45be5929"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 b\"I first saw Rob Roy twelve years ago. With little money for entertainment, I rented it for my fianc\\xc3\\xa9 and I to watch on a bone chilling winter's night. The movie I had wanted was gone, so I rented this instead, not expecting much, and was very much surprised with how good it was. I just recently watched it again, and loved it every bit as much as the first time. <br /><br />For those unfamiliar with the story, it's about Scottish outlaw Robert Roy MacGregor, a cattleman and folk hero. From the little I know about the man and his story, liberties have been taken with the facts, but it's a movie, not a textbook, and so the filmmakers can be excused. Basically, the plot of the movie is that Rob Roy borrows money from the Marquis of Montrose to buy cattle which he then intends to sell and reap a large profit from. But, his plan is foiled when the friend entrusted with the money is robbed of the cash and murdered in the forest. Our hero finds himself on the run after failing to settle the matter with the Marquis, and Mary, his wife, suffers a sadistic rape at the hands of Archibald Cunningham, a smarmy Englishman with no soul. Atrocities ensue, until, in an immensely satisfying conclusion, Rob carves Archibald up like a Christmas turkey. <br /><br />There are many great performances in this movie, but allow me to touch specifically on a few. Liam Neeson, as usual, is fantastic, a sexy beast you can't take your eyes off of. Honestly, this man is like ice cream: even when he's bad he's good. His Rob Roy is an honourable man struggling to provide for those who depend on him, in the best way he knows how. Jessica Lange, as Mary, gives this woman a fierceness which is a nice change from the simpering, dull movie wives audiences are usually forced to endure. You just know she doesn't take any b.s from Rob, or anyone else for that matter. Tim Roth is completely over the top with his portrayal of the evil Archibald, yet somehow, it works. All the posturing and preening, combined with some wicked dialogue, result in one of the most memorable movie villains in recent memory. Combine all of this, and the stellar work by other supporting players, with the luscious scenery of Scotland, and you have what amounts to one really, really cool movie. If you haven't seen this, I highly recommend that you do.\"\n",
            "1 b\"Welcome back Kiefer Sutherland. it's been too long since you've appeared in a movie,, and what a movie this was, was it 24 no,, but very intriguing, especially with a pro like Michael Douglas in the lead as the embattled Secret Service Agent. Kiefer's character is the one chasing Michael Douglas the whole movie,, Kiefer's partner,, is Eva Longoria,, the Desperate houswife. wow she can actually act besides flirt all day and look good,, i wish though that Kim Bassinger had a bigger role,, but other than that, i really think the whole movie was a blast from start to finish. This movie is what i consider to b e a political thriller, everybody played their part to the hilt. nothing was revealed to sooon in the movie,, so as to keep you guessing at all times. and i really think that Kiefer did one heck of a job here in this movie,, but in my opinion Michael Douglas had the besxt performance of the day,, thumbs up.\"\n",
            "1 b'\"Tulip\" is on the \"Australian All Shorts\" video from \"Tribe First Rites\" showcasing the talents of first time directors.<br /><br />I wish more scripts had such excellent dialogue.<br /><br />I hope Rachel Griffiths has more stories to tell, she does it so well.'\n",
            "0 b'Well, on the endless quest for horror, we will come across this film, apparently re-released on DVD recently for some ungodly reason. The transfer is awful and the quality just sucks. I don\\'t think this is due to a bad remaster or anything, I just think the film is poorly done.<br /><br />Obviously filmed at an abandoned school with a budget that was no doubt wasted on cheap beer and no talent hacks, \"Slaughter High\" starts out slow and doesn\\'t pick up pace until about an hour in. First, we get to see a \\'nerd\\' as he is picked on by a group of...I actually don\\'t even know what they were supposed to be...jocks? The ringleader, with his ultra hooknose is so ugly he should have definitely been cast as the nerd. Then, there is a \\'big guy\\' and a couple of dumb losers and chicks who are supposed to be \\'hot\\' but aren\\'t. It\\'s a mystery why this group of rejects is picking on one of their own, but I guess the viewer is to assume these are \\'cool kids\\' picking on a dweeb. The casting choices are horrendous as most of the high schoolers are played by thirty-somethings. As other reviewers on here have pointed out, the actors (if you can call them that,) are a bunch of Brits whose accents slip out numerous times throughout this piece of crap. We are left to assume that this group of \\'children\\' were the only students at this school, as their \\'reunion\\' is only them at the school, which is now shown to be abandoned, is just them.<br /><br />The kills are lame, the gore is not great and the script is like Scooby Doo with real people; lines like: \\'This place gives me the creeps...\\' and \\'Someone gimme a beer\\' are highlights...It\\'s just not good. Skip this one unless you are getting wasted with some friends and wanna laugh at a real lame attempt at a slasher. If you wanna see good, get Bava\\'s \"Bay Of Blood,\" done 14 years earlier and a heck of a lot better. If you wanna see a good BAD slasher, see \"Just Before Dawn\" or \"The Burning.\" 2 out of 10, kids.'\n",
            "0 b'Although the beginning suggests All Quiet on the Western Front, this silly and superficial version of war falls far astray of its much better contemporary. This depicts the funnest war ever fought, with the first hour and a half devoted to romance and good times.<br /><br />When we finally see some battle, it is lame: An enemy plane flies over? Shoot it down (in one shot). Sniper in the tree? Kill him before he gets a shot off. Enemy soldiers in the woods? Not to worry, they gladly surrender. Ho-hum.<br /><br />Tepid, turgid, predictable...<br /><br />'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZByEuDIoIYN"
      },
      "source": [
        "### Configuring the dataset for performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r9kq6Amkmit"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao6GA86doqM9"
      },
      "source": [
        "### Text preprocessing\n",
        "\n",
        "Next, define the dataset preprocessing steps required for our sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. \n",
        "\n",
        "\n",
        "* [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci-wOFAkkmfr"
      },
      "source": [
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhoZZRl2kmcy"
      },
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 10000\n",
        "sequence_length = 100"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8PY5T7GkmZ4"
      },
      "source": [
        "# Use the text vectorization layer to normalize, split, and map strings to \n",
        "# integers. Note that the layer uses the custom standardization defined above. \n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjVFMdnukmXA",
        "outputId": "43a2cf03-3caa-437e-d69e-31a92c967f51"
      },
      "source": [
        "vectorize_layer.vocabulary_size()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TauKReype_G"
      },
      "source": [
        "### Creating a model.\n",
        "\n",
        "\n",
        "* The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) layer transforms strings into vocabulary indices. We have already initialized `vectorize_layer` as a TextVectorization layer and built it's vocabulary by calling `adapt` on `text_ds`. Now ``vectorize_layer`` can be used as the first layer of our end-to-end classification model, feeding tranformed strings into the Embedding layer.\n",
        "\n",
        "* The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
        "\n",
        "* The [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "* The fixed-length output vector is piped through a fully-connected ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer with 16 hidden units.\n",
        "\n",
        "* The last layer is densely connected with a single output node. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B1QHQvWkmUM"
      },
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  keras.layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  keras.layers.GlobalAveragePooling1D(),\n",
        "  keras.layers.Dense(16, activation='relu'),\n",
        "  keras.layers.Dense(1)\n",
        "], name=\"my_model\")\n",
        "\n",
        "# Compiling the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4W25FaTrSZE",
        "outputId": "fcb305bb-0605-4615-df2c-71764882a143"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization (TextVect (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHL3Sg8rEBP"
      },
      "source": [
        "### Training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl1S9LAwkmRO",
        "outputId": "18777add-4b18-45fd-c218-3537c1364301"
      },
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds, \n",
        "    epochs=15)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "20/20 [==============================] - 6s 149ms/step - loss: 0.6920 - accuracy: 0.5019 - val_loss: 0.6899 - val_accuracy: 0.4924\n",
            "Epoch 2/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.6861 - accuracy: 0.5019 - val_loss: 0.6828 - val_accuracy: 0.4924\n",
            "Epoch 3/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.6763 - accuracy: 0.5019 - val_loss: 0.6717 - val_accuracy: 0.4924\n",
            "Epoch 4/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.6609 - accuracy: 0.5019 - val_loss: 0.6551 - val_accuracy: 0.4924\n",
            "Epoch 5/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.6392 - accuracy: 0.5019 - val_loss: 0.6331 - val_accuracy: 0.4924\n",
            "Epoch 6/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.6116 - accuracy: 0.5076 - val_loss: 0.6064 - val_accuracy: 0.5266\n",
            "Epoch 7/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.5792 - accuracy: 0.5989 - val_loss: 0.5767 - val_accuracy: 0.6094\n",
            "Epoch 8/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.5443 - accuracy: 0.6777 - val_loss: 0.5463 - val_accuracy: 0.6678\n",
            "Epoch 9/15\n",
            "20/20 [==============================] - 1s 68ms/step - loss: 0.5092 - accuracy: 0.7301 - val_loss: 0.5172 - val_accuracy: 0.7090\n",
            "Epoch 10/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.4758 - accuracy: 0.7663 - val_loss: 0.4911 - val_accuracy: 0.7384\n",
            "Epoch 11/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.4454 - accuracy: 0.7926 - val_loss: 0.4686 - val_accuracy: 0.7574\n",
            "Epoch 12/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.4183 - accuracy: 0.8127 - val_loss: 0.4497 - val_accuracy: 0.7694\n",
            "Epoch 13/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.3944 - accuracy: 0.8267 - val_loss: 0.4342 - val_accuracy: 0.7828\n",
            "Epoch 14/15\n",
            "20/20 [==============================] - 1s 69ms/step - loss: 0.3734 - accuracy: 0.8394 - val_loss: 0.4215 - val_accuracy: 0.7904\n",
            "Epoch 15/15\n",
            "20/20 [==============================] - 1s 70ms/step - loss: 0.3549 - accuracy: 0.8492 - val_loss: 0.4111 - val_accuracy: 0.7972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0ed15a490>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS2Ys2DGrZfc"
      },
      "source": [
        "### Retrieving the trained word embeddings and save them to disk.\n",
        "\n",
        "Next we will retrieve the word embeddings lerand during model training and save them to disk. The weights matrix is of shape `(vocab_size, embedding_dimension)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE5eVdw_kmOH",
        "outputId": "c472952c-1c9f-4679-99d7-d29ca70ac8b6"
      },
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n",
        "print(vocab[:10])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHVKiy-kmLE"
      },
      "source": [
        "assert len(vocab) == vectorize_layer.vocabulary_size() == 10_000, \"failed equality\" "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgWnD-tasQY4"
      },
      "source": [
        "### Getting the embedding weigths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-JV6NoPkmF0",
        "outputId": "25b0b6bc-3f09-49a9-ad95-24865b9787ef"
      },
      "source": [
        "weights = model.get_layer(\"embedding\").get_weights()[0]\n",
        "weights.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0h-jpgkkmDO",
        "outputId": "0d64d9e6-6e3b-42a5-a610-9decc07eda04"
      },
      "source": [
        "weights[:2]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02068263,  0.01661886,  0.02658953, -0.03932418,  0.01821075,\n",
              "         0.1190527 ,  0.0630055 ,  0.09768607,  0.04270636, -0.01584527,\n",
              "        -0.01829978, -0.06380946, -0.01145105,  0.12682624, -0.01018622,\n",
              "         0.00402084],\n",
              "       [ 0.00280237,  0.06672003,  0.04118211, -0.00034019,  0.04540986,\n",
              "         0.13215236,  0.00456196,  0.03950495,  0.12480214, -0.02775864,\n",
              "         0.050193  , -0.19038193, -0.04517735,  0.07841194,  0.08379428,\n",
              "        -0.02369561]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNqXtjEZso51"
      },
      "source": [
        "### Using the embedding projector.\n",
        "We will write the weighs to the disk inorder for us to ues the [Embedding Projector](http://projector.tensorflow.org/). The filed will be in a tab separated formart `tsv`. These two files will:\n",
        "\n",
        "1.  a file of vectors (containing the embedding), \n",
        "2. file of meta data (containing the words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDTG0xlFkmAE",
        "outputId": "71fd6675-4702-4e0a-b640-c5f05746c2c6"
      },
      "source": [
        "out_v = open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(vocab):\n",
        "  if num == 0: continue # skipping the padding token from the vocab\n",
        "  vec = weights[num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "print(\"Done...\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CTvIY8-kl9M"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "8zmZ1qrDt7I7",
        "outputId": "e18bddbc-e435-466b-d097-e52c289ead19"
      },
      "source": [
        "files.download('vecs.tsv')\n",
        "files.download('meta.tsv')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_be4b79b2-9b59-43c1-b65f-f7027806f355\", \"vecs.tsv\", 1884883)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3fe14d16-069f-40a9-9d37-61104517dc5b\", \"meta.tsv\", 76436)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmYFk04yuRDd"
      },
      "source": [
        "### Visualize the embeddings\n",
        "\n",
        "To visualize the embeddings, upload them to the embedding projector.\n",
        "\n",
        "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
        "\n",
        "* Click on \"Load data\".\n",
        "\n",
        "* Upload the two files you created above: `vecs.tsv` and `meta.tsv`.\n",
        "\n",
        "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
        "\n",
        "Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.\n",
        "\n",
        "Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN-4dCkbvTrP"
      },
      "source": [
        "### Saving word embeddings in the way `glove.6B` does. \n",
        "\n",
        "We are going to save these word embeddings as we did in [at the end of this notebook](https://github.com/CrispenGari/keras-api/blob/main/14_NLP/00_Sentiment_Analyisis/00_Sentiment_Analysis_With_A_Closer_Look_Plus_Embeddings.ipynb) where the ``txt`` file of word vectors will be looking as follows:\n",
        "\n",
        "```txt\n",
        "the  0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658\n",
        "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.4365\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gK5hSUfpuBcE",
        "outputId": "02059e1d-f48e-494a-eb5c-e8460361df68"
      },
      "source": [
        "out_v = open('word-embeddings.16d.txt', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate(vocab):\n",
        "  if num == 0: continue # skipping the padding token from the vocab\n",
        "  vec = weights[num]\n",
        "  out_v.write(f\"{word} \")\n",
        "  out_v.write(' '.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "print(\"Done...\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "jA0j5CbtwTiG",
        "outputId": "5083ab57-c2f3-47a8-ea3a-ab87ff3631f8"
      },
      "source": [
        "files.download('word-embeddings.16d.txt')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c0be9295-01a7-4abd-ac3e-122fd5af8f56\", \"word-embeddings.16d.txt\", 1961319)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEd5g-FFwY_A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}