{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_NMT_Encoder_Decoder_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNqu7pFKchNr"
      },
      "source": [
        "### 13. Encoder Decoder Model With Attention- ``NMT``\n",
        "\n",
        "This notebook is just an exansion from what we did in the previous notebook. We are going to create an Encoder Decoder model with attention. The rest of the notebook will remain the same as in the previous notebook, we are going to tweek few things.\n",
        "\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I00tsWmRbxmL",
        "outputId": "86aff097-ef02-44a3-d93e-a7f5f45aebe2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "import re, os, time, unicodedata\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qv-N1-Mg_1O"
      },
      "source": [
        "### Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vbu4Gv8b4cY",
        "outputId": "120035f1-a7a6-40bb-8436-23196df6c1dd"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKh1Onbdb4Zr",
        "outputId": "2dd07ff5-ca93-4f23-a630-e64cb48f8e07"
      },
      "source": [
        "file_path = \"/content/drive/My Drive/NLP Data/seq2seq/spa-en/spa.txt\"\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvtnHUHxiYkG"
      },
      "source": [
        "We will use spainish as our source language and english as our target language in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSj--AajiXw6"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvIrHBQ8iXn4"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    # Adding the start and end of sequences tokens \n",
        "    # w = '<sos> ' + w + ' <eos>'\n",
        "    return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-WtkAejhssT"
      },
      "source": [
        "### Data and text processing.\n",
        "\n",
        "We need to remove accents, lower case the sentences and replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w3ZgSewN9TJ"
      },
      "source": [
        "INPUT_COLUMN = 'input'\n",
        "TARGET_COLUMN = 'target'\n",
        "\n",
        "TARGET_FOR_INPUT = 'target_for_input'\n",
        "NUM_SAMPLES = 20000 #40000\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM= 1024\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 10  # Number of epochs to train for."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5BELCFLb4W9",
        "outputId": "b3113901-599d-4c8c-fc94-72c83486491a"
      },
      "source": [
        "# Load the dataset: sentence in english, sentence in spanish \n",
        "df=pd.read_csv(file_path, sep=\"\\t\", header=None, names=[TARGET_COLUMN, INPUT_COLUMN], usecols=[0,1], \n",
        "               nrows=NUM_SAMPLES)\n",
        "\n",
        "# Preprocess the input data\n",
        "input_data=df[INPUT_COLUMN].apply(lambda x : preprocess_sentence(x)).tolist()\n",
        "# Preprocess and include the end of sentence token to the target text\n",
        "target_data=df[TARGET_COLUMN].apply(lambda x : preprocess_sentence(x)+ ' <eos>').tolist()\n",
        "# Preprocess and include a start of setence token to the input text to the decoder, it is rigth shifted\n",
        "target_input_data=df[TARGET_COLUMN].apply(lambda x : '<sos> '+ preprocess_sentence(x)).tolist()\n",
        "\n",
        "print(input_data[:5])\n",
        "print(target_data[:5])\n",
        "print(target_input_data[:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ve .', 'vete .', 'vaya .', 'vayase .', 'hola .']\n",
            "['go . <eos>', 'go . <eos>', 'go . <eos>', 'go . <eos>', 'hi . <eos>']\n",
            "['<sos> go .', '<sos> go .', '<sos> go .', '<sos> go .', '<sos> hi .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3wBerjHlezP"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "* Tokenize the data, to convert the raw text into a sequence of integers. First, we create a Tokenizer object from the keras library and fit it to our text (one tokenizer for the input and another one for the output).\n",
        "* Extract sequence of integers from the text: we call the ``text_to_sequence`` method of the tokenizer for every input and output text.\n",
        "* Calculate the maximum length of the input and output sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtVRfcb0b4RY",
        "outputId": "4745632b-0cf2-451e-a7aa-b1e7b02fa074"
      },
      "source": [
        "tokenizer_inputs = Tokenizer(\n",
        "    num_words = MAX_VOCAB_SIZE, filters=\"\"\n",
        ")\n",
        "tokenizer_inputs.fit_on_texts(input_data)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n",
        "\n",
        "input_max_len = max(len(s) for s in input_sequences)\n",
        "print('max input length: ', input_max_len)\n",
        "\n",
        "# Show some example of tokenize sentences, useful to check the tokenization\n",
        "print(input_data[1000])\n",
        "print(input_sequences[1000])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max input length:  17\n",
            "tomas lo intento .\n",
            "[60, 15, 765, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pExrjo17neVm"
      },
      "source": [
        "We can do the same thing to the output sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leFgqUS7b4N1",
        "outputId": "499cfacb-48ce-44f3-e56e-3a51ee51fd9f"
      },
      "source": [
        "\n",
        "# Create a tokenizer for the output texts and fit it to them \n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_data)\n",
        "tokenizer_outputs.fit_on_texts(target_input_data)\n",
        "\n",
        "# Tokenize and transform output texts to sequence of integers\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n",
        "\n",
        "# determine maximum length output sequence\n",
        "target_max_len = max(len(s) for s in target_sequences)\n",
        "print('max target length: ', target_max_len)\n",
        "\n",
        "print(target_data[1000])\n",
        "print(target_sequences[1000])\n",
        "print(target_input_data[1000])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max target length:  9\n",
            "tom tried . <eos>\n",
            "[7, 414, 1, 2]\n",
            "<sos> tom tried .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQj3S3HNoH-_"
      },
      "source": [
        "### Creating vocabularies\n",
        "\n",
        "Using the tokenizer we have created previously we can retrieve the vocabularies, one to match word to integer (word2idx) and a second one to match the integer to the corresponding word (idx2word).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjqBg7-vb4K8",
        "outputId": "474f5639-4870-45bf-8245-3f9d87066520"
      },
      "source": [
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output and input words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7183 unique input tokens.\n",
            "Found 3669 unique output tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvJFF6hjovy5"
      },
      "source": [
        "### Padding Sequences\n",
        "Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQrBk3Z_b4Hp",
        "outputId": "cdbc5425-eedd-4e5f-a98f-738063ce5425"
      },
      "source": [
        "\n",
        "# pad the input sequences\n",
        "encoder_inputs = pad_sequences(input_sequences,\n",
        "                               maxlen=input_max_len, padding='post', truncating=\"post\")\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "\n",
        "\n",
        "# pad the decoder input sequences\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "\n",
        "# pad the target output sequences\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')\n",
        "print(\"decoder_targets.shape:\", decoder_targets.shape)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_inputs.shape: (20000, 17)\n",
            "decoder_inputs[0]: [ 3 31  1  0  0  0  0  0  0]\n",
            "decoder_inputs.shape: (20000, 9)\n",
            "decoder_targets.shape: (20000, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A3y_CaXrsre"
      },
      "source": [
        "### Creating a Batch Data Generator\n",
        "\n",
        "* Create a batch data generator: we want to train the model on batches, group of sentences, so we need to create a Dataset using the tf.data library and the function ``batch_on_slices`` on the input and output sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fyoKhylb4FE"
      },
      "source": [
        "BUFFER_SIZE = len(input_data)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (encoder_inputs, decoder_inputs, decoder_targets)\n",
        ").shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLluMo0sSjf"
      },
      "source": [
        "### Encoder Decoder Model. (This will remain the same as from the prevoius notebook)\n",
        "\n",
        "For a better understanding, we can divide the model in three basic components:\n",
        "\n",
        "![img](https://github.com/edumunozsala/NMT-encoder-decoder-Attention/raw/ca7d7f969a17ddf390f707fceb22d5881d33b1c3/images/encoder_decoder_RNN.jpeg)\n",
        "\n",
        "*  **The encoder:** Layers of recurrent units where in each time step, receive a an input token, collects relevant information and produce a hidden state. Depends on the type of RNN, in our example a LSTM, the unit \"mixes\" the current hidden state and the input and return an output, discarded, and a new hidden state. \n",
        "\n",
        "* **The encoder vector:** it is the last hidden state of the encoder and it tries to contain as much of the useful input information as possible to help the decoder get the best results. It is only information from the input that the decoder will get.\n",
        "\n",
        "* **The decoder:** Layers of recurrent units, i.e. LSTMs, where each unit produces an output at a time step t. The hidden state of the first unit is the encoder vector and the rest of units accept the hidden state from the previous unit. The output is calculated using a **``_softmax_``** function to obtain a probability for every token in the output vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Encoder Class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-J_Ho3lb4Cl"
      },
      "source": [
        "class Encoder(keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim\n",
        "    )\n",
        "    self.lstm = keras.layers.LSTM(hidden_dim,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True\n",
        "                                  )\n",
        "    \n",
        "  def call(self, input_sequence, states):\n",
        "    embedded  = self.embedding(input_sequence)\n",
        "    output, h_0, c_0 = self.lstm(embedded, initial_state= states)\n",
        "    return output, h_0, c_0 \n",
        "\n",
        "  def init_states(self, batch_size):\n",
        "    return(\n",
        "        tf.zeros([batch_size, self.hidden_dim]),\n",
        "        tf.zeros([batch_size, self.hidden_dim]),\n",
        "    )\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0z3R4Js7ikS"
      },
      "source": [
        "### The Attention Mechanism\n",
        "The previously described model based on RNNs has a serious problem when working with long sequences, because the information of the first tokens is lost or diluted as more tokens are processed. The context vector has been given the responsibility of encoding all the information in a given source sentence in to a vector of few hundred elements. it made it challenging for the models to deal with long sentences. A solution was proposed in Bahdanau et al., 2014 [4] and Luong et al., 2015,[5].\n",
        "\n",
        "They introduce a technique called **_\"Attention\"_**, which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed, accessing to all the past hidden states of the encoder, instead of just the last one. At each decoding step, the decoder gets to look at any particular state of the encoder and can selectively pick out specific elements from that sequence to produce the output. We will focus on the Luong perspective.\n",
        "\n",
        "### Loung Attention layer\n",
        "![img](https://github.com/edumunozsala/NMT-encoder-decoder-Attention/raw/ca7d7f969a17ddf390f707fceb22d5881d33b1c3/images/luong_attention.PNG)\n",
        "\n",
        "There are two relevant points to focus on:\n",
        "\n",
        "* **The alignment vector:** is a vector with the same length that the input or source sequence and is computed at every time step of the decoder. Each of its values is the score (or the probability) of the corresponding word within the source sequence, they tell the decoder what to focus on at each time step.\n",
        "\n",
        "     There are three ways to calculate the alingment scores:\n",
        "\n",
        "  * **Dot product:** we only need to take the hidden states of the encoder and multiply them by the hidden state of the decoder\n",
        "  * **General:** very similar to dot product but a weight matrix is included.\n",
        "  * **Concat:** the decoder hidden state and encoder hidden states are added together first before being passed through a Linear layer with an tanh activation function and finally multiply by a weight matrix.\n",
        "\n",
        "  ![img](https://github.com/edumunozsala/NMT-encoder-decoder-Attention/raw/ca7d7f969a17ddf390f707fceb22d5881d33b1c3/images/formula_luong_attention.PNG)\n",
        "\n",
        "  The alignment scores are softmaxed so that the weights will be between 0 to 1.\n",
        "\n",
        "* **The context vector:** It's the weighted average sum of the encoder's output, the dot product of the alignment vector and the encoder's output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5xcuLb58wP_"
      },
      "source": [
        "class LuongAttention(keras.Model):\n",
        "  def __init__(self, rnn_size, attention_func):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.attention_func = attention_func\n",
        "    if attention_func not in ['dot', 'general', 'concat']:\n",
        "      raise ValueError(\n",
        "        'Attention score must be either dot, general or concat but got %s.' % attention_func  \n",
        "      )\n",
        "    if attention_func == 'general':\n",
        "      # General score function\n",
        "      self.wa = keras.layers.Dense(rnn_size)\n",
        "    elif attention_func == 'concat':\n",
        "      self.wa = keras.layers.Dense(rnn_size, activation='tanh')\n",
        "      self.va = keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, decoder_output, encoder_output):\n",
        "    if self.attention_func == 'dot':\n",
        "      score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n",
        "    elif self.attention_func == 'general':\n",
        "      score = tf.matmul(decoder_output, self.wa(\n",
        "                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n",
        "    elif self.attention_func == 'concat':\n",
        "      decoder_output = tf.tile(\n",
        "                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n",
        "      score = self.va(\n",
        "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n",
        "      score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n",
        "    \n",
        "    alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n",
        "    context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n",
        "    return context, alignment\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCt4dFwSulYv"
      },
      "source": [
        "### Decoder With Attention\n",
        "Once our Attention Class has been defined, we can create the decoder. The complete sequence of steps when calling the decoder are:\n",
        "\n",
        "* Generate the encoder hidden states as usual, one for every input token\n",
        "* Apply a RNN to produce a new hidden state, taking its previous hidden state and the target output from the previous time step\n",
        "* Calculate the alignment scores as described previously\n",
        "* Calculate the context vector\n",
        "* In the last operation, the context vector is concatenated with the decoder hidden state we generated previously, then it is passed through a linear layer which acts as a classifier for us to obtain the probability scores of the next predicted word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdoT9kcxb3_H"
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim,\n",
        "               hidden_dim, attention_func):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.attention = LuongAttention(hidden_dim, attention_func)\n",
        "    \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embedding = keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim\n",
        "    )\n",
        "    self.lstm = keras.layers.LSTM(\n",
        "         hidden_dim, return_sequences=True, return_state=True\n",
        "    )\n",
        "\n",
        "    self.wc = keras.layers.Dense(hidden_dim, activation='tanh')\n",
        "    self.out = keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, input_sequence, state, encoder_output):\n",
        "    # Remember that the input to the decoder\n",
        "    # is now a batch of one-word sequences,\n",
        "    # which means that its shape is (batch_size, 1)\n",
        "    embedded = self.embedding(input_sequence)\n",
        "    output, h_0, c_0 = self.lstm(embedded, initial_state=state)\n",
        "    # Use self.attention to compute the context and alignment vectors\n",
        "    # context vector's shape: (batch_size, 1, hidden_dim)\n",
        "    # alignment vector's shape: (batch_size, 1, source_length)\n",
        "    context, alignment = self.attention(output, encoder_output)\n",
        "    # Combine the context vector and the LSTM output\n",
        "    # Before combined, both have shape of (batch_size, 1, hidden_dim),\n",
        "    # so let's squeeze the axis 1 first\n",
        "    # After combined, it will have shape of (batch_size, 2 * hidden_dim)\n",
        "    output =  tf.concat(\n",
        "            [tf.squeeze(context, 1), tf.squeeze(output, 1)], 1)\n",
        "    # output now has shape (batch_size, hidden_dim)\n",
        "    output = self.wc(output)\n",
        "    # Finally, it is converted back to vocabulary space: (batch_size, vocab_size\n",
        "\n",
        "    logits = self.out(output)\n",
        "    return logits, h_0, c_0, alignment\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_SjwivHvtt-"
      },
      "source": [
        "For testing purposes, we create a decoder and call it to check the output shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrv3WeKkb38d"
      },
      "source": [
        "#Set the length of the input and output vocabulary\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "\n",
        "ATTENTION_FUNC = \"general\"\n",
        "\n",
        "#Create the encoder\n",
        "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM, ATTENTION_FUNC)\n",
        "\n",
        "# Call the encoder and then the decoder\n",
        "initial_state = encoder.init_states(1)\n",
        "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
        "decoder_outputs = decoder(tf.constant(\n",
        "    [[1]]), encoder_outputs[1:], encoder_outputs[0])\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUkP0AswI-H"
      },
      "source": [
        "### Loss function and metrics\n",
        "\n",
        "The loss functions and metrics will remain unchanged from the previous notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB3JtQs9b35T"
      },
      "source": [
        "def loss_func(targets, logits):\n",
        "  crossentropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "  # Mask padding values, they do not have to compute for loss\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64) \n",
        "  loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "  return loss\n",
        "     "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN2HT0xZb32W"
      },
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  # y_pred shape is batch_size, seq length, vocab size\n",
        "  # y_true shape is batch_size, seq length\n",
        "  pred_values = keras.backend.cast(keras.backend.argmax(y_pred, axis=-1), dtype='int32')\n",
        "  correct = keras.backend.cast(keras.backend.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "  # 0 is padding, don't include those\n",
        "  mask = keras.backend.cast(keras.backend.greater(y_true, 0),\n",
        "                            dtype='float32')\n",
        "  n_correct = keras.backend.sum(mask * correct)\n",
        "  n_total = keras.backend.sum(mask)\n",
        "  return n_correct / n_total\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxaF6g9xMNH"
      },
      "source": [
        "### Training\n",
        "\n",
        "Now we can define our step train function, to train a batch data. It is very similar to the one we coded for the previous notebook model without attention but this time we pass all the hidden states returned by the encoder to the decoder. And we need to create a loop to iterate through the target sequences, calling the decoder for each one and calculating the loss function comparing the decoder output to the expected target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uABOj_7b3zS"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "    loss = 0.\n",
        "    acc = 0.\n",
        "    logits = None\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "        en_states = en_outputs[1:]\n",
        "        de_state_h, de_state_c = en_states\n",
        "\n",
        "        # We need to create a loop to iterate through the target sequences\n",
        "        for i in range(target_seq_out.shape[1]):\n",
        "            # Input to the decoder must have shape of (batch_size, length)\n",
        "            # so we need to expand one dimension\n",
        "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "            logit, de_state_h, de_state_c, _ = decoder(\n",
        "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "\n",
        "            # The loss is now accumulated through the whole batch\n",
        "            loss += loss_func(target_seq_out[:, i], logit)\n",
        "            # Store the logits to calculate the accuracy\n",
        "            logit = keras.backend.expand_dims(logit, axis=1)\n",
        "            if logits is None:\n",
        "                logits = logit\n",
        "            else:\n",
        "                logits = keras.backend.concatenate((logits,logit), axis=1)\n",
        "        # Calculate the accuracy for the batch data        \n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "    # Update the parameters and the optimizer\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss / target_seq_out.shape[1], acc\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3KOGX3oyrbi"
      },
      "source": [
        "Our train function receives three sequences:\n",
        "\n",
        "**Input sequence:** array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the encoder.\n",
        "\n",
        "**target sequence:** array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the target of our model, the output that we want for our model.\n",
        "\n",
        "**Target input sequence:** array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the decoder because we use Teacher Forcing.\n",
        "\n",
        "\n",
        "### Oh Teacher Forcing\n",
        "Teacher forcing is a training method critical to the development of deep learning models in NLP. It is a way for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input.\n",
        "\n",
        "In a recurrent network usually the input to a RNN at the time step t is the output of the RNN in the previous time step, t-1. But with teacher forcing we can use the actual output to improve the learning capabilities of the model.\n",
        "\n",
        "_\"Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network. So, in our example, the input to the decoder is the target sequence right-shifted, the target output at time step t is the decoder input at time step t+1.\"_\n",
        "\n",
        "When our model output do not vary from what was seen by the model during training, teacher forcing is very effective. But if we need a more \"creative\" model, where given an input sequence there can be several possible outputs, we should avoid this technique or apply it randomly (only in some random time steps).\n",
        "\n",
        "Now, we can code the whole training process:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmrvv4ehb3wQ"
      },
      "source": [
        "def fit(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
        "    \n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # Get the initial time\n",
        "        start = time.time()\n",
        "        # Get the initial state for the encoder\n",
        "        en_initial_states = encoder.init_states(batch_size)\n",
        "        # For every batch data\n",
        "        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "            # Train and get the loss value \n",
        "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
        "        \n",
        "            if batch % 100 == 0:\n",
        "                # Store the loss and accuracy values\n",
        "                losses.append(loss)\n",
        "                accuracies.append(accuracy)\n",
        "                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))\n",
        "                \n",
        "        # saving (checkpoint) the model every 2 epochs\n",
        "        if (e + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
        "        \n",
        "    return losses, accuracies"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uawlTlLI0mrP"
      },
      "source": [
        "We are almost ready, our last step include a call to the main train function and we create a checkpoint object to save our model. Because the training process require a long time to run, every two epochs we save it. Later we can restore it and use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skD0n1Edb3tC",
        "outputId": "620b85c2-0e55-4fd7-84a7-c8eaef69b39c"
      },
      "source": [
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq_att'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "losses, accuracies = fit(encoder, decoder, dataset, \n",
        "                                EPOCHS, BATCH_SIZE, \n",
        "                         optimizer, checkpoint, checkpoint_prefix)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.1443 Acc:0.0000\n",
            "Epoch 1 Batch 100 Loss 2.1913 Acc:0.4190\n",
            "Epoch 1 Batch 200 Loss 1.9849 Acc:0.4504\n",
            "Epoch 1 Batch 300 Loss 1.6921 Acc:0.5179\n",
            "Time taken for 1 epoch 79.6839 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.6905 Acc:0.5432\n",
            "Epoch 2 Batch 100 Loss 1.5678 Acc:0.5571\n",
            "Epoch 2 Batch 200 Loss 1.5056 Acc:0.5577\n",
            "Epoch 2 Batch 300 Loss 1.4290 Acc:0.6108\n",
            "Time taken for 1 epoch 48.3789 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.3067 Acc:0.6029\n",
            "Epoch 3 Batch 100 Loss 1.2139 Acc:0.6575\n",
            "Epoch 3 Batch 200 Loss 1.1159 Acc:0.6800\n",
            "Epoch 3 Batch 300 Loss 1.0038 Acc:0.6474\n",
            "Time taken for 1 epoch 47.6628 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8628 Acc:0.6962\n",
            "Epoch 4 Batch 100 Loss 0.8903 Acc:0.7151\n",
            "Epoch 4 Batch 200 Loss 0.8578 Acc:0.7083\n",
            "Epoch 4 Batch 300 Loss 0.7999 Acc:0.7598\n",
            "Time taken for 1 epoch 48.1545 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5670 Acc:0.7879\n",
            "Epoch 5 Batch 100 Loss 0.5863 Acc:0.7626\n",
            "Epoch 5 Batch 200 Loss 0.4846 Acc:0.8319\n",
            "Epoch 5 Batch 300 Loss 0.5071 Acc:0.8147\n",
            "Time taken for 1 epoch 47.6263 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4061 Acc:0.8481\n",
            "Epoch 6 Batch 100 Loss 0.3999 Acc:0.8431\n",
            "Epoch 6 Batch 200 Loss 0.4162 Acc:0.8237\n",
            "Epoch 6 Batch 300 Loss 0.3642 Acc:0.8567\n",
            "Time taken for 1 epoch 48.0286 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3198 Acc:0.8587\n",
            "Epoch 7 Batch 100 Loss 0.2745 Acc:0.8920\n",
            "Epoch 7 Batch 200 Loss 0.2847 Acc:0.8746\n",
            "Epoch 7 Batch 300 Loss 0.2298 Acc:0.9034\n",
            "Time taken for 1 epoch 47.6693 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2072 Acc:0.9037\n",
            "Epoch 8 Batch 100 Loss 0.2304 Acc:0.8968\n",
            "Epoch 8 Batch 200 Loss 0.3117 Acc:0.8959\n",
            "Epoch 8 Batch 300 Loss 0.2697 Acc:0.8857\n",
            "Time taken for 1 epoch 48.0860 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1852 Acc:0.9239\n",
            "Epoch 9 Batch 100 Loss 0.1961 Acc:0.9155\n",
            "Epoch 9 Batch 200 Loss 0.1546 Acc:0.9270\n",
            "Epoch 9 Batch 300 Loss 0.2769 Acc:0.8839\n",
            "Time taken for 1 epoch 47.5320 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1293 Acc:0.9298\n",
            "Epoch 10 Batch 100 Loss 0.1324 Acc:0.9460\n",
            "Epoch 10 Batch 200 Loss 0.1247 Acc:0.9605\n",
            "Epoch 10 Batch 300 Loss 0.1803 Acc:0.9136\n",
            "Time taken for 1 epoch 47.9130 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HR9d8ea08vv"
      },
      "source": [
        "### Model Evaulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCh6uXGhb3oH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4ac0e7f5-2293-4bde-9b46-ebd2bcdc9b2f"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "\n",
        "ax1.plot(losses, label='loss')\n",
        "#plt.plot(results.history['val_loss'], label='val_loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "# accuracies\n",
        "ax2.plot(accuracies, label='acc')\n",
        "#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\n",
        "ax2.set_title('Training Accuracy')\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iojOKHp1I4c"
      },
      "source": [
        "### Inference\n",
        "To restore the lastest checkpoint, saved model, you can run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDwXcqIwb3k5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323d8c7a-cd1d-47b8-e24c-4296e6437e25"
      },
      "source": [
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq_att'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3efde46250>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To67sEZr1RdX"
      },
      "source": [
        "In the prediction step, our input is a sequence of length one, the ``sos`` token, then we call the encoder and ``decoder`` repeatedly until we get the eos token or reach the maximum length defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYg6__z0b3iG"
      },
      "source": [
        "def predict(input_text, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n",
        "    if input_text is None:\n",
        "        input_text = input_data[np.random.choice(len(input_data))]\n",
        "    print(input_text)\n",
        "    # Tokenize the input text\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
        "    # Pad the sentence\n",
        "    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n",
        "    # Get the encoder initial states\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    # Get the encoder outputs or hidden states\n",
        "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
        "    # Set the decoder input to the sos token\n",
        "    de_input = tf.constant([[word2idx_outputs['<sos>']]])\n",
        "    # Set the initial hidden states of the decoder to the hidden states of the encoder\n",
        "    de_state_h, de_state_c = en_outputs[1:]\n",
        "    \n",
        "    out_words = []\n",
        "    alignments = []\n",
        "\n",
        "    while True:\n",
        "        # Get the decoder with attention output\n",
        "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
        "            de_input, (de_state_h, de_state_c), en_outputs[0])\n",
        "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "        # Detokenize the output\n",
        "        out_words.append(idx2word_outputs[de_input.numpy()[0][0]])\n",
        "        # Save the aligment matrix\n",
        "        alignments.append(alignment.numpy())\n",
        "\n",
        "        if out_words[-1] == '<eos>' or len(out_words) >= 20:\n",
        "            break\n",
        "    # Join the output words\n",
        "    # print(' '.join(out_words))\n",
        "    return np.array(alignments), input_text.split(' '), out_words\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyuUvqSfb3fG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "0c5ccce4-c4ee-4796-830c-b3a17718ac7d"
      },
      "source": [
        "\n",
        "n_predictions=1\n",
        "test_sents = input_data[1200:(1200+n_predictions)]\n",
        "print(target_data[1200])\n",
        "\n",
        "# Create the figure to plot in\n",
        "fig = plt.figure(figsize=(10, 20))\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "  # Call the predict function to get the translation\n",
        "  alignments, source, prediction = predict(test_sent, input_max_len, tokenizer_inputs, \n",
        "                                                    word2idx_outputs, idx2word_outputs)\n",
        "  attention = np.squeeze(alignments, (1, 2))\n",
        "  # Create a subplot\n",
        "  ax = fig.add_subplot(1, n_predictions, i+1)\n",
        "  ax.matshow(attention[:len(prediction), :len(source)], cmap='viridis')\n",
        "  ax.set_xticklabels([''] + source, rotation=90)\n",
        "  ax.set_yticklabels([''] + prediction)\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he ate out . <eos>\n",
            "el salio a comer .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZTCZsLLSARA",
        "outputId": "596b0b54-3dd4-41e9-bb6a-5edf1485e8b7"
      },
      "source": [
        "\n",
        "for inp, trg in zip(input_data[:10], target_data[:10]):\n",
        "  alignments, source, prediction = predict(inp, input_max_len, tokenizer_inputs, \n",
        "                                                      word2idx_outputs, idx2word_outputs)\n",
        "  print(\"input: > \", inp)\n",
        "  print(\"target: > \", trg.replace(\"<eos>\", \"\"))\n",
        "  print(\"predicted: = \", \" \".join(prediction).replace(\"<eos>\", \"\"))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ve .\n",
            "input: >  ve .\n",
            "target: >  go . \n",
            "predicted: =  that s amazing . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vete .\n",
            "input: >  vete .\n",
            "target: >  go . \n",
            "predicted: =  go . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vaya .\n",
            "input: >  vaya .\n",
            "target: >  go . \n",
            "predicted: =  go . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vayase .\n",
            "input: >  vayase .\n",
            "target: >  go . \n",
            "predicted: =  go on . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "hola .\n",
            "input: >  hola .\n",
            "target: >  hi . \n",
            "predicted: =  hi . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "corre !\n",
            "input: >  corre !\n",
            "target: >  run ! \n",
            "predicted: =  run ! \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "corran !\n",
            "input: >  corran !\n",
            "target: >  run ! \n",
            "predicted: =  run ! \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "corra !\n",
            "input: >  corra !\n",
            "target: >  run ! \n",
            "predicted: =  run ! \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "corred !\n",
            "input: >  corred !\n",
            "target: >  run ! \n",
            "predicted: =  run ! \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "corred .\n",
            "input: >  corred .\n",
            "target: >  run . \n",
            "predicted: =  run . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y5eLX_2S4B_",
        "outputId": "508257a8-ab12-465b-c450-ac897e7e2661"
      },
      "source": [
        "for inp, trg in zip(input_data[2210:2220], target_data[2210:2220]):\n",
        "  alignments, source, prediction = predict(inp, input_max_len, tokenizer_inputs, \n",
        "                                                      word2idx_outputs, idx2word_outputs)\n",
        "  print(\"input: > \", inp)\n",
        "  print(\"target: > \", trg.replace(\"<eos>\", \"\"))\n",
        "  print(\"predicted: = \", \" \".join(prediction).replace(\"<eos>\", \"\"))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dije que pararas .\n",
            "input: >  dije que pararas .\n",
            "target: >  i said stop . \n",
            "predicted: =  i said stop . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "dije que te detuvieras .\n",
            "input: >  dije que te detuvieras .\n",
            "target: >  i said stop . \n",
            "predicted: =  i said stop . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "yo dije eso .\n",
            "input: >  yo dije eso .\n",
            "target: >  i said that . \n",
            "predicted: =  i said that . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "yo vi un ovni .\n",
            "input: >  yo vi un ovni .\n",
            "target: >  i saw a ufo . \n",
            "predicted: =  i saw a ufo . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vi a un perro .\n",
            "input: >  vi a un perro .\n",
            "target: >  i saw a dog . \n",
            "predicted: =  i saw a dog . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vi un perro .\n",
            "input: >  vi un perro .\n",
            "target: >  i saw a dog . \n",
            "predicted: =  i saw a dog . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "yo vi un perro .\n",
            "input: >  yo vi un perro .\n",
            "target: >  i saw a dog . \n",
            "predicted: =  i saw a dog . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "vendo autos .\n",
            "input: >  vendo autos .\n",
            "target: >  i sell cars . \n",
            "predicted: =  i m selling . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "me deberia ir .\n",
            "input: >  me deberia ir .\n",
            "target: >  i should go . \n",
            "predicted: =  i should go out . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "huelo a gas .\n",
            "input: >  huelo a gas .\n",
            "target: >  i smell gas . \n",
            "predicted: =  i smell gas . \n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjaSpMz4XjP"
      },
      "source": [
        "### Credits\n",
        "\n",
        "As we can see the encoder decoder model with Attention mechanisim performs better.\n",
        "\n",
        "* [edumunozsala](https://github.com/edumunozsala/NMT-encoder-decoder-Attention/blob/main/Intro-seq2seq-Encoder-Decoder-ENG-SPA-translator-tf2.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYDD5lxyTH2_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}