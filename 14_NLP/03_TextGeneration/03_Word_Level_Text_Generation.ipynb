{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Word_Level_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgO7VlBXUA-Y"
      },
      "source": [
        "### Word level Text Generation.\n",
        "\n",
        "In this notebook we are going to learn how to efficiently build an input pipeline for word level text generation in `tf`. First we need to download the dataset [here](https://s3.amazonaws.com/text-datasets/nietzsche.txt). I've already downloaded the dataset and loaded it to my google drive so that we can load it easy in google colab. We are going to follow the following steps:\n",
        "\n",
        "1. Read the file line by line and split words into tokens\n",
        "2. We will generate a tuple including word sequence input X maping to word sequence output Y\n",
        "3. We wil then use keras API `TextVectorization` to:\n",
        "  * preprocess text\n",
        "  * convert the words into integer represantation\n",
        "  * prepare training sets from pairs\n",
        "  * Optmize the data pipeline.\n",
        "\n",
        "### Definition\n",
        "From the last notebooks we have leant how to generate text character by character. A word level text generation will then generate the text word by word. After training, the Language Model learns to generate a conditional probability distribution over the vocabulary of words according to the given input sequence.\n",
        "\n",
        "\n",
        "### Steps\n",
        "* **Step 1:** we provide **a sequence of words** to the Language Model as input\n",
        "* **Step 2:** the Language Model outputs **a conditional probability distribution** over the **vocabulary**\n",
        "* **Step 3:** we **sample** a word from the distribution\n",
        "* **Step 4:** we **concatenate** the newly sampled word to the ***generated text***\n",
        "* **Step 4:** **a new input sequence** is genareted by appending the newly sampled word\n",
        "\n",
        "\n",
        "### Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0RzRJxgtTtzD",
        "outputId": "cf4e8317-aff1-48b0-aa2a-a7758e72f44d"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import os, re, string, time\n",
        "\n",
        "tf.__version__\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fyY-91uXKDK"
      },
      "source": [
        "### Mounting the Drive and paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm7l-BmDXcUe",
        "outputId": "5359751c-c4ec-489c-def5-a12765f3dc91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RegR7QpXrs3",
        "outputId": "182be3d0-00f2-48b5-d3d9-9f808543a087"
      },
      "source": [
        "file_path = '/content/drive/My Drive/NLP Data/text-gen/nietzsche.txt'\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk5OQjosXFym"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJqMLVM7XDTM"
      },
      "source": [
        "raw_data = tf.data.TextLineDataset([file_path])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ztKL8xYxqX"
      },
      "source": [
        "Checking some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFfJDRzqYwB_",
        "outputId": "e94bf195-1b0a-47b8-caee-7e42041c1dbe"
      },
      "source": [
        "for el in raw_data.take(10):\n",
        "  print(el)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'PREFACE', shape=(), dtype=string)\n",
            "tf.Tensor(b'', shape=(), dtype=string)\n",
            "tf.Tensor(b'', shape=(), dtype=string)\n",
            "tf.Tensor(b'SUPPOSING that Truth is a woman--what then? Is there not ground', shape=(), dtype=string)\n",
            "tf.Tensor(b'for suspecting that all philosophers, in so far as they have been', shape=(), dtype=string)\n",
            "tf.Tensor(b'dogmatists, have failed to understand women--that the terrible', shape=(), dtype=string)\n",
            "tf.Tensor(b'seriousness and clumsy importunity with which they have usually paid', shape=(), dtype=string)\n",
            "tf.Tensor(b'their addresses to Truth, have been unskilled and unseemly methods for', shape=(), dtype=string)\n",
            "tf.Tensor(b'winning a woman? Certainly she has never allowed herself to be won; and', shape=(), dtype=string)\n",
            "tf.Tensor(b'at present every kind of dogma stands with sad and discouraged mien--IF,', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW9uTFxCZddP"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkOS1Ko6ZCYO",
        "outputId": "bf44acc6-6cd5-4fe9-ef2b-53e7e333adcd"
      },
      "source": [
        "import en_core_web_sm\n",
        "en_tokenizer = en_core_web_sm.load()\n",
        "\n",
        "tokenize = lambda x: tf.strings.split(x)\n",
        "\n",
        "tokenize(\"I love machine leaning.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'I', b'love', b'machine', b'leaning.'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59iEqEE9Z4Kf",
        "outputId": "443f2f17-f583-4f0a-85eb-b98e2476f7c0"
      },
      "source": [
        "raw_dataset = raw_data.map(tokenize)\n",
        "for el in raw_dataset.take(10):\n",
        "  print(el)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'PREFACE'], shape=(1,), dtype=string)\n",
            "tf.Tensor([], shape=(0,), dtype=string)\n",
            "tf.Tensor([], shape=(0,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'SUPPOSING' b'that' b'Truth' b'is' b'a' b'woman--what' b'then?' b'Is'\n",
            " b'there' b'not' b'ground'], shape=(11,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'for' b'suspecting' b'that' b'all' b'philosophers,' b'in' b'so' b'far'\n",
            " b'as' b'they' b'have' b'been'], shape=(12,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'dogmatists,' b'have' b'failed' b'to' b'understand' b'women--that'\n",
            " b'the' b'terrible'], shape=(8,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'seriousness' b'and' b'clumsy' b'importunity' b'with' b'which' b'they'\n",
            " b'have' b'usually' b'paid'], shape=(10,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'their' b'addresses' b'to' b'Truth,' b'have' b'been' b'unskilled' b'and'\n",
            " b'unseemly' b'methods' b'for'], shape=(11,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'winning' b'a' b'woman?' b'Certainly' b'she' b'has' b'never' b'allowed'\n",
            " b'herself' b'to' b'be' b'won;' b'and'], shape=(13,), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'at' b'present' b'every' b'kind' b'of' b'dogma' b'stands' b'with' b'sad'\n",
            " b'and' b'discouraged' b'mien--IF,'], shape=(12,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdO8RvBibCvH"
      },
      "source": [
        "Futher preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnH-VXlZadeQ",
        "outputId": "0a0fbff2-0eb0-4bc4-f382-a5e97da96e59"
      },
      "source": [
        "raw_dataset = raw_dataset.flat_map(\n",
        "    lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
        ")\n",
        "for el in raw_dataset.take(5):\n",
        "  print(el.numpy())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'PREFACE'\n",
            "b'SUPPOSING'\n",
            "b'that'\n",
            "b'Truth'\n",
            "b'is'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHUkHwDrblxo"
      },
      "source": [
        "### Vocalbulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHeJKGpkbWly",
        "outputId": "1dfe9588-98d4-4e70-b8b6-460668fbfe5b"
      },
      "source": [
        "vocab_size = len(set(\n",
        "    raw_dataset.as_numpy_iterator()\n",
        "))\n",
        "\n",
        "print(\"> vocab size: \",vocab_size )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> vocab size:  18809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcSO0O9zb4K4"
      },
      "source": [
        "### Generating `X` and `y` tuples.\n",
        "\n",
        "We can split the text into two datasets  as below:\n",
        "* The first dataset (**X**) is  the **input data** to the model which will hold fixed-size word sequences (*partial sentences*) \n",
        "* The second dataset (**y**) is  the **output data** which has only  one-word samples (*next word*)\n",
        "\n",
        "To create these datasets (**X** ***input sequence of words*** & **y** ***next word***), we can apply `tf.data.Dataset.window()` transformation.\n",
        "\n",
        "First, define the size of the input sequence: How many words will be in the input?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adkZBRahb1n0"
      },
      "source": [
        "input_sequence_size = 4"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTE3f-Q-iBQA"
      },
      "source": [
        "Then, apply the `window()` transformation such that each window will have `input_sequence_size+1` words (|X|+|y|)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEWFyXP-cSxR",
        "outputId": "1f526f65-7318-4955-99c1-045c8b970d3a"
      },
      "source": [
        "sequence_dataset= raw_dataset.window(input_sequence_size+1, drop_remainder=True)\n",
        "for window in sequence_dataset.take(10):\n",
        "  print(list(window.as_numpy_iterator()))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'PREFACE', b'SUPPOSING', b'that', b'Truth', b'is']\n",
            "[b'a', b'woman--what', b'then?', b'Is', b'there']\n",
            "[b'not', b'ground', b'for', b'suspecting', b'that']\n",
            "[b'all', b'philosophers,', b'in', b'so', b'far']\n",
            "[b'as', b'they', b'have', b'been', b'dogmatists,']\n",
            "[b'have', b'failed', b'to', b'understand', b'women--that']\n",
            "[b'the', b'terrible', b'seriousness', b'and', b'clumsy']\n",
            "[b'importunity', b'with', b'which', b'they', b'have']\n",
            "[b'usually', b'paid', b'their', b'addresses', b'to']\n",
            "[b'Truth,', b'have', b'been', b'unskilled', b'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-b_7f5iisQX"
      },
      "source": [
        "But we just want a regular dataset containing tensors: {[1,2,3,4,5],[6,7,8,9,10],...}, where [...] represents a tensor. The `flat_map()` method returns all the tensors in a nested dataset, after transforming each nested dataset. \n",
        " \n",
        "If we didn't batch, we would get: {1,2,3,4,5,6,7,8,9,10,...}. By batching each window to its full size, we get {[1,2,3,4,5],[6,7,8,9,10],...} as we desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTcTjgEXiO6T",
        "outputId": "44b7a92e-140b-481f-a8e2-5c4d6ab77db2"
      },
      "source": [
        "sequence_dataset = sequence_dataset.flat_map(lambda window: window.batch(5))\n",
        "for ele in sequence_dataset.take(10):\n",
        "  print(ele)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'PREFACE' b'SUPPOSING' b'that' b'Truth' b'is'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'a' b'woman--what' b'then?' b'Is' b'there'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'not' b'ground' b'for' b'suspecting' b'that'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'all' b'philosophers,' b'in' b'so' b'far'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'as' b'they' b'have' b'been' b'dogmatists,'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'have' b'failed' b'to' b'understand' b'women--that'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'the' b'terrible' b'seriousness' b'and' b'clumsy'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'importunity' b'with' b'which' b'they' b'have'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'usually' b'paid' b'their' b'addresses' b'to'], shape=(5,), dtype=string)\n",
            "tf.Tensor([b'Truth,' b'have' b'been' b'unskilled' b'and'], shape=(5,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmJXXpxsjVzQ"
      },
      "source": [
        "Now each item in the dataset is a tensor, so we can split it into **X** & **y** datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axGsIqoUi71k"
      },
      "source": [
        "sequence_dataset = sequence_dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "\n",
        "X_train_ds_raw = sequence_dataset.map(lambda X,y: X)\n",
        "y_train_ds_raw = sequence_dataset.map(lambda X,y: y)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzNM2OGaj7FT"
      },
      "source": [
        "Let's see some input-output pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQBjNGwGj2Mk"
      },
      "source": [
        "from prettytable import PrettyTable"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyBehFqakR_Z",
        "outputId": "cc64af00-6e59-4546-a63a-35a84786b137"
      },
      "source": [
        "def tabulate(column_names, data):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= \"Input output pairs\"\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "\n",
        "column_names = [\"X\", \"y\"]\n",
        "row_data = []\n",
        "\n",
        "for X, y in zip(X_train_ds_raw.take(10),y_train_ds_raw.take(10)):\n",
        "  row_data.append([X.numpy(), y.numpy()])\n",
        "\n",
        "tabulate(column_names, row_data)  \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------+\n",
            "|                       Input output pairs                      |\n",
            "+--------------------------------------------+------------------+\n",
            "|                     X                      |        y         |\n",
            "+--------------------------------------------+------------------+\n",
            "| [b'PREFACE' b'SUPPOSING' b'that' b'Truth'] |     [b'is']      |\n",
            "|    [b'a' b'woman--what' b'then?' b'Is']    |    [b'there']    |\n",
            "|  [b'not' b'ground' b'for' b'suspecting']   |    [b'that']     |\n",
            "|   [b'all' b'philosophers,' b'in' b'so']    |     [b'far']     |\n",
            "|      [b'as' b'they' b'have' b'been']       | [b'dogmatists,'] |\n",
            "|  [b'have' b'failed' b'to' b'understand']   | [b'women--that'] |\n",
            "| [b'the' b'terrible' b'seriousness' b'and'] |   [b'clumsy']    |\n",
            "| [b'importunity' b'with' b'which' b'they']  |    [b'have']     |\n",
            "| [b'usually' b'paid' b'their' b'addresses'] |     [b'to']      |\n",
            "|  [b'Truth,' b'have' b'been' b'unskilled']  |     [b'and']     |\n",
            "+--------------------------------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMnRvCjNlZci"
      },
      "source": [
        "### Reshaping `X` dataset.\n",
        "\n",
        "Input (X) is **a vector of strings** but we need to convert it to **a  string vector** so that we can vectorize it properly.\n",
        "\n",
        "Below is a python function for iterating the given tensor to join all the strings into a single string:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18Exfvkk72s"
      },
      "source": [
        "def convert_string(X: tf.Tensor)->str:\n",
        "  str1 = \"\"\n",
        "  for ele in X:\n",
        "    str1 += ele.numpy().decode('utf-8')+ \" \"\n",
        "\n",
        "  return tf.convert_to_tensor(str1[:-1])\n",
        "  "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-l2XNSAmIFw"
      },
      "source": [
        "We will apply the `convert_string` function to every element of `X_train_ds_raw`\n",
        "\n",
        "**Note that** to use a ***python function*** as a mapping function, you need to apply `tf.py_function()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul8jt35XlshT"
      },
      "source": [
        "X_train_ds_raw = X_train_ds_raw.map(\n",
        "    lambda x: tf.py_function(func=convert_string,\n",
        "     inp=[x], Tout=tf.string)\n",
        ")\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcVKBCcCmaze",
        "outputId": "145fc06f-dfea-41ad-8614-ebd64644e34b"
      },
      "source": [
        "column_names = [\"X (sequence)\", \"y (next word)\"]\n",
        "row_data = []\n",
        "\n",
        "for X, y in zip(X_train_ds_raw.take(10), y_train_ds_raw.take(10)):\n",
        "  row_data.append([X.numpy(), y.numpy()])\n",
        "\n",
        "tabulate(column_names, row_data)   "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------+\n",
            "|                 Input output pairs                 |\n",
            "+---------------------------------+------------------+\n",
            "|           X (sequence)          |  y (next word)   |\n",
            "+---------------------------------+------------------+\n",
            "| b'PREFACE SUPPOSING that Truth' |     [b'is']      |\n",
            "|    b'a woman--what then? Is'    |    [b'there']    |\n",
            "|   b'not ground for suspecting'  |    [b'that']     |\n",
            "|    b'all philosophers, in so'   |     [b'far']     |\n",
            "|       b'as they have been'      | [b'dogmatists,'] |\n",
            "|   b'have failed to understand'  | [b'women--that'] |\n",
            "| b'the terrible seriousness and' |   [b'clumsy']    |\n",
            "|  b'importunity with which they' |    [b'have']     |\n",
            "| b'usually paid their addresses' |     [b'to']      |\n",
            "|  b'Truth, have been unskilled'  |     [b'and']     |\n",
            "+---------------------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Hfh-clmwJT"
      },
      "source": [
        "However, the shape of X is unknown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfjEhcwhmm9_",
        "outputId": "d57ce382-c78a-43c3-db93-4c955eee91d5"
      },
      "source": [
        "print(X_train_ds_raw.element_spec, y_train_ds_raw.element_spec)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorSpec(shape=<unknown>, dtype=tf.string, name=None) TensorSpec(shape=(None,), dtype=tf.string, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FImuVCTOm1Mn"
      },
      "source": [
        "To fix this, we can explicitly set the shape with another transformation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWk1SFWXmy83"
      },
      "source": [
        "X_train_ds_raw=X_train_ds_raw.map(lambda x: tf.reshape(x, [1]))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-k-oitCm5gP",
        "outputId": "87ef9f73-ac25-4b4e-85c8-b3bba2464e40"
      },
      "source": [
        "X_train_ds_raw.element_spec, y_train_ds_raw.element_spec"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(1,), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXrFHc2ym_-3"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "### What are the preprocessing steps?\n",
        "The processing of each sample contains the following steps:\n",
        "\n",
        "* **standardize** each sample (usually lowercasing + punctuation stripping): \n",
        "\n",
        "* **split** each sample into substrings (usually words):\n",
        "\n",
        "  As in this part, we aim at splitting the text into **fixed-size word sequences**, we ***do not*** need to use a **custom split function**.\n",
        "\n",
        "* **recombine** substrings into tokens (usually ngrams):\n",
        "  We will leave it as 1 ngram (word)\n",
        "\n",
        "* **index tokens** (associate a unique int value with each token)\n",
        "\n",
        "* **transform** each sample using this index, either into a vector of ints or a dense float vector.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-i_LK_sm95r"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase     = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "  stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
        "  stripped_punc  =tf.strings.regex_replace(stripped_num, \n",
        "                            \"[%s]\" % re.escape(string.punctuation), \"\")    \n",
        "  return stripped_punc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86QEK6-6nh2v"
      },
      "source": [
        "### Text vectorization params\n",
        "\n",
        "\n",
        "* We can limit the number of distinct words by setting `max_features`\n",
        "* We set an explicit `sequence_length`, since our  model needs **fixed-size** input sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkjIgivanchO"
      },
      "source": [
        "max_features = 60000           # Number of distinct words in the vocabulary  \n",
        "sequence_length = input_sequence_size            # Input sequence size\n",
        "batch_size = 128                # Batch size \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUCQ1Wipn1F4"
      },
      "source": [
        "### Create the text vectorization layer\n",
        "\n",
        "* The **text vectorization layer** is initialized below. \n",
        "* We are using this layer to normalize, split, and map strings to integers, so we set our 'output_mode' to '**int**'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9xJhtFAnw7s"
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    # split --> DEFAULT: split each sample into substrings (usually words)\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARe8aMP2n9wn"
      },
      "source": [
        "### Adapt the Text Vectorization layer to the train dataset\n",
        "\n",
        "Now that the **Text Vectorization layer** has been created, we can call `adapt` on a text-only dataset to create the vocabulary with indexing. \n",
        "\n",
        "We don't have to batch, but for very large datasets this means you're not keeping spare copies of the dataset in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMYp7tc3oVz3"
      },
      "source": [
        "vectorize_layer.adapt(raw_dataset.batch(batch_size))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjLH_DeyoFrZ",
        "outputId": "d5c3b605-f778-4d19-ef0f-fc9156628810"
      },
      "source": [
        "print(\"The size of the vocabulary (number of distinct words): \", vectorize_layer.vocabulary_size())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of the vocabulary (number of distinct words):  9903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ByygJIokkn"
      },
      "source": [
        "Let's see the first 10 entries in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQAX6WcooMrP",
        "outputId": "4658f274-fa57-495f-9c9d-c9bd0a47ad7e"
      },
      "source": [
        "print(\"The first 10 entries: \", vectorize_layer.get_vocabulary()[:10])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first 10 entries:  ['', '[UNK]', 'the', 'of', 'and', 'to', 'in', 'is', 'a', 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CrEeAXowzn"
      },
      "source": [
        "After preparing the **Text Vectorization layer**,  we need a helper function to **convert a given raw text to a Tensor** by using this layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzzP3BpOonFv"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ivJAw_o31T"
      },
      "source": [
        "### Apply the **Text Vectorization** onto X and y datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-awCj72Ho0Cy",
        "outputId": "c4ed2c96-46f8-4238-917b-7520665e388c"
      },
      "source": [
        "for elem in X_train_ds_raw.take(3):\n",
        "  print(\"X: \",elem.numpy())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  [b'PREFACE SUPPOSING that Truth']\n",
            "X:  [b'a woman--what then? Is']\n",
            "X:  [b'not ground for suspecting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKxT_Tlo9YE"
      },
      "source": [
        "# Vectorize the data.\n",
        "X_train_ds = X_train_ds_raw.map(vectorize_text)\n",
        "y_train_ds = y_train_ds_raw.map(vectorize_text)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeUmI9EbpCuM",
        "outputId": "a911f578-32c0-487f-fb75-973efca05326"
      },
      "source": [
        "column_names = [\"X (sequence)\", \"y (next word)\"]\n",
        "row_data = []\n",
        "\n",
        "for X, y in zip(X_train_ds.take(10), y_train_ds.take(10)):\n",
        "  row_data.append([X.numpy(), y.numpy()])\n",
        "\n",
        "tabulate(column_names, row_data)  "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------+\n",
            "|               Input output pairs              |\n",
            "+-----------------------+-----------------------+\n",
            "|      X (sequence)     |     y (next word)     |\n",
            "+-----------------------+-----------------------+\n",
            "| [4041  576    9  119] |       [7 0 0 0]       |\n",
            "|   [  8 147  41 143]   |     [40  0  0  0]     |\n",
            "| [  15 1083   12 5783] |       [9 0 0 0]       |\n",
            "|   [ 18 160   6  38]   |   [121   0   0   0]   |\n",
            "|     [11 30 27 59]     | [2543    0    0    0] |\n",
            "| [  27 4596    5  263] |   [610   9   0   0]   |\n",
            "|   [  2 575 701   4]   | [1107    0    0    0] |\n",
            "| [7812   16   13   30] |     [27  0  0  0]     |\n",
            "| [ 796 3004   33 5117] |       [5 0 0 0]       |\n",
            "| [ 119   27   59 5414] |       [4 0 0 0]       |\n",
            "+-----------------------+-----------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD43p4cDpgka"
      },
      "source": [
        "### Convert **y** to a single char representation\n",
        "\n",
        "Notice that even we want **y** to be a **single word**, after the text vectorization, it becomes **a vector of integers** as well!\n",
        "\n",
        "We need to fix this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8udGm7RpFyi",
        "outputId": "50524f11-ceea-4aba-a003-5466a99079b3"
      },
      "source": [
        "for elem in y_train_ds.take(2):\n",
        "  print(\"shape: \", elem.shape, \"\\n next_char: \",elem.numpy())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape:  (4,) \n",
            " next_char:  [7 0 0 0]\n",
            "shape:  (4,) \n",
            " next_char:  [40  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvwNwxGxpsl5"
      },
      "source": [
        "We can solve this by simply selecting the first element of the array only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0tomnsSpjIK"
      },
      "source": [
        "y_train_ds=y_train_ds.map(lambda x: x[:1])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0S3bSEUp140"
      },
      "source": [
        "Now it's as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ChFs_-3pwja",
        "outputId": "1dd728e3-a5fa-4d26-e837-c31c30a3790d"
      },
      "source": [
        "for elem in y_train_ds.take(2):\n",
        "  print(\"shape: \", elem.shape, \"\\n next_char: \",elem.numpy())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape:  (1,) \n",
            " next_char:  [7]\n",
            "shape:  (1,) \n",
            " next_char:  [40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL77ExIkp9m2",
        "outputId": "a8aa4878-2096-4717-9e5a-054d9ed4df8a"
      },
      "source": [
        "column_names = [\"X (sequence)\", \"y (next word)\"]\n",
        "row_data = []\n",
        "\n",
        "for X, y in zip(X_train_ds.take(10), y_train_ds.take(10)):\n",
        "  row_data.append([X.numpy(), y.numpy()])\n",
        "\n",
        "tabulate(column_names, row_data)  "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------+\n",
            "|           Input output pairs          |\n",
            "+-----------------------+---------------+\n",
            "|      X (sequence)     | y (next word) |\n",
            "+-----------------------+---------------+\n",
            "| [4041  576    9  119] |      [7]      |\n",
            "|   [  8 147  41 143]   |      [40]     |\n",
            "| [  15 1083   12 5783] |      [9]      |\n",
            "|   [ 18 160   6  38]   |     [121]     |\n",
            "|     [11 30 27 59]     |     [2543]    |\n",
            "| [  27 4596    5  263] |     [610]     |\n",
            "|   [  2 575 701   4]   |     [1107]    |\n",
            "| [7812   16   13   30] |      [27]     |\n",
            "| [ 796 3004   33 5117] |      [5]      |\n",
            "| [ 119   27   59 5414] |      [4]      |\n",
            "+-----------------------+---------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUJ8dQ8iqEIx"
      },
      "source": [
        "### Finilizing the data pipeline.\n",
        "\n",
        "We want to join the inpu (X) and the output (y) to be a single dataset. However after transfomation the shape will become unknown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHDmxAazqAC4",
        "outputId": "1312ecb7-16d5-42f4-8c47-82ef6b6e39ee"
      },
      "source": [
        "train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))\n",
        "train_ds.element_spec"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-v-s4bvqjji"
      },
      "source": [
        "To fix this we can apply another transformation to set the shapes explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z28_5SZ3qfUR"
      },
      "source": [
        "def _fixup_shape(X, y):\n",
        "   X.set_shape([4])\n",
        "   y.set_shape([1])\n",
        "   return X, y"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1La3v6j2qyS4",
        "outputId": "02eac671-e102-4ef2-ef51-f92725adf8d5"
      },
      "source": [
        "train_ds = train_ds.map(_fixup_shape)\n",
        "train_ds.element_spec"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(4,), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(1,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofz2bypSrQQG"
      },
      "source": [
        "Everything is now looking okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idnt055-q0O0",
        "outputId": "d0a50edd-2ff4-4ce5-b9a6-3982a305e322"
      },
      "source": [
        "for el in train_ds.take(5):\n",
        "  print(el)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([4041,  576,    9,  119])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>)\n",
            "(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([  8, 147,  41, 143])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([40])>)\n",
            "(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([  15, 1083,   12, 5783])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([9])>)\n",
            "(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 18, 160,   6,  38])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([121])>)\n",
            "(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([11, 30, 27, 59])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2543])>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-XWmwMMrU0o"
      },
      "source": [
        "### Data pipeline optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QojkAijjrOqK"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUzF5AVErhuo"
      },
      "source": [
        "Checking the shape of our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR04ktoIrcy8",
        "outputId": "74e8bd2c-82f5-43c9-9b3d-976326d389d6"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(128, 4), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(128, 1), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3gddFdrsh6"
      },
      "source": [
        "### Basic `LSTM` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfZV1QSSrgCt"
      },
      "source": [
        "embedding_dim = 16 "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5j5Y2essC-X",
        "outputId": "3ef1bb38-1cef-4664-bc65-e237193a5d39"
      },
      "source": [
        "inputs = tf.keras.Input(\n",
        "    shape=(sequence_length, ),\n",
        "    dtype=\"int32\"\n",
        ")\n",
        "x = keras.layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = keras.layers.LSTM(128, return_sequences=True)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "outputs =  keras.layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name=\"model\")\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 4)]               0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 4, 16)             960000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 4, 128)            74240     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 60000)             30780000  \n",
            "=================================================================\n",
            "Total params: 31,814,240\n",
            "Trainable params: 31,814,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEmVhoO5sCzx",
        "outputId": "93438a7a-d917-4fa9-95d9-38cac96f039a"
      },
      "source": [
        "model.fit(train_ds, epochs=1) "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154/154 [==============================] - 34s 171ms/step - loss: 8.2563 - accuracy: 0.0554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cc57ee3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbfGlaqUsCqQ"
      },
      "source": [
        "def sample(preds, temperature=0.2):\n",
        "  # helper function to sample an index from a probability array\n",
        "  preds=np.squeeze(preds)\n",
        "  \n",
        "  preds = np.asarray(preds).astype(\"float64\")\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOCi5trVr0sn"
      },
      "source": [
        "def generate_text(model, seed_original, step):\n",
        "    seed= vectorize_text(seed_original)\n",
        "    decode_sentence(seed.numpy().squeeze())\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print(\"...Diversity:\", diversity)\n",
        "        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n",
        "        \n",
        "\n",
        "        generated = (seed)\n",
        "        for i in range(step):\n",
        "            #print(seed.shape)\n",
        "            predictions=model.predict(seed)\n",
        "            pred_max= np.argmax(predictions.squeeze())\n",
        "            #print(\"pred_max: \", pred_max)\n",
        "            next_index = sample(predictions, diversity)\n",
        "            #print(\"next_index: \", next_index)\n",
        "            generated = np.append(generated, next_index)\n",
        "            seed= generated[-sequence_length:].reshape(1,sequence_length)\n",
        "        decode_sentence(generated)\n",
        "    "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uxr7_Gdtxgu"
      },
      "source": [
        "def decode_sentence (encoded_sentence):\n",
        "  deceoded_sentence=[]\n",
        "  for word in encoded_sentence:\n",
        "    \n",
        "    deceoded_sentence.append(vectorize_layer.get_vocabulary()[word])\n",
        "  sentence= ' '.join(deceoded_sentence)\n",
        "  print(sentence)\n",
        "  return sentence\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "EOIlBT3mtz7U",
        "outputId": "702442e0-26c3-45ef-870a-2649ae44b4da"
      },
      "source": [
        "generate_text(model, \n",
        "              \"PREFACE SUPPOSING that Truth\",  \n",
        "              100)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preface supposing that truth\n",
            "...Diversity: 0.2\n",
            "preface supposing that truth the the the the the the the the the the the the the the the the the the the in of the the of the of the the the the the of the the the the the the the of the the the the the the the the the the the the the the the the of the the the the the the the the the the the the of the the the the the the the of the the the the the the the the the the the of the the the the the the the the the the\n",
            "...Diversity: 0.5\n",
            "preface supposing that truth and for the of the of of to and to and the so the the for the the the the the a of the of the of that the the verdict the for the the the that and that of of the he the the the to to in is the in the in the the to the the the of of that to the and of the the to that the the to as the the of as the and not that as the the the the is as by good of to the the the that the he\n",
            "...Diversity: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3e4ddc63409f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m generate_text(model, \n\u001b[1;32m      2\u001b[0m               \u001b[0;34m\"PREFACE SUPPOSING that Truth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               100)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-9296bdc4672b>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, seed_original, step)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdecode_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-e71c2dc24e9a>\u001b[0m in \u001b[0;36mdecode_sentence\u001b[0;34m(encoded_sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdeceoded_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeceoded_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlBUdNaRuG7_"
      },
      "source": [
        "### Encoder-Decoder Model with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stSxTIBjt3T1"
      },
      "source": [
        "LSTMoutputDimension=16"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NfhXS2buL4_"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, verbose=0):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    self.verbose= verbose\n",
        "\n",
        "  def call(self, query, values):\n",
        "    if self.verbose:\n",
        "      print('\\n******* Bahdanau Attention STARTS******')\n",
        "      print('query (decoder hidden state): (batch_size, hidden size) ', query.shape)\n",
        "      print('values (encoder all hidden state): (batch_size, max_len, hidden size) ', values.shape)\n",
        "\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    if self.verbose:\n",
        "      print('query_with_time_axis:(batch_size, 1, hidden size) ', query_with_time_axis.shape)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    if self.verbose:\n",
        "      print('score: (batch_size, max_length, 1) ',score.shape)\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    if self.verbose:\n",
        "      print('attention_weights: (batch_size, max_length, 1) ',attention_weights.shape)\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    if self.verbose:\n",
        "      print('context_vector before reduce_sum: (batch_size, max_length, hidden_size) ',context_vector.shape)\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    if self.verbose:\n",
        "      print('context_vector after reduce_sum: (batch_size, hidden_size) ',context_vector.shape)\n",
        "      print('\\n******* Bahdanau Attention ENDS******')\n",
        "    return context_vector, attention_weights\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96O3H90uPxf"
      },
      "source": [
        "verbose= 0 \n",
        "#See all debug messages\n",
        "\n",
        "#batch_size=1\n",
        "if verbose:\n",
        "  print('***** Model Hyper Parameters *******')\n",
        "  print('latentSpaceDimension: ', LSTMoutputDimension)\n",
        "  print('batch_size: ', batch_size)\n",
        "  print('sequence length (n_timesteps_in): ', max_features )\n",
        "  print('n_features: ', embedding_dim)\n",
        "\n",
        "  print('\\n***** TENSOR DIMENSIONS *******')\n",
        "\n",
        "# The first part is encoder\n",
        "# A integer input for vocab indices.\n",
        "encoder_inputs = tf.keras.Input(shape=(sequence_length,), dtype=\"int64\", name='encoder_inputs')\n",
        "#encoder_inputs = Input(shape=(n_timesteps_in, n_features), name='encoder_inputs')\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "embedding = layers.Embedding(max_features, embedding_dim)\n",
        "embedded= embedding(encoder_inputs)\n",
        "\n",
        "encoder_lstm = layers.LSTM(LSTMoutputDimension,return_sequences=True, return_state=True,  name='encoder_lstm')\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(embedded)\n",
        "\n",
        "if verbose:\n",
        "  print ('Encoder output shape: (batch size, sequence length, latentSpaceDimension) {}'.format(encoder_outputs.shape))\n",
        "  print ('Encoder Hidden state shape: (batch size, latentSpaceDimension) {}'.format(encoder_state_h.shape))\n",
        "  print ('Encoder Cell state shape: (batch size, latentSpaceDimension) {}'.format(encoder_state_c.shape))\n",
        "# initial context vector is the states of the encoder\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "if verbose:\n",
        "  print(encoder_states)\n",
        "# Set up the attention layer\n",
        "attention= BahdanauAttention(LSTMoutputDimension, verbose=verbose)\n",
        "\n",
        "\n",
        "# Set up the decoder layers\n",
        "decoder_inputs = layers.Input(shape=(1, (embedding_dim+LSTMoutputDimension)),name='decoder_inputs')\n",
        "decoder_lstm = layers.LSTM(LSTMoutputDimension,  return_state=True, name='decoder_lstm')\n",
        "decoder_dense = layers.Dense(max_features, activation='softmax',  name='decoder_dense')\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "# 1 initial decoder's input data\n",
        "# Prepare initial decoder input data that just contains the start character \n",
        "# Note that we made it a constant one-hot-encoded in the model\n",
        "# that is, [1 0 0 0 0 0 0 0 0 0] is the first input for each loop\n",
        "# one-hot encoded zero(0) is the start symbol\n",
        "inputs = np.zeros((batch_size, 1, max_features))\n",
        "inputs[:, 0, 0] = 1 \n",
        "# 2 initial decoder's state\n",
        "# encoder's last hidden state + last cell state\n",
        "decoder_outputs = encoder_state_h\n",
        "states = encoder_states\n",
        "if verbose:\n",
        "  print('initial decoder inputs: ', inputs.shape)\n",
        "\n",
        "# decoder will only process one time step at a time.\n",
        "for _ in range(1):\n",
        "\n",
        "    # 3 pay attention\n",
        "    # create the context vector by applying attention to \n",
        "    # decoder_outputs (last hidden state) + encoder_outputs (all hidden states)\n",
        "    context_vector, attention_weights=attention(decoder_outputs, encoder_outputs)\n",
        "    if verbose:\n",
        "      print(\"Attention context_vector: (batch size, units) {}\".format(context_vector.shape))\n",
        "      print(\"Attention weights : (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
        "      print('decoder_outputs: (batch_size,  latentSpaceDimension) ', decoder_outputs.shape )\n",
        "\n",
        "    context_vector = tf.expand_dims(context_vector, 1)\n",
        "    if verbose:\n",
        "      print('Reshaped context_vector: ', context_vector.shape )\n",
        "\n",
        "    # 4. concatenate the input + context vectore to find the next decoder's input\n",
        "    inputs = tf.concat([context_vector, tf.dtypes.cast(inputs, tf.float32)], axis=-1)\n",
        "    \n",
        "    if verbose:\n",
        "      print('After concat inputs: (batch_size, 1, n_features + hidden_size): ',inputs.shape )\n",
        "\n",
        "    # 5. passing the concatenated vector to the LSTM\n",
        "    # Run the decoder on one timestep with attended input and previous states\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(inputs,\n",
        "                                                     initial_state=states)\n",
        "    #decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))\n",
        "  \n",
        "    outputs = decoder_dense(decoder_outputs)\n",
        "    # 6. Use the last hidden state for prediction the output\n",
        "    # save the current prediction\n",
        "    # we will concatenate all predictions later\n",
        "    outputs = tf.expand_dims(outputs, 1)\n",
        "    all_outputs.append(outputs)\n",
        "    # 7. Reinject the output (prediction) as inputs for the next loop iteration\n",
        "    # as well as update the states\n",
        "    inputs = outputs\n",
        "    states = [state_h, state_c]\n",
        "\n",
        "\n",
        "# 8. After running Decoder for max time steps\n",
        "# we had created a predition list for the output sequence\n",
        "# convert the list to output array by Concatenating all predictions \n",
        "# such as [batch_size, timesteps, features]\n",
        "decoder_outputs = layers.Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "\n",
        "# 9. Define and compile model \n",
        "model_encoder_decoder_Bahdanau_Attention = keras.Model(encoder_inputs, \n",
        "                                                 decoder_outputs, name='model_encoder_decoder')\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pss9zfBqu9Vt"
      },
      "source": [
        "model_encoder_decoder_Bahdanau_Attention.compile(optimizer= tf.keras.optimizers.RMSprop(learning_rate=0.001), \n",
        "                                                 loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoqrxqYouzr6",
        "outputId": "0ed485b9-9e99-4125-f940-5c7fe7668e09"
      },
      "source": [
        "model_encoder_decoder_Bahdanau_Attention.fit(train_ds, epochs=3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "154/154 [==============================] - 6s 22ms/step - loss: 9.5935 - accuracy: 0.0604\n",
            "Epoch 2/3\n",
            "154/154 [==============================] - 3s 21ms/step - loss: 7.0884 - accuracy: 0.0615\n",
            "Epoch 3/3\n",
            "154/154 [==============================] - 3s 22ms/step - loss: 6.6640 - accuracy: 0.0615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cd124cfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-I5R6vSvDM4"
      },
      "source": [
        "model_encoder_decoder_Bahdanau_Attention.save(\"model_encoder_decoder_Bahdanau_Attention\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjNjG7UpvNI7"
      },
      "source": [
        "# The first part is encoder\n",
        "# A integer input for vocab indices.\n",
        "encoder_inputs = tf.keras.Input(shape=(sequence_length,), dtype=\"int64\", name='encoder_inputs')\n",
        "\n",
        "embedded= embedding(encoder_inputs)\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(embedded)\n",
        "\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "inputs = np.zeros((1, 1, max_features))\n",
        "inputs[:, 0, 0] = 1 \n",
        "\n",
        "decoder_outputs = encoder_state_h\n",
        "states = encoder_states\n",
        "\n",
        "context_vector, attention_weights=attention(decoder_outputs, encoder_outputs)\n",
        "context_vector = tf.expand_dims(context_vector, 1)\n",
        "inputs = tf.concat([context_vector, tf.dtypes.cast(inputs, tf.float32)], axis=-1)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)\n",
        "outputs = decoder_dense(decoder_outputs)\n",
        "outputs = tf.expand_dims(outputs, 1)\n",
        "\n",
        "\n",
        "# 9. Define and compile model \n",
        "model_encoder_decoder_Bahdanau_Attention_PREDICTION = keras.Model(encoder_inputs, \n",
        "                                                 outputs, name='model_encoder_decoder')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "f9vEdQBwvJVl",
        "outputId": "a96ef4d2-c3fe-42ac-86ac-e68b26b1007e"
      },
      "source": [
        "generate_text(model_encoder_decoder_Bahdanau_Attention_PREDICTION, \n",
        "              \"PREFACE SUPPOSING that Truth\",  \n",
        "              100)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preface supposing that truth\n",
            "...Diversity: 0.2\n",
            "preface supposing that truth the the the the and the the of the the the the the the the the the the of the the the the the the the of the the the the the the the the the the the the the the the of the the the the the the and the the the of of the of the the the the the the the the and the the the the the the and the the the the the the of the the of of the the the the the the of the the the the of the the the the\n",
            "...Diversity: 0.5\n",
            "preface supposing that truth and who the to of of as for the of and the the a the for to and man that the all that of and the the the of the of of of the in and the of this the the and of to and and the for is the the of the have of the the the was his of and of and be the the such the that of the the the to be the the in the the of of it the it of the the to to the of a or the to of the and\n",
            "...Diversity: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-bfc997075161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m generate_text(model_encoder_decoder_Bahdanau_Attention_PREDICTION, \n\u001b[1;32m      2\u001b[0m               \u001b[0;34m\"PREFACE SUPPOSING that Truth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               100)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-9296bdc4672b>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, seed_original, step)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdecode_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-e71c2dc24e9a>\u001b[0m in \u001b[0;36mdecode_sentence\u001b[0;34m(encoded_sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_sentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdeceoded_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeceoded_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXjTllu6vYwy"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The model did not perform well, next we will look at the better model to perform this very same task."
      ]
    }
  ]
}