{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_LSTM_word-level-text-gen+ Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBqPiQJFp_I3"
      },
      "source": [
        "### Word level text generation + Embedding vectors\n",
        "\n",
        "In this notebook we are going to learn how load pretrained word vectors to our model for text generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J74qtHCJXqa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a17396-29dd-40e5-842f-a7e055146afc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx36fbJirBxm"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eBk1Zrxtp7xj",
        "outputId": "6fc1db22-b179-463b-902e-50d7b435a8a9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import os, time, math, string, random\n",
        "\n",
        "np.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.19.5'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljvqyz3rhd4"
      },
      "source": [
        "#### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lug0yJVdp7up",
        "outputId": "bbd71c1c-d97f-4923-bbf0-750f55040a8c"
      },
      "source": [
        "base_path = '/content/drive/My Drive/NLP Data/text-gen/New York Times Comments'\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu7ZBabYp7qP",
        "outputId": "a635cbea-85f3-49e1-d40d-cd6f45fb6dbe"
      },
      "source": [
        "%%time\n",
        "all_headlines = []\n",
        "for filename in os.listdir(base_path):\n",
        "  if 'Articles' in filename:\n",
        "    df = pd.read_csv(os.path.join(base_path, filename))\n",
        "    all_headlines.extend(list(\n",
        "        df.headline.values\n",
        "    ))\n",
        "    break\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "print(len(all_headlines))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1214\n",
            "CPU times: user 14.1 ms, sys: 6.89 ms, total: 20.9 ms\n",
            "Wall time: 28.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7mB3D3gp7n-",
        "outputId": "8a5af675-4d03-4cdd-e85e-738660c9dfd1"
      },
      "source": [
        "all_headlines[:15]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
              " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
              " 'The New Noma, Explained',\n",
              " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
              " 'Is School a Place for Self-Expression?',\n",
              " 'Commuter Reprogramming',\n",
              " 'Ford Changed Leaders, Looking for a Lift. It’s Still Looking.',\n",
              " 'Romney Failed to Win at Utah Convention, But Few Believe He’s Doomed',\n",
              " 'Chain Reaction',\n",
              " 'He Forced the Vatican to Investigate Sex Abuse. Now He’s Meeting With Pope Francis.',\n",
              " 'In Berlin, artists find a home',\n",
              " 'The Right Stuff',\n",
              " 'Jimmy Carter Knows What North Korea Wants',\n",
              " 'The Truth Is Out There',\n",
              " 'New Jersey Ruling Could Reignite Battle Over Church-State Separation']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQXMR5VvteYG"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "The first step that we are going to do for data preperation is text-clening. We are going to remove punctuations and lowercase all words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0-vorPap7je",
        "outputId": "b2b0fef1-0342-47e6-82d1-64b5b901b026"
      },
      "source": [
        "def clean_text(txt):\n",
        "  txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "  txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
        "  return txt\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "random.shuffle(corpus)\n",
        "corpus[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['childhood fears no parent can allay',\n",
              " 'variety acrostic',\n",
              " 'teaching activities for i spy at new yorks museum of deception',\n",
              " 'how strenuous exercise affects our immune system',\n",
              " 'voter suppressions legacy lives on',\n",
              " 'us set to blunt pollution rules for automakers',\n",
              " 'the americans season 6 episode 4 recap the birth of a honey pot',\n",
              " 'perus love affair with the potato',\n",
              " 'how to make facebook more accountable',\n",
              " 'why your bank account interest is still paltry']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUaQthWUxpq2"
      },
      "source": [
        "### Generating Sequence of N-grams Tokens\n",
        "Language modelling requires a sequence input data, as given a sequence of word-tokens the aim is to predict the next word.\n",
        "\n",
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9bm_bWsp7hR"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "    token_list= tokenizer.texts_to_sequences([\n",
        "        line\n",
        "    ])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "  return input_sequences, total_words"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwJX110hp7Vu",
        "outputId": "12837836-d36d-4e58-a9e0-4e99f88a2a03"
      },
      "source": [
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:28]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[582, 269],\n",
              " [582, 269, 36],\n",
              " [582, 269, 36, 1115],\n",
              " [582, 269, 36, 1115, 27],\n",
              " [582, 269, 36, 1115, 27, 1116],\n",
              " [270, 371],\n",
              " [31, 47],\n",
              " [31, 47, 6],\n",
              " [31, 47, 6, 59],\n",
              " [31, 47, 6, 59, 372],\n",
              " [31, 47, 6, 59, 372, 13],\n",
              " [31, 47, 6, 59, 372, 13, 14],\n",
              " [31, 47, 6, 59, 372, 13, 14, 583],\n",
              " [31, 47, 6, 59, 372, 13, 14, 583, 1117],\n",
              " [31, 47, 6, 59, 372, 13, 14, 583, 1117, 4],\n",
              " [31, 47, 6, 59, 372, 13, 14, 583, 1117, 4, 1118],\n",
              " [15, 1119],\n",
              " [15, 1119, 584],\n",
              " [15, 1119, 584, 1120],\n",
              " [15, 1119, 584, 1120, 66],\n",
              " [15, 1119, 584, 1120, 66, 585],\n",
              " [15, 1119, 584, 1120, 66, 585, 1121],\n",
              " [1122, 1123],\n",
              " [1122, 1123, 586],\n",
              " [1122, 1123, 586, 373],\n",
              " [1122, 1123, 586, 373, 8],\n",
              " [22, 587],\n",
              " [22, 587, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CT-qh9xzgZU"
      },
      "source": [
        "### How ngrams works?\n",
        "\n",
        "```\n",
        "[1, 12] -> this is\n",
        "[1, 12, 5170] -> this is a\n",
        "[1, 12, 5170, 3366]-> this is a dog\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgtInLXrDVix"
      },
      "source": [
        "### Loading the pretrain word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m1p0KJDCmkX"
      },
      "source": [
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDg181TECh4O"
      },
      "source": [
        "embedding_path = \"/content/drive/MyDrive/NLP Data/glove.6B/glove.6B.100d.txt\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsP06hLACh1O",
        "outputId": "2cf846bb-782c-4814-d3f1-c6004b3b6fe4"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "start = time.time()\n",
        "with open(embedding_path, encoding='utf8') as glove_file:\n",
        "    for line in glove_file:\n",
        "        records = line.split()\n",
        "        word  = records[0]\n",
        "        vectors = np.asarray(records[1:], dtype='float32')\n",
        "        embeddings_dictionary[word] = vectors\n",
        "\n",
        "print(f\"ETA: {hms_string(time.time() - start)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ETA: 0:00:11.62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rFNfnwzDj-K"
      },
      "source": [
        "Prepare embedding vectors to suit our own data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQHZa5q0Chxe",
        "outputId": "cc01d24b-1a95-4d14-a80e-b980f9d24e99"
      },
      "source": [
        "start = time.time()\n",
        "embedding_matrix = np.zeros((total_words, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    vector = embeddings_dictionary.get(word)\n",
        "    if vector is not None:\n",
        "      embedding_matrix[index] = vector\n",
        "    \n",
        "print(f\"ETA: {hms_string(time.time() - start)}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ETA: 0:00:00.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P7DqzUJz6H1"
      },
      "source": [
        "### Padding sequences and generating target.\n",
        "\n",
        "Sequences must be of the same length. Ww will create n-grams sequences as predictors and the next word of the n-grams as label.\n",
        "\n",
        "```\n",
        "they -> are (label)\n",
        "they are -> standing (label)\n",
        "they are standing -> at (label)\n",
        "they are standing at -> the (label)\n",
        "they are standing at the -> door (label)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU0uHnKFp7Sk"
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, \n",
        "                                            padding='pre'))\n",
        "  predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "  label = keras.utils.to_categorical(label, num_classes=total_words)\n",
        "  return predictors, label, max_sequence_len"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcC7Lk59p7P9"
      },
      "source": [
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIaO1FaR3I5B"
      },
      "source": [
        "### LSTM for text generation\n",
        "\n",
        "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
        "\n",
        "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
        "\n",
        "LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3fROykH3uNU",
        "outputId": "d64fc46e-5f60-4adb-f3fc-c68b1dce8e2f"
      },
      "source": [
        "predictors"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0,  582],\n",
              "       [   0,    0,    0, ...,    0,  582,  269],\n",
              "       [   0,    0,    0, ...,  582,  269,   36],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,    0, 3492,  439],\n",
              "       [   0,    0,    0, ...,    0,    0,  207],\n",
              "       [   0,    0,    0, ...,    0,  207, 3493]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCb7l--1k6S",
        "outputId": "6a0c2da7-eaea-408e-fa71-567cc1efeb90"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "  input_len = max_sequence_len - 1\n",
        "  model = keras.Sequential(name=\"text-generator\")\n",
        "  model.add(keras.layers.Embedding( total_words, 100,\n",
        "                                    weights=[embedding_matrix], \n",
        "                                    trainable=True,\n",
        "                                    input_length=input_len))\n",
        "  model.add(keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True)))\n",
        "  model.add(keras.layers.LSTM(100))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(512, activation='relu'))\n",
        "  model.add(keras.layers.Dense(512, activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.5))\n",
        "  model.add(keras.layers.Dense(total_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam', metrics=[\"acc\"])\n",
        "  return model\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"text-generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 23, 100)           349400    \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 23, 200)           160800    \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 100)               120400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               51712     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3494)              1792422   \n",
            "=================================================================\n",
            "Total params: 2,737,390\n",
            "Trainable params: 2,737,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TLnGvTU17zy",
        "outputId": "608e0316-14dd-4f02-beac-dd4ec1aa2d42"
      },
      "source": [
        "model.fit(predictors, label,\n",
        "          epochs=100, verbose=1\n",
        "          )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 7.0916 - acc: 0.0292\n",
            "Epoch 2/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.9852 - acc: 0.0324\n",
            "Epoch 3/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.8692 - acc: 0.0364\n",
            "Epoch 4/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.7432 - acc: 0.0404\n",
            "Epoch 5/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.6500 - acc: 0.0452\n",
            "Epoch 6/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.5323 - acc: 0.0458\n",
            "Epoch 7/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.4127 - acc: 0.0506\n",
            "Epoch 8/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.2904 - acc: 0.0545\n",
            "Epoch 9/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.1625 - acc: 0.0596\n",
            "Epoch 10/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 6.0521 - acc: 0.0672\n",
            "Epoch 11/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.9291 - acc: 0.0784\n",
            "Epoch 12/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.8102 - acc: 0.0868\n",
            "Epoch 13/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.6962 - acc: 0.0902\n",
            "Epoch 14/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.6093 - acc: 0.1023\n",
            "Epoch 15/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.5015 - acc: 0.1118\n",
            "Epoch 16/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.4136 - acc: 0.1232\n",
            "Epoch 17/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.3063 - acc: 0.1302\n",
            "Epoch 18/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.2249 - acc: 0.1348\n",
            "Epoch 19/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.1373 - acc: 0.1420\n",
            "Epoch 20/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 5.0645 - acc: 0.1497\n",
            "Epoch 21/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.9763 - acc: 0.1557\n",
            "Epoch 22/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.9239 - acc: 0.1624\n",
            "Epoch 23/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.8291 - acc: 0.1721\n",
            "Epoch 24/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.7766 - acc: 0.1756\n",
            "Epoch 25/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.7021 - acc: 0.1908\n",
            "Epoch 26/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.6409 - acc: 0.1890\n",
            "Epoch 27/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.5571 - acc: 0.1956\n",
            "Epoch 28/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.5113 - acc: 0.2013\n",
            "Epoch 29/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.4200 - acc: 0.2111\n",
            "Epoch 30/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.3776 - acc: 0.2165\n",
            "Epoch 31/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.3309 - acc: 0.2168\n",
            "Epoch 32/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.2688 - acc: 0.2263\n",
            "Epoch 33/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.1954 - acc: 0.2304\n",
            "Epoch 34/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.1481 - acc: 0.2344\n",
            "Epoch 35/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.0900 - acc: 0.2445\n",
            "Epoch 36/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 4.0406 - acc: 0.2468\n",
            "Epoch 37/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.9787 - acc: 0.2529\n",
            "Epoch 38/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.9242 - acc: 0.2557\n",
            "Epoch 39/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.8868 - acc: 0.2568\n",
            "Epoch 40/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.8375 - acc: 0.2652\n",
            "Epoch 41/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.7770 - acc: 0.2753\n",
            "Epoch 42/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.7316 - acc: 0.2781\n",
            "Epoch 43/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.6791 - acc: 0.2760\n",
            "Epoch 44/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.6438 - acc: 0.2845\n",
            "Epoch 45/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.5958 - acc: 0.2871\n",
            "Epoch 46/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.5337 - acc: 0.2937\n",
            "Epoch 47/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.5276 - acc: 0.2917\n",
            "Epoch 48/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.4660 - acc: 0.3008\n",
            "Epoch 49/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.4071 - acc: 0.3060\n",
            "Epoch 50/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.3737 - acc: 0.3127\n",
            "Epoch 51/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.3403 - acc: 0.3165\n",
            "Epoch 52/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.3083 - acc: 0.3151\n",
            "Epoch 53/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.2502 - acc: 0.3258\n",
            "Epoch 54/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.2167 - acc: 0.3285\n",
            "Epoch 55/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.1955 - acc: 0.3394\n",
            "Epoch 56/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.1547 - acc: 0.3363\n",
            "Epoch 57/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.1088 - acc: 0.3428\n",
            "Epoch 58/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.0526 - acc: 0.3481\n",
            "Epoch 59/100\n",
            "244/244 [==============================] - 7s 31ms/step - loss: 3.0664 - acc: 0.3442\n",
            "Epoch 60/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 3.0310 - acc: 0.3515\n",
            "Epoch 61/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.9686 - acc: 0.3615\n",
            "Epoch 62/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.9654 - acc: 0.3610\n",
            "Epoch 63/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.9435 - acc: 0.3595\n",
            "Epoch 64/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.8858 - acc: 0.3725\n",
            "Epoch 65/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.8596 - acc: 0.3693\n",
            "Epoch 66/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.8446 - acc: 0.3731\n",
            "Epoch 67/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.8118 - acc: 0.3823\n",
            "Epoch 68/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.7747 - acc: 0.3831\n",
            "Epoch 69/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.7522 - acc: 0.3841\n",
            "Epoch 70/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.7165 - acc: 0.3875\n",
            "Epoch 71/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.7098 - acc: 0.3931\n",
            "Epoch 72/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.6478 - acc: 0.4043\n",
            "Epoch 73/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.6550 - acc: 0.4004\n",
            "Epoch 74/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.6560 - acc: 0.4001\n",
            "Epoch 75/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.6088 - acc: 0.4064\n",
            "Epoch 76/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.5608 - acc: 0.4124\n",
            "Epoch 77/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.5805 - acc: 0.4120\n",
            "Epoch 78/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.5347 - acc: 0.4165\n",
            "Epoch 79/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.5430 - acc: 0.4169\n",
            "Epoch 80/100\n",
            "244/244 [==============================] - 7s 31ms/step - loss: 2.4989 - acc: 0.4261\n",
            "Epoch 81/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4478 - acc: 0.4266\n",
            "Epoch 82/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4480 - acc: 0.4305\n",
            "Epoch 83/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4358 - acc: 0.4334\n",
            "Epoch 84/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4234 - acc: 0.4393\n",
            "Epoch 85/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4027 - acc: 0.4351\n",
            "Epoch 86/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.4176 - acc: 0.4334\n",
            "Epoch 87/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.3768 - acc: 0.4423\n",
            "Epoch 88/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.3388 - acc: 0.4478\n",
            "Epoch 89/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.3070 - acc: 0.4502\n",
            "Epoch 90/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.3184 - acc: 0.4551\n",
            "Epoch 91/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.2695 - acc: 0.4593\n",
            "Epoch 92/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.2815 - acc: 0.4533\n",
            "Epoch 93/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.2773 - acc: 0.4603\n",
            "Epoch 94/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.2454 - acc: 0.4584\n",
            "Epoch 95/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.2052 - acc: 0.4765\n",
            "Epoch 96/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.1735 - acc: 0.4761\n",
            "Epoch 97/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.1861 - acc: 0.4760\n",
            "Epoch 98/100\n",
            "244/244 [==============================] - 7s 30ms/step - loss: 2.1741 - acc: 0.4814\n",
            "Epoch 99/100\n",
            "244/244 [==============================] - 7s 31ms/step - loss: 2.2269 - acc: 0.4783\n",
            "Epoch 100/100\n",
            "244/244 [==============================] - 7s 31ms/step - loss: 2.1412 - acc: 0.4806\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc3f68b610>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr1CJw1K57c-"
      },
      "source": [
        "### Generating text.\n",
        "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYlmp1d4vET"
      },
      "source": [
        "def generate_text(seed_text, max_len, model, max_sequence_len):\n",
        "  for _ in range(max_len):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list],\n",
        "                               maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list)\n",
        "    predicted = np.argmax(predicted, axis=1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "  return seed_text.title()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuKb9rEy6P4D",
        "outputId": "9bcb901b-7c41-4a7b-e3a4-6ab5b2b7f168"
      },
      "source": [
        "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
        "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "United States Are Doing What Scott Pruitt\n",
            "Preident Trump Blasts Comey In Barrage\n",
            "Donald Trump Blasts Comey In Barrage\n",
            "India And China Triumph Of Liang Populist\n",
            "New York City Decline Crisis Plot\n",
            "Science And Technology Video To Close Back At\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncF38KBGIcim"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}