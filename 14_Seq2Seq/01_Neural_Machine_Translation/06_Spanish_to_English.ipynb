{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Spanish_to_English.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEmZC70Drnk"
      },
      "source": [
        "### Simple `Encoder-Decoder` model.\n",
        "\n",
        "In this notebook we are going to combine the three previous notebooks, by creating a simple model that translate sentences in spanish to english.\n",
        "\n",
        "### What I've done so far.\n",
        "1. I've downloaded the data that we are going to work with [here](http://www.manythings.org/anki/)\n",
        "2. I've extracted the zipped file and uploaded it on my google drive so that we can easily work with it here in google colab.\n",
        "\n",
        "\n",
        "### What are we going to do ?\n",
        "1. We are going to load the data and prepare it just like from the previous notebooks\n",
        "2. We are going to create 5 models and train them with same epochs and compare the results.\n",
        "3. This time around we want to split our data into respective sets which are:\n",
        "  * train set\n",
        "  * val set\n",
        "  * and the test set.\n",
        "\n",
        "4. Evaluate the models using the `test` set.\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEGRciNODh3o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "68b6a976-77a6-4e5e-e278-06baaccce7d3"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import helper, os, time\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-6SXAJDHBHa"
      },
      "source": [
        "### Mounting the Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3C26uJADrWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc550edd-b386-4c74-9e28-6c93a2e02d72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc3hfXU7HZix"
      },
      "source": [
        "### Paths to the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_690XgdzDrTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e764d0a-b2d9-4af6-d70d-d8e8e4a93e80"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/NLP Data/seq2seq/spa-en'\n",
        "file_name =\"spa.txt\"\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gltNO_9sH4B7"
      },
      "source": [
        "### Data Loading and preparation.\n",
        "Our file has a file name `spa.txt` and this is a huge file with the following structure in it:\n",
        "\n",
        "```\n",
        "Go.\tVe.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)\n",
        "Go.\tVete.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986656 (cueyayotl)\n",
        "Go.\tVaya.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986657 (cueyayotl)\n",
        "....\n",
        "```\n",
        "As we can see there's a lot of cleaning that need to happen here. The language pairs are seperated with tabs and we have some gabbage that we are not interested in after the second tab. So What we will do is to just ignore that gabbage.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "581k-VQpDrOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4ab3e2-e1dd-4471-f019-9a69f6415fcd"
      },
      "source": [
        "\n",
        "unclean_data = open(os.path.join(base_path, file_name),\n",
        "                    encoding=\"utf8\").read().split('\\n')\n",
        "print(f\"Data Loaded, {len(unclean_data)} pairs found\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Loaded, 134737 pairs found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EroZAWUvJneb",
        "outputId": "b4cfd593-0d3c-4047-828b-352e463a0de6"
      },
      "source": [
        "unclean_data[1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Go.\\tVete.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986656 (cueyayotl)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKrzHwIxP_xe"
      },
      "source": [
        "Next, we want to create lists of sentences from this uncleaned huge list for all our languages which are spanish and english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJj8mUM7QXl3",
        "outputId": "0a549f23-1b38-454d-e8c9-b1260787f53b"
      },
      "source": [
        "spanish_data =[]\n",
        "english_data =[]\n",
        "for line in unclean_data:\n",
        "  try:\n",
        "    en, sp, _ = line.split('\\t')\n",
        "    spanish_data.append(sp)\n",
        "    english_data.append(en)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "print(f\"Spanish: {len(spanish_data)}\")\n",
        "print(f\"English: {len(english_data)}\")\n",
        "\n",
        "for i, (e, s) in enumerate(zip(english_data[:10], spanish_data)):\n",
        "  print(f\"> {e} |> {s}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spanish: 134736\n",
            "English: 134736\n",
            "> Go. |> Ve.\n",
            "> Go. |> Vete.\n",
            "> Go. |> Vaya.\n",
            "> Go. |> Váyase.\n",
            "> Hi. |> Hola.\n",
            "> Run! |> ¡Corre!\n",
            "> Run! |> ¡Corran!\n",
            "> Run! |> ¡Corra!\n",
            "> Run! |> ¡Corred!\n",
            "> Run. |> Corred.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJGFS4RSHPX"
      },
      "source": [
        "We have loaded our data, Next we are going to split this data into 3 sets, the train, validation and the test set. For that we are going to use my favourite `sklearn` `train_test_split`function from `model_selection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DRFaF1cSgs8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj3ehVlpSm7s",
        "outputId": "b5f4856a-c0b9-47e8-d690-4ca9c8cce263"
      },
      "source": [
        "eng_train, eng_val, spa_train, spa_val = train_test_split(\n",
        "    english_data, spanish_data, random_state=42, test_size = .05\n",
        ")\n",
        "eng_train, eng_test, spa_train, spa_test = train_test_split(\n",
        "    eng_train, spa_train, random_state=42, test_size = .005\n",
        ")\n",
        "len(eng_train), len(eng_val), len(spa_train), len(spa_val), len(eng_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(127359, 6737, 127359, 6737, 640)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BzqvRxNSm4y",
        "outputId": "e505e651-bd1f-428d-bb94-39a11c5d18fb"
      },
      "source": [
        "eng_train[0], spa_train[0], eng_val[0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Do you have money?', '¿Tiene usted dinero?', 'Where do you think Tom is?')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzQHLHzDJwYB"
      },
      "source": [
        "\n",
        "\n",
        "### Next, Bulding the Vocabulary.\n",
        "\n",
        "I'm going to use the `spacy` library which is my favourite when it comes to tokenization of languages.\n",
        "\n",
        "* We are only going to build the vocabulary on the train data because we want the validation data to represent the test data as much as possible. And in machine learning models dont have to look on the test data during training only at inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYdwgsDDDrIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af295f8-cde1-4a77-a226-a1abb2ab6092"
      },
      "source": [
        "import spacy\n",
        "spacy.cli.download('es_core_news_sm')\n",
        "\n",
        "spacy_es = spacy.load('es_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em20rVNkDrFo"
      },
      "source": [
        "def tokenize_es(sent):\n",
        "  return [tok.text for tok in spacy_es.tokenizer(sent)]\n",
        "  \n",
        "def tokenize_en(sent):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(sent)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqbCQ512DrCi"
      },
      "source": [
        "en_counter = Counter()\n",
        "es_counter = Counter()\n",
        "\n",
        "for sent in eng_train:\n",
        "  en_counter.update(tokenize_en(sent.lower()))\n",
        "for sent in spa_train:\n",
        "  es_counter.update(tokenize_es(sent.lower()))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq9JM8HIL6RT",
        "outputId": "45e8f79e-f93e-42d0-f784-e3a2274ea7db"
      },
      "source": [
        "en_vocab_size = len(en_counter)\n",
        "es_vocab_size = len(es_counter)\n",
        "\n",
        "es_vocab_size, en_vocab_size"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26986, 13642)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4n7K3LsMWW8"
      },
      "source": [
        "Here we have `~2.5M` unique words for the spanish laguage  and `~1.5M` unique words for english language.\n",
        "\n",
        "### Preprocessing.\n",
        "\n",
        "We will convert our text data into sequence of integers so basically we are going to perform the following:\n",
        "\n",
        "1. Tokenize the words into ids\n",
        "2. Pad the tokens so that they will have same length.\n",
        "\n",
        "For this task we are going to use the keras `Tokenizer` class to perform the task.\n",
        "\n",
        "We are going to have two tokenizers for each language.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JJVUlADq_q"
      },
      "source": [
        "en_tokenizer = Tokenizer(num_words=en_vocab_size, oov_token=\"<oov>\")\n",
        "en_tokenizer.fit_on_texts(eng_train)\n",
        "\n",
        "es_tokenizer = Tokenizer(num_words=es_vocab_size, oov_token=\"<oov>\")\n",
        "es_tokenizer.fit_on_texts(spa_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APoyIFAiDq7x"
      },
      "source": [
        "en_word_indices = en_tokenizer.word_index\n",
        "en_word_indices_reversed = dict([\n",
        "    (v, k) for (k, v) in en_word_indices.items()\n",
        "])\n",
        "\n",
        "es_word_indices = es_tokenizer.word_index\n",
        "es_word_indices_reversed = dict([\n",
        "    (v, k) for (k, v) in es_word_indices.items()\n",
        "])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An7_ADx7Olap"
      },
      "source": [
        "### Helper functions\n",
        "We will create some helper function that converts sequences to text and text to sequences for each language. These function will be used for inference later on.\n",
        "\n",
        "**We have set the out of vocabulary `oov_token|| <\"oov\">`token to `1`  which means the word that does not exist in the vocabulary it's integer representation is 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDQsSkoAOlBS"
      },
      "source": [
        "def en_seq_to_text(sequences):\n",
        "  return \" \".join(en_word_indices_reversed[i] for i in sequences )\n",
        "\n",
        "def es_seq_to_text(sequences):\n",
        "  return \" \".join(es_word_indices_reversed[i] for i in sequences )\n",
        "\n",
        "def en_text_to_seq(sent):\n",
        "  words = tokenize_en(sent.lower())\n",
        "  sequences = []\n",
        "  for word in words:\n",
        "    try:\n",
        "      sequences.append(en_word_indices[word])\n",
        "    except:\n",
        "      sequences.append(1)\n",
        "  return sequences\n",
        "\n",
        "def es_text_to_seq(sent):\n",
        "  words = tokenize_es(sent.lower())\n",
        "  sequences = []\n",
        "  for word in words:\n",
        "    try:\n",
        "      sequences.append(es_word_indices[word])\n",
        "    except:\n",
        "      sequences.append(1)\n",
        "  return sequences"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvQ_uFHTTBK"
      },
      "source": [
        "### Converting text to sequences\n",
        "\n",
        "Unlike from the previous notebooks where we had only one set. This time around we have three sets, So we are going to create all the sequences for all these three sets in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nspRxCCTSUs"
      },
      "source": [
        "en_sequences_train = en_tokenizer.texts_to_sequences(eng_train)\n",
        "es_sequences_train = es_tokenizer.texts_to_sequences(spa_train)\n",
        "\n",
        "en_sequences_test = en_tokenizer.texts_to_sequences(eng_test)\n",
        "es_sequences_test = es_tokenizer.texts_to_sequences(spa_test)\n",
        "\n",
        "en_sequences_val = en_tokenizer.texts_to_sequences(eng_val)\n",
        "es_sequences_val = es_tokenizer.texts_to_sequences(spa_val)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnbY97HGUrGP",
        "outputId": "e616e489-e60d-4097-da80-2105ee8897ba"
      },
      "source": [
        "en_sequences_test[0:4], es_sequences_test[:4]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[70, 48, 5, 492, 53, 2, 104],\n",
              "  [6, 14, 2012],\n",
              "  [138, 1116, 8, 7, 1592, 459],\n",
              "  [2, 802, 8, 579, 31, 2, 890, 35, 1007, 31, 2, 597]],\n",
              " [[72, 32, 7500, 52, 8, 85],\n",
              "  [6, 44, 3424],\n",
              "  [328, 1237, 10, 16, 239, 1696],\n",
              "  [7, 1090, 10, 1612, 17, 703, 29, 1178, 17, 196]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kaXOHaBYJcD"
      },
      "source": [
        "### Testing our helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAFnHg7nYNBa",
        "outputId": "7c8dbb0f-7b11-4f6c-ce42-54fab4f4c51d"
      },
      "source": [
        "for en, es in zip(en_sequences_test[:4], es_sequences_test[:4]):\n",
        "  print(f\"> English: {en_seq_to_text(en)} |> Spanish: {es_seq_to_text(es)}\")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> English: why did you spend all the money |> Spanish: ¿por qué gastaste todo el dinero\n",
            "> English: tom was impressed |> Spanish: tom estaba impresionado\n",
            "> English: new york is a huge city |> Spanish: nueva york es una ciudad enorme\n",
            "> English: the wall is white on the inside and green on the outside |> Spanish: la pared es blanca por dentro y verde por fuera\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKF4CdRUQJCy"
      },
      "source": [
        "### Padding Sequences.\n",
        "\n",
        "In our case we are going to assume that the longest sentence has `50` words for both `es` and `en` languages.\n",
        "\n",
        "We are going to pad all the sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkhdZ-8QI4F"
      },
      "source": [
        "max_words = 50\n",
        "\n",
        "# Train data\n",
        "en_tokens_padded_train = pad_sequences(\n",
        "    en_sequences_train, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")\n",
        "es_tokens_padded_train = pad_sequences(\n",
        "    es_sequences_train, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")\n",
        "\n",
        "# Validation data\n",
        "\n",
        "en_tokens_padded_val = pad_sequences(\n",
        "    en_sequences_val, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")\n",
        "es_tokens_padded_val = pad_sequences(\n",
        "    es_sequences_val, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")\n",
        "\n",
        "# Test data\n",
        "en_tokens_padded_test = pad_sequences(\n",
        "    en_sequences_test, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")\n",
        "es_tokens_padded_test = pad_sequences(\n",
        "    es_sequences_test, \n",
        "    maxlen=max_words, \n",
        "    padding=\"post\", \n",
        "    truncating=\"post\"\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHomR9cmDq2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f079a2cb-ce56-4e58-8436-b50dc1fdf234"
      },
      "source": [
        "en_tokens_padded_train[:2]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 15,   5,  18, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       [  3,  18,   7, 412, 561,   4, 144,   5,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FJ69jNAVOnT"
      },
      "source": [
        "### Logits to text.\n",
        "\n",
        "We are going to create 1 more helper function that will help us to take logits or the predictions probabilities and then we convert them to human understandable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDlG7MzYVvi2"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "  index_to_words = {id: word for word, id\n",
        "                    in tokenizer.word_index.items()}\n",
        "  index_to_words[0] = '<pad>'\n",
        "  \"\"\"\n",
        "  For every prediction we are going to ignore the pad token\n",
        "  \"\"\"\n",
        "  return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)]).replace(\"<pad>\", \"\")\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhYsT65Ianc8"
      },
      "source": [
        "### Models.\n",
        "As i said we are going to create 4 different models and evaluate them serparatly these models will be:\n",
        "1. Simple RNN\n",
        "2. GRU Model With Embeding\n",
        "3. LSTM Model with Embedding and Bidirectional layers\n",
        "4. Simple Encoder-Decoder Model \n",
        "\n",
        "### 1. Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfgHhEZnbPaS",
        "outputId": "f9379303-c5c1-44b0-8d4e-3f1a8194d7a8"
      },
      "source": [
        "rnn_model = keras.Sequential([\n",
        "      keras.layers.Input(shape=(max_words, 1)),\n",
        "      keras.layers.GRU(128, return_sequences=True),\n",
        "      keras.layers.TimeDistributed(\n",
        "          keras.layers.Dense(en_vocab_size, activation=\"softmax\")\n",
        "      )\n",
        "], name=\"simple_rnn\")\n",
        "rnn_model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"simple_rnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, 50, 128)           50304     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 50, 13642)         1759818   \n",
            "=================================================================\n",
            "Total params: 1,810,122\n",
            "Trainable params: 1,810,122\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JAJlYKJbPYI",
        "outputId": "4ba3a998-19d5-4af4-9f57-9d3ad8bc034b"
      },
      "source": [
        "src_train = es_tokens_padded_train.reshape(-1, max_words, 1)\n",
        "src_test = es_tokens_padded_test.reshape(-1, max_words, 1)\n",
        "src_val = es_tokens_padded_val.reshape(-1, max_words, 1)\n",
        "src.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(127359, 50, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkLDSOU9cnvX"
      },
      "source": [
        "### Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNCExmFqcr56"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 15\n",
        "VALIDATION_DATA = (src_val, en_tokens_padded_val)\n",
        "VALIDATION_BATCH_SIZE = 64"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzdyXQT_bPWX",
        "outputId": "69df9b67-9e3b-4691-8bac-74f3ccd82868"
      },
      "source": [
        "rnn_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "rnn_model.fit(\n",
        "    src_train, \n",
        "    en_tokens_padded_train, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=VALIDATION_DATA,\n",
        "    validation_batch_size = VALIDATION_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "995/995 [==============================] - 169s 168ms/step - loss: 1.0594 - accuracy: 0.8784 - val_loss: 0.8052 - val_accuracy: 0.8819\n",
            "Epoch 2/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7854 - accuracy: 0.8839 - val_loss: 0.7743 - val_accuracy: 0.8845\n",
            "Epoch 3/15\n",
            "995/995 [==============================] - 167s 167ms/step - loss: 0.7604 - accuracy: 0.8860 - val_loss: 0.7567 - val_accuracy: 0.8858\n",
            "Epoch 4/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7438 - accuracy: 0.8870 - val_loss: 0.7444 - val_accuracy: 0.8869\n",
            "Epoch 5/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7314 - accuracy: 0.8876 - val_loss: 0.7358 - val_accuracy: 0.8876\n",
            "Epoch 6/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7207 - accuracy: 0.8885 - val_loss: 0.7275 - val_accuracy: 0.8886\n",
            "Epoch 7/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7117 - accuracy: 0.8892 - val_loss: 0.7213 - val_accuracy: 0.8889\n",
            "Epoch 8/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.7040 - accuracy: 0.8898 - val_loss: 0.7160 - val_accuracy: 0.8893\n",
            "Epoch 9/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6969 - accuracy: 0.8905 - val_loss: 0.7111 - val_accuracy: 0.8901\n",
            "Epoch 10/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6907 - accuracy: 0.8909 - val_loss: 0.7077 - val_accuracy: 0.8903\n",
            "Epoch 11/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6854 - accuracy: 0.8912 - val_loss: 0.7038 - val_accuracy: 0.8905\n",
            "Epoch 12/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6805 - accuracy: 0.8916 - val_loss: 0.7003 - val_accuracy: 0.8912\n",
            "Epoch 13/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6762 - accuracy: 0.8920 - val_loss: 0.6976 - val_accuracy: 0.8914\n",
            "Epoch 14/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6724 - accuracy: 0.8923 - val_loss: 0.6953 - val_accuracy: 0.8918\n",
            "Epoch 15/15\n",
            "995/995 [==============================] - 166s 167ms/step - loss: 0.6689 - accuracy: 0.8924 - val_loss: 0.6933 - val_accuracy: 0.8916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fec30032e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjiSfOVfS7A"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0DzmVhbPRi",
        "outputId": "56e9d5a1-08b7-415b-c45a-02774ed98b0b"
      },
      "source": [
        "rnn_model.evaluate(\n",
        "    src_test,\n",
        "    en_tokens_padded_test,\n",
        "    batch_size=VALIDATION_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 50ms/step - loss: 0.6721 - accuracy: 0.8938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.672109842300415, 0.8937812447547913]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1IYIOnxfzwd"
      },
      "source": [
        "### GRU and Embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgGBNx4lbPPZ",
        "outputId": "4d6eabe8-24d0-40e2-85f7-81d6d5f9c4f7"
      },
      "source": [
        "gru_embedding_model = keras.Sequential([\n",
        "    keras.layers.Embedding(\n",
        "        es_vocab_size,\n",
        "        128, \n",
        "        input_length=max_words\n",
        "    ),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(256, return_sequences=True),\n",
        "    keras.layers.GRU(512, return_sequences=True),\n",
        "    keras.layers.Dense(1024, activation=\"relu\"),\n",
        "    keras.layers.Dense(en_vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "gru_embedding_model.summary()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 128)           3454208   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 50, 128)           99072     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 50, 256)           296448    \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 50, 512)           1182720   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50, 1024)          525312    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50, 13642)         13983050  \n",
            "=================================================================\n",
            "Total params: 19,540,810\n",
            "Trainable params: 19,540,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCOmML0nbPL8",
        "outputId": "2b8729c5-5bb9-448d-977d-0baf978b39ba"
      },
      "source": [
        "gru_embedding_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "gru_embedding_model.fit(\n",
        "    src_train, \n",
        "    en_tokens_padded_train, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=VALIDATION_DATA,\n",
        "    validation_batch_size = VALIDATION_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "995/995 [==============================] - 310s 309ms/step - loss: 0.8341 - accuracy: 0.8862 - val_loss: 0.6719 - val_accuracy: 0.8971\n",
            "Epoch 2/15\n",
            "995/995 [==============================] - 307s 308ms/step - loss: 0.6086 - accuracy: 0.9044 - val_loss: 0.5634 - val_accuracy: 0.9091\n",
            "Epoch 3/15\n",
            "995/995 [==============================] - 306s 308ms/step - loss: 0.4991 - accuracy: 0.9147 - val_loss: 0.4927 - val_accuracy: 0.9166\n",
            "Epoch 4/15\n",
            "995/995 [==============================] - 306s 307ms/step - loss: 0.4201 - accuracy: 0.9220 - val_loss: 0.4591 - val_accuracy: 0.9208\n",
            "Epoch 5/15\n",
            "995/995 [==============================] - 306s 307ms/step - loss: 0.3641 - accuracy: 0.9274 - val_loss: 0.4415 - val_accuracy: 0.9233\n",
            "Epoch 6/15\n",
            "995/995 [==============================] - 306s 308ms/step - loss: 0.3220 - accuracy: 0.9320 - val_loss: 0.4359 - val_accuracy: 0.9245\n",
            "Epoch 7/15\n",
            "995/995 [==============================] - 306s 308ms/step - loss: 0.2896 - accuracy: 0.9362 - val_loss: 0.4422 - val_accuracy: 0.9253\n",
            "Epoch 8/15\n",
            "995/995 [==============================] - 307s 309ms/step - loss: 0.2639 - accuracy: 0.9398 - val_loss: 0.4497 - val_accuracy: 0.9258\n",
            "Epoch 9/15\n",
            "995/995 [==============================] - 307s 309ms/step - loss: 0.2421 - accuracy: 0.9433 - val_loss: 0.4581 - val_accuracy: 0.9256\n",
            "Epoch 10/15\n",
            "995/995 [==============================] - 307s 308ms/step - loss: 0.2236 - accuracy: 0.9462 - val_loss: 0.4751 - val_accuracy: 0.9252\n",
            "Epoch 11/15\n",
            "995/995 [==============================] - 307s 308ms/step - loss: 0.2077 - accuracy: 0.9491 - val_loss: 0.4869 - val_accuracy: 0.9249\n",
            "Epoch 12/15\n",
            "995/995 [==============================] - 307s 308ms/step - loss: 0.1935 - accuracy: 0.9517 - val_loss: 0.5059 - val_accuracy: 0.9253\n",
            "Epoch 13/15\n",
            "995/995 [==============================] - 307s 309ms/step - loss: 0.1812 - accuracy: 0.9541 - val_loss: 0.5266 - val_accuracy: 0.9241\n",
            "Epoch 14/15\n",
            "995/995 [==============================] - 306s 308ms/step - loss: 0.1706 - accuracy: 0.9562 - val_loss: 0.5342 - val_accuracy: 0.9244\n",
            "Epoch 15/15\n",
            "995/995 [==============================] - 307s 309ms/step - loss: 0.1611 - accuracy: 0.9581 - val_loss: 0.5521 - val_accuracy: 0.9244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7febe22d89d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0hSPkqgaAO"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRzsPptybPHS",
        "outputId": "6b25889b-7008-4e81-c7d9-03ca1ab43eb9"
      },
      "source": [
        "gru_embedding_model.evaluate(\n",
        "    src_test,\n",
        "    en_tokens_padded_test,\n",
        "    batch_size=VALIDATION_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 76ms/step - loss: 0.5223 - accuracy: 0.9260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5223050713539124, 0.926031231880188]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1L3_LGTgkNU"
      },
      "source": [
        "### LSTM Bidirectional and Embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFsAchHpbPEb",
        "outputId": "dd43878a-d542-4fa0-ad20-6ee1b539d002"
      },
      "source": [
        "forward_layer = keras.layers.LSTM(128, dropout=.5,\n",
        "                                    return_sequences=True,\n",
        "                                  go_backwards=False\n",
        "                                    )\n",
        "backward_layer = keras.layers.LSTM(128, dropout=.5,\n",
        "                                    return_sequences=True,\n",
        "                                  go_backwards=True\n",
        "                                    )\n",
        "bidirectinal_lstm_model = keras.Sequential([\n",
        "    keras.layers.Embedding(\n",
        "        es_vocab_size,\n",
        "        128, \n",
        "        input_length=max_words\n",
        "    ),\n",
        "  keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=False)),\n",
        "   keras.layers.RepeatVector(max_words),\n",
        "  keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer),\n",
        "  keras.layers.TimeDistributed(keras.layers.Dense(en_vocab_size, activation='softmax'))\n",
        "    \n",
        "])\n",
        "bidirectinal_lstm_model.summary()\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 128)           3454208   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 50, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 50, 256)           394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 50, 13642)         3505994   \n",
            "=================================================================\n",
            "Total params: 7,617,610\n",
            "Trainable params: 7,617,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMRwwRYKbPC9",
        "outputId": "6374b037-fc26-411f-ee9e-2ba55510a216"
      },
      "source": [
        "bidirectinal_lstm_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "bidirectinal_lstm_model.fit(\n",
        "    src_train, \n",
        "    en_tokens_padded_train, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=VALIDATION_DATA,\n",
        "    validation_batch_size = VALIDATION_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "995/995 [==============================] - 241s 238ms/step - loss: 0.9769 - accuracy: 0.8784 - val_loss: 0.8049 - val_accuracy: 0.8819\n",
            "Epoch 2/15\n",
            "995/995 [==============================] - 236s 237ms/step - loss: 0.7948 - accuracy: 0.8832 - val_loss: 0.7754 - val_accuracy: 0.8855\n",
            "Epoch 3/15\n",
            "995/995 [==============================] - 236s 237ms/step - loss: 0.7407 - accuracy: 0.8882 - val_loss: 0.7109 - val_accuracy: 0.8911\n",
            "Epoch 4/15\n",
            "995/995 [==============================] - 236s 237ms/step - loss: 0.6714 - accuracy: 0.8945 - val_loss: 0.6395 - val_accuracy: 0.8980\n",
            "Epoch 5/15\n",
            "995/995 [==============================] - 236s 238ms/step - loss: 0.6001 - accuracy: 0.9011 - val_loss: 0.5746 - val_accuracy: 0.9041\n",
            "Epoch 6/15\n",
            "995/995 [==============================] - 237s 238ms/step - loss: 0.5469 - accuracy: 0.9057 - val_loss: 0.5275 - val_accuracy: 0.9080\n",
            "Epoch 7/15\n",
            "995/995 [==============================] - 238s 239ms/step - loss: 0.4894 - accuracy: 0.9104 - val_loss: 0.4922 - val_accuracy: 0.9112\n",
            "Epoch 8/15\n",
            "995/995 [==============================] - 238s 239ms/step - loss: 0.4491 - accuracy: 0.9140 - val_loss: 0.4625 - val_accuracy: 0.9148\n",
            "Epoch 9/15\n",
            "995/995 [==============================] - 237s 239ms/step - loss: 0.4151 - accuracy: 0.9172 - val_loss: 0.4412 - val_accuracy: 0.9172\n",
            "Epoch 10/15\n",
            "995/995 [==============================] - 237s 238ms/step - loss: 0.3863 - accuracy: 0.9202 - val_loss: 0.4232 - val_accuracy: 0.9194\n",
            "Epoch 11/15\n",
            "995/995 [==============================] - 237s 238ms/step - loss: 0.3618 - accuracy: 0.9229 - val_loss: 0.4107 - val_accuracy: 0.9209\n",
            "Epoch 12/15\n",
            "995/995 [==============================] - 237s 239ms/step - loss: 0.3412 - accuracy: 0.9253 - val_loss: 0.4007 - val_accuracy: 0.9213\n",
            "Epoch 13/15\n",
            "995/995 [==============================] - 236s 237ms/step - loss: 0.3233 - accuracy: 0.9276 - val_loss: 0.3940 - val_accuracy: 0.9227\n",
            "Epoch 14/15\n",
            "995/995 [==============================] - 235s 237ms/step - loss: 0.3080 - accuracy: 0.9296 - val_loss: 0.3889 - val_accuracy: 0.9240\n",
            "Epoch 15/15\n",
            "995/995 [==============================] - 235s 237ms/step - loss: 0.2947 - accuracy: 0.9316 - val_loss: 0.3837 - val_accuracy: 0.9243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feb52bb5050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoPKdLjkhPq9"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvHSLUAwbO-b",
        "outputId": "bde60187-3a81-478a-853e-c624a76b8aae"
      },
      "source": [
        "bidirectinal_lstm_model.evaluate(\n",
        "    src_test,\n",
        "    en_tokens_padded_test,\n",
        "    batch_size=VALIDATION_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 2s 60ms/step - loss: 0.3662 - accuracy: 0.9268\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3661833703517914, 0.9268437623977661]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moeeDvRxYQhd"
      },
      "source": [
        "### Encoder Decoder Model.\n",
        "The following cell shows how we can create our very simple encoder decoder model using the sequential API.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvuC7kQYMcU",
        "outputId": "bafbdc5c-14f1-47ec-d1cb-48fed3109558"
      },
      "source": [
        "encoder_decoder_model = keras.Sequential([\n",
        "    keras.layers.Embedding(\n",
        "        es_vocab_size,\n",
        "        128, \n",
        "        input_length=max_words\n",
        "    ),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128, return_sequences=False),\n",
        "    keras.layers.RepeatVector(max_words),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(\n",
        "        en_vocab_size, activation= \"softmax\"\n",
        "    ))   \n",
        "], name=\"encoder_decoder_model\")\n",
        "\n",
        "encoder_decoder_model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder_decoder_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 128)           3454208   \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 50, 128)           99072     \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, 50, 128)           99072     \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 50, 128)           99072     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 50, 13642)         1759818   \n",
            "=================================================================\n",
            "Total params: 5,610,314\n",
            "Trainable params: 5,610,314\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpbN8rTBb9Qy",
        "outputId": "62bd11c2-478c-40a6-a7d8-efedd0575958"
      },
      "source": [
        "encoder_decoder_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "encoder_decoder_model.fit(\n",
        "    src_train, \n",
        "    en_tokens_padded_train, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    epochs=EPOCHS,\n",
        "    validation_data=VALIDATION_DATA,\n",
        "    validation_batch_size = VALIDATION_BATCH_SIZE\n",
        ")\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "995/995 [==============================] - 217s 214ms/step - loss: 1.0824 - accuracy: 0.8761 - val_loss: 0.8689 - val_accuracy: 0.8777\n",
            "Epoch 2/15\n",
            "995/995 [==============================] - 212s 213ms/step - loss: 0.8612 - accuracy: 0.8793 - val_loss: 0.8582 - val_accuracy: 0.8788\n",
            "Epoch 3/15\n",
            "995/995 [==============================] - 212s 213ms/step - loss: 0.8545 - accuracy: 0.8796 - val_loss: 0.8548 - val_accuracy: 0.8794\n",
            "Epoch 4/15\n",
            "995/995 [==============================] - 212s 213ms/step - loss: 0.7943 - accuracy: 0.8828 - val_loss: 0.7677 - val_accuracy: 0.8849\n",
            "Epoch 5/15\n",
            "995/995 [==============================] - 211s 212ms/step - loss: 0.7356 - accuracy: 0.8885 - val_loss: 0.7232 - val_accuracy: 0.8891\n",
            "Epoch 6/15\n",
            "995/995 [==============================] - 210s 211ms/step - loss: 0.6904 - accuracy: 0.8921 - val_loss: 0.6731 - val_accuracy: 0.8940\n",
            "Epoch 7/15\n",
            "995/995 [==============================] - 211s 212ms/step - loss: 0.6305 - accuracy: 0.8989 - val_loss: 0.6197 - val_accuracy: 0.9004\n",
            "Epoch 8/15\n",
            "995/995 [==============================] - 211s 212ms/step - loss: 0.5711 - accuracy: 0.9054 - val_loss: 0.5708 - val_accuracy: 0.9062\n",
            "Epoch 9/15\n",
            "995/995 [==============================] - 212s 213ms/step - loss: 0.5195 - accuracy: 0.9108 - val_loss: 0.5370 - val_accuracy: 0.9097\n",
            "Epoch 10/15\n",
            "995/995 [==============================] - 211s 212ms/step - loss: 0.4777 - accuracy: 0.9151 - val_loss: 0.5107 - val_accuracy: 0.9129\n",
            "Epoch 11/15\n",
            "995/995 [==============================] - 212s 214ms/step - loss: 0.4419 - accuracy: 0.9190 - val_loss: 0.4892 - val_accuracy: 0.9155\n",
            "Epoch 12/15\n",
            "995/995 [==============================] - 213s 215ms/step - loss: 0.4099 - accuracy: 0.9227 - val_loss: 0.4726 - val_accuracy: 0.9157\n",
            "Epoch 13/15\n",
            "995/995 [==============================] - 213s 214ms/step - loss: 0.3816 - accuracy: 0.9261 - val_loss: 0.4530 - val_accuracy: 0.9195\n",
            "Epoch 14/15\n",
            "995/995 [==============================] - 212s 213ms/step - loss: 0.3570 - accuracy: 0.9292 - val_loss: 0.4424 - val_accuracy: 0.9208\n",
            "Epoch 15/15\n",
            "995/995 [==============================] - 213s 214ms/step - loss: 0.3358 - accuracy: 0.9321 - val_loss: 0.4331 - val_accuracy: 0.9220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feb4bdb9dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lScHt8fViGH-"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaLt4bDjiJ7O",
        "outputId": "36e52b24-2a60-4330-981d-cf40bf39bf99"
      },
      "source": [
        "encoder_decoder_model.evaluate(\n",
        "    src_test,\n",
        "    en_tokens_padded_test,\n",
        "    batch_size=VALIDATION_BATCH_SIZE,\n",
        "    verbose=1,\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 56ms/step - loss: 0.4095 - accuracy: 0.9253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4094642102718353, 0.9252812266349792]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVPSOGBSDpb8"
      },
      "source": [
        "As we can see our models are producing simmilar accuracy value of `~93%` except for the first one. \n",
        "\n",
        "Next let's make some predictions using our models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kszwT1o-drRi"
      },
      "source": [
        "### Making some predictions.\n",
        "Our model is targeting to predict french words, during the predict function we are going to do the following:\n",
        "\n",
        "1. Get the sequence of the english sentence \n",
        "2. Pad the english sequences and pass them to the model'\n",
        "3. Reshape the logits output to the shape of `(max_len, trg_vocabsize(eng)`\n",
        "4. Call the `logits_to_text` function and pass the tokenizer as the `es_tokenizer`.\n",
        "5. Get the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3iYsDDTzZzx0",
        "outputId": "a013f83b-301f-406c-f5e3-2ab99664d178"
      },
      "source": [
        "def predict(sent, model):\n",
        "  sequences = es_text_to_seq(sent)\n",
        "  padded_tokens = pad_sequences([sequences], maxlen=max_words, padding=\"post\", truncating=\"post\")\n",
        "  logits = model(padded_tokens)\n",
        "  logits = tf.reshape(logits, (max_words, -1))\n",
        "  return logits_to_text(logits, en_tokenizer)\n",
        "predict(spa_test[1], bidirectinal_lstm_model)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tom was impressed impressed                                              '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6E8zA6t7dJcl",
        "outputId": "95a7961d-9283-417f-f820-171bac5a25a4"
      },
      "source": [
        "predict(\"hola\", gru_embedding_model)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hi                                                 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLbX_HGdwa9l"
      },
      "source": [
        "### Making more predictions with different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfsVPgfWqwsr"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "def tabulate_translations(column_names, data, title, max_characters=25):\n",
        "  table = PrettyTable(column_names)\n",
        "  table.title= title\n",
        "  table.align[column_names[0]] = 'l'\n",
        "  table.align[column_names[1]] = 'l'\n",
        "  table.align[column_names[2]] = 'l'\n",
        "  table._max_width = {column_names[0] :max_characters, column_names[1] :max_characters, column_names[2]:max_characters}\n",
        "  for row in data:\n",
        "    table.add_row(row)\n",
        "  print(table)\n",
        "columns_names = [\n",
        "    \"Spanish (real src sentence)\", \"English (the actual text)\", \"Translated (translated version)\", \"MODEL USED\"\n",
        "]\n",
        "title = \"SPANISH TO ENGLISH TRANSLATOR\""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO1nrCJvxKXt",
        "outputId": "d482e133-7eb4-4a6a-a89c-8af158d73275"
      },
      "source": [
        "max_characters= 25\n",
        "total_translations= 10\n",
        "for i, (eng, spa) in enumerate(zip(eng_test[:total_translations], spa_test)):\n",
        "    rows_data = [\n",
        "                 [spa, eng, predict(spa, gru_embedding_model), \"GRU Embedding model\"],\n",
        "                 [spa, eng, predict(spa, bidirectinal_lstm_model), \"Bidirectional LSTM model\"],\n",
        "                 [spa, eng, predict(spa, encoder_decoder_model), \"Encoder Decoder model\"],\n",
        "                ]\n",
        "    tabulate_translations(columns_names, rows_data, title, max_characters)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| ¿Por qué gastaste todo el   | Why did you spend all the | we me why we spend all all all  |   GRU Embedding model    |\n",
            "| dinero?                     | money?                    | of                              |                          |\n",
            "| ¿Por qué gastaste todo el   | Why did you spend all the | all all all all all all all     | Bidirectional LSTM model |\n",
            "| dinero?                     | money?                    | money money                     |                          |\n",
            "| ¿Por qué gastaste todo el   | Why did you spend all the | all far what what know the the  |  Encoder Decoder model   |\n",
            "| dinero?                     | money?                    | the the                         |                          |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Tom estaba impresionado.    | Tom was impressed.        | tom was impressed from          |   GRU Embedding model    |\n",
            "| Tom estaba impresionado.    | Tom was impressed.        | tom was impressed impressed     | Bidirectional LSTM model |\n",
            "| Tom estaba impresionado.    | Tom was impressed.        | tom was acting of               |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Nueva York es una ciudad    | New York is a huge city.  | new york is a very city wide    |   GRU Embedding model    |\n",
            "| enorme.                     |                           |                                 |                          |\n",
            "| Nueva York es una ciudad    | New York is a huge city.  | the york is a huge huge         | Bidirectional LSTM model |\n",
            "| enorme.                     |                           |                                 |                          |\n",
            "| Nueva York es una ciudad    | New York is a huge city.  | new york is is a huge city      |  Encoder Decoder model   |\n",
            "| enorme.                     |                           |                                 |                          |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| La pared es blanca por      | The wall is white on the  | the wall is white for the and   |   GRU Embedding model    |\n",
            "| dentro y verde por fuera.   | inside and green on the   | by the the                      |                          |\n",
            "|                             | outside.                  |                                 |                          |\n",
            "| La pared es blanca por      | The wall is white on the  | the wall is is the the the and  | Bidirectional LSTM model |\n",
            "| dentro y verde por fuera.   | inside and green on the   | is is                           |                          |\n",
            "|                             | outside.                  |                                 |                          |\n",
            "| La pared es blanca por      | The wall is white on the  | the cat is under with the the   |  Encoder Decoder model   |\n",
            "| dentro y verde por fuera.   | inside and green on the   | and and                         |                          |\n",
            "|                             | outside.                  |                                 |                          |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Tienes un montón de amigos. | You have many friends.    | you have lots of friends        |   GRU Embedding model    |\n",
            "|                             |                           | friends                         |                          |\n",
            "| Tienes un montón de amigos. | You have many friends.    | you have a lot of of friends    | Bidirectional LSTM model |\n",
            "| Tienes un montón de amigos. | You have many friends.    | you have a lot of friends       |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Él es un principiante.      | He is a beginner.         | he is a beginner stranger       |   GRU Embedding model    |\n",
            "| Él es un principiante.      | He is a beginner.         | he's is a amateur amateur       | Bidirectional LSTM model |\n",
            "| Él es un principiante.      | He is a beginner.         | he is a good addict             |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Tom pensó que la oferta de  | Tom thought Mary's offer  | tom thought mary teacher was    |   GRU Embedding model    |\n",
            "| Mary no era razonable.      | was unreasonable.         | was was good                    |                          |\n",
            "| Tom pensó que la oferta de  | Tom thought Mary's offer  | tom thought that offer was be   | Bidirectional LSTM model |\n",
            "| Mary no era razonable.      | was unreasonable.         | really                          |                          |\n",
            "| Tom pensó que la oferta de  | Tom thought Mary's offer  | tom thought that that wasn't    |  Encoder Decoder model   |\n",
            "| Mary no era razonable.      | was unreasonable.         | not                             |                          |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Ellos pedirían ayuda.       | They would call for help. | they <oov> them of              |   GRU Embedding model    |\n",
            "| Ellos pedirían ayuda.       | They would call for help. | they gave us help               | Bidirectional LSTM model |\n",
            "| Ellos pedirían ayuda.       | They would call for help. | they paid your for              |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Ahora no podemos ayudarte.  | We can't help you now.    | i don't go now you must you     |   GRU Embedding model    |\n",
            "|                             |                           | side                            |                          |\n",
            "| Ahora no podemos ayudarte.  | We can't help you now.    | we can't help help you now      | Bidirectional LSTM model |\n",
            "| Ahora no podemos ayudarte.  | We can't help you now.    | we don't don't do you           |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "+----------------------------------------------------------------------------------------------------------------------+\n",
            "|                                            SPANISH TO ENGLISH TRANSLATOR                                             |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Spanish (real src sentence) | English (the actual text) | Translated (translated version) |        MODEL USED        |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n",
            "| Soy muy tímida.             | I'm very shy.             | i'm very very                   |   GRU Embedding model    |\n",
            "| Soy muy tímida.             | I'm very shy.             | i'm very very of                | Bidirectional LSTM model |\n",
            "| Soy muy tímida.             | I'm very shy.             | i'm very very and               |  Encoder Decoder model   |\n",
            "+-----------------------------+---------------------------+---------------------------------+--------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLm2D3MzNLb"
      },
      "source": [
        "### Conclusion.\n",
        "In this notebook we have covered mush, and we observed that the GRU model performed better as compared to other models during prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIMMcUScc2Hd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}