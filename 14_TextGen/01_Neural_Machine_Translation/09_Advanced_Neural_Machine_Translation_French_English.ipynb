{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_Advanced_Neural_Machine_Translation_French_English.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8qel9jeBRs0"
      },
      "source": [
        "### French English.\n",
        "In this notebook we are going to create an `Encoder-Decoder` model that will learn to translate text from one domain to the other. We are going to create a model that translates sentences from French to English. The dataset that we will be using can be found [here](http://www.manythings.org/anki/)\n",
        "\n",
        "Note: This notebook is based on the previous notebook so there will be fewer changes. Where there's changes I will highlight the changes.\n",
        "\n",
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akXlvfkJBQ-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42cee7d4-fb2f-4549-d205-6cdac820c221"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.37.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.34.1)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.34.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.5)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.5.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6QGMTWoGBQ9B",
        "outputId": "e64445ac-6334-48a0-e5d4-c7ec13a454da"
      },
      "source": [
        "import numpy as np\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6w8t8EVGhY2"
      },
      "source": [
        "### Data.\n",
        "\n",
        "We are going to use the data from  http://www.manythings.org/anki/ This data contains langauge pair translations in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book? ¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "### Dataset Preparation\n",
        "I've already downloaded the dataset and uploaded it on my google drive. For data preparation we are going to do the following:\n",
        "\n",
        "1. Add a start and end token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length.\n",
        "\n",
        "### Mounting my google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOy_ssh0BQzY",
        "outputId": "9327e929-3cab-410a-92be-864153082ae9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru6mSFlvBQxA",
        "outputId": "7112637e-8f6a-4460-81ef-23410f8d7c6c"
      },
      "source": [
        "import os\n",
        "path_to_file = '/content/drive/My Drive/NLP Data/seq2seq/en-fr/fra.txt'\n",
        "os.path.exists(path_to_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIlT0RSvBQtI"
      },
      "source": [
        "def load_data(path):\n",
        "  lines = open(path, encoding='utf-8').read().split('\\n')\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  inp, targ = [], []\n",
        "  for pair in pairs:\n",
        "    if (len(pair) != 3):\n",
        "      continue\n",
        "    else:\n",
        "      a, b, _ = pair\n",
        "      inp.append(b.lower())\n",
        "      targ.append(a.lower())\n",
        "  return targ, inp"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOeffXG0BQq0",
        "outputId": "a17921c2-861c-4a89-b8ce-7050a6f9b43b"
      },
      "source": [
        "targ, inp = load_data(path_to_file)\n",
        "print(len(inp), len(targ))\n",
        "print(inp[0], targ[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "190206 190206\n",
            "va ! go.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uQ50_dXLTuv"
      },
      "source": [
        "### Creating the dataset.\n",
        "From this list of strings we are going to create a `tf.data.Dataset` of strings that shuffles and batches them efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1KAr1j5BQm6"
      },
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, \n",
        "                                              targ)).shuffle(\n",
        "                                                  BUFFER_SIZE).batch(\n",
        "                                                      BATCH_SIZE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqrARBDWBQlD",
        "outputId": "2a0f6927-aa33-47c4-8a93-d532d5f9098d"
      },
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"mais non ! tu n'es pas si vieille que \\xc3\\xa7a !\"\n",
            " b\"c'est l'endroit dont je t'ai parl\\xc3\\xa9.\"\n",
            " b'je veux que tom revienne.'\n",
            " b\"ils ont port\\xc3\\xa9 l'eau dans des seaux.\"\n",
            " b\"je suppose que je ferais mieux de m'en aller.\"], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b\"come on. you're not that old.\" b'this is the place i told you about.'\n",
            " b'i want tom to come back.' b'they carried water in buckets.'\n",
            " b\"i guess i'd better be going.\"], shape=(5,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8UOT2vPMDaj"
      },
      "source": [
        "### Text preprocessing.\n",
        "\n",
        "In this notebook we are going to create a model that takes in string inputs and output string outputs not the model that takes strings and output logits.\n",
        "\n",
        "### Standardization.\n",
        "The model is dealing with multilingual text with a limited vocabulary. So it will be important to standardize the input text.\n",
        "\n",
        "The first step is Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents.\n",
        "\n",
        "The `tensroflow_text` package contains a unicode normalize operation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBERE1TnBQiz",
        "outputId": "d1b2fd66-77dd-41d3-ef7e-9b4d0b8cdf44"
      },
      "source": [
        "example_text = tf.constant('¿Todavía está en casa?') # This is for spanish, but the preprocessing is the same\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
            "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKtx-mXsR2ZB"
      },
      "source": [
        "Unicode normalization will be the first step in the text standardization function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS0qmnWBBQf2"
      },
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['<sos>', text, '<eos>'], separator=' ')\n",
        "  return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCH-5wZkBQcI",
        "outputId": "c68c61d5-cac4-48fa-8339-ccd777a52ab4"
      },
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "¿Todavía está en casa?\n",
            "<sos> ¿ todavia esta en casa ? <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O00BLVxDSchJ"
      },
      "source": [
        "### Text Vectorization\n",
        "This standardization function will be wrapped up in a `preprocessing.TextVectorization` layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens.\n",
        "\n",
        "The `TextVectorization` layer and many other `experimental.preprocessing` layers have an adapt method. This method reads one epoch of the training data, and works a lot like `Model.fix`. This adapt method initializes the layer based on the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuqKJAl3BQaS"
      },
      "source": [
        "max_vocab_size = 10_000\n",
        "\n",
        "input_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size\n",
        "    )\n",
        "output_text_processor = preprocessing.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size\n",
        "    )\n",
        "input_text_processor.adapt(inp)\n",
        "output_text_processor.adapt(targ)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzVEosfrTCPz"
      },
      "source": [
        "The `preprocessing.TextVectorization` has a method called `get_vocabulary()` which list the number of uninque words in each source.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMaf9q4EBQXG",
        "outputId": "aa3180a8-fbfa-4933-b48a-1c569d73903b"
      },
      "source": [
        "print(input_text_processor.get_vocabulary()[:5])\n",
        "print(output_text_processor.get_vocabulary()[:5])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', '<sos>', '<eos>', '.']\n",
            "['', '[UNK]', '<sos>', '<eos>', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zHjYOy1BQVB",
        "outputId": "d0f4e3d6-3f23-4f41-8661-bce37c06f2a7"
      },
      "source": [
        "print(\"Vocab size french: \", len(input_text_processor.get_vocabulary()))\n",
        "print(\"Vocab size english: \", output_text_processor.vocabulary_size())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size french:  10000\n",
            "Vocab size english:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXU92CCVBrL"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE9RUzAfBQRZ",
        "outputId": "6af222c5-f769-4eb8-bea5-1114a277e5bc"
      },
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,  126,  258,   34,   21,  361,    9,   49,  703,   10],\n",
              "       [   2,   31, 2004,  314,    5,  320,  182,    4,    3,    0],\n",
              "       [   2,    5,   47,   10,   15, 2681,    4,    3,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC12q6_yVc0v"
      },
      "source": [
        "### Getting the vocabulary.\n",
        "We can use the `get_vocabulary()` method to convert tokens IDS \n",
        " back to text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RZD8tgY6BQPe",
        "outputId": "a21736c4-81cc-4c57-b5cb-76b2ced4eb02"
      },
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<sos> mais non ! tu nes pas si vieille que ca ! <eos>         '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6ylg0YSBQL5",
        "outputId": "f59fc324-5164-47a9-e0b4-ac898a494659"
      },
      "source": [
        "example_tokens[1]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(22,), dtype=int64, numpy=\n",
              "array([   2,   31, 2004,  314,    5,  320,  182,    4,    3,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryRFMNFWBG-8"
      },
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g55ued9kXm_y"
      },
      "source": [
        "### The `Enconder.`\n",
        "\n",
        "We are going to start by building the ecoder the blue part of the diagram. The encoder:\n",
        "\n",
        "1. Takes a list of tokens IDs (from `input_text_processor`)\n",
        "2. Looks up an embbeding vector for each token using the keras `Layers.Embedding`\n",
        "3. Processed the embeddings into a new sequence a `GRU` and then returns:\n",
        "* The processed sequence. This will be passed to the attention head.\n",
        "* The internal state. This will be used to initialize the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgG6RMIJXj36"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size,\n",
        "               embedding_dim,\n",
        "               enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.embedding = keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "    self.gru = keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    vectors = self.embedding(tokens)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    return output, state\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6PaINHCa6Li"
      },
      "source": [
        "### The attention head\n",
        "\n",
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. This attention layer is similar to a `layers.GlobalAveragePoling1D` but the attention layer performs a weighted average.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6lllUA1aVeH"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    w1_query = self.W1(query)\n",
        "    w2_key = self.W2(value)\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67BRomnbeXnl"
      },
      "source": [
        "### The Decoder.\n",
        "\n",
        "The decoder's job is to generate predictions for the next output token.\n",
        "\n",
        "1. The decoder receives the complete encoder output.\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "4. It combines the RNN output and the context vector using Equation 3 (below) to generate the \"attention vector\".\n",
        "5. It generates logit predictions for the next token based on the \"attention vector\".\n",
        "\n",
        "<img src=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/images/attention_equation_3.jpg?raw=1\" alt=\"attention equation 3\" width=\"800\">\n",
        "\n",
        "Here is the `Decoder` class and its initializer. The initializer creates all the necessary layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQeb68iCiTwn"
      },
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoY55Qs5arzM"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "    self.gru = keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    self.Wc = keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "    self.fc = keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None)  -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    logits = self.fc(attention_vector)\n",
        "    return DecoderOutput(logits, attention_weights), state"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l55slHharw5"
      },
      "source": [
        "The **encoder** processes its full input sequence with a single call to its RNN. This implementation of the **decoder** _can_ do that as well for efficient training. But this notebook will run the decoder in a loop for a few reasons:\n",
        "\n",
        "* Flexibility: Writing the loop gives you direct control over the training procedure.\n",
        "* Clarity: It's possible to do masking tricks and use `layers.RNN`, or `tfa.seq2seq` APIs to pack this all into a single call. But writing it out as a loop may be clearer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvQkLT0-j75d"
      },
      "source": [
        "### Training.\n",
        "Now that we have all the model components, it's time to start training the model. We'll need:\n",
        "\n",
        "- A loss function and optimizer to perform the optimization.\n",
        "- A training step function defining how to update the model for each input/target batch.\n",
        "- A training loop to drive the training and save checkpoints.\n",
        "\n",
        "### Loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHr0T_d2jzzn"
      },
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    \n",
        "  def __call__(self, y_true, y_pred):\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSzBtg0VkjCa"
      },
      "source": [
        "### The train step\n",
        "\n",
        "Start with a model class, the training process will be implemented as the `train_step` method on this model.\n",
        "\n",
        "Here the `train_step` method is a wrapper around the `_train_step` implementation which will come later. This wrapper includes a switch to turn on and off `tf.function` compilation, to make debugging easier.\n",
        "\n",
        "The `_train_step` method, handles the remaining steps except for actually running the decoder.\n",
        "\n",
        "The `_loop_step` method executes the decoder and calculates the incremental loss and new decoder state (`dec_state`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-SgI553kgQk"
      },
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)\n",
        "  \n",
        "  def _preprocess(self, input_text, target_text):\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "    target_mask = target_tokens != 0\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs  \n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "      \"\"\"\n",
        "      Initialize the decoder's state to the encoder's final state.\n",
        "      This only works if the encoder and decoder have the same number of\n",
        "      units.\n",
        "      \"\"\"\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "        \"\"\"\n",
        "        Pass in two tokens from the target sequence:\n",
        "        1. The current input to the decoder.\n",
        "        2. The target the target for the decoder's next prediction.\n",
        "        \"\"\"\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      # Average the loss over all non padding tokens.\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                  enc_output=enc_output,\n",
        "                                  mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "    return step_loss, dec_state\n",
        "\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _tf_train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiI7qQ4LlFm2"
      },
      "source": [
        "Overall the implementation for the `Model.train_step` method is as follows:\n",
        "\n",
        "1. Receive a batch of `input_text, target_text` from the `tf.data.Dataset`.\n",
        "2. Convert those raw text inputs to token-embeddings and masks. \n",
        "3. Run the encoder on the `input_tokens` to get the `encoder_output` and `encoder_state`.\n",
        "4. Initialize the decoder state and loss. \n",
        "5. Loop over the `target_tokens`:\n",
        "   1. Run the decoder one step at a time.\n",
        "   2. Calculate the loss for each step.\n",
        "   3. Accumulate the average loss.\n",
        "6. Calculate the gradient of the loss and use the optimizer to apply updates to the model's `trainable_variables`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNQOsOCLo5HA"
      },
      "source": [
        "Now that we're confident that the training step is working, build a fresh copy of the model to train from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71uRCfYLnZNx"
      },
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsmzVi2mo_-p"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "While there's nothing wrong with writing our own custom training loop, implementing the `Model.train_step` method, as in the previous section, allows you to run `Model.fit` and avoid rewriting all that boiler-plate code. \n",
        "\n",
        "This notebook only trains for a couple of epochs, so use a `callbacks.Callback` to collect the history of batch losses, for plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOd8MvpCnZIU"
      },
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qKbR-u0nZE_",
        "outputId": "b93d2c5c-9e64-4a9d-e8b3-65e4312c7e65"
      },
      "source": [
        "train_translator.fit(dataset, epochs=3,\n",
        "                     callbacks=[batch_loss])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1486/1486 [==============================] - 581s 385ms/step - batch_loss: 3.1434\n",
            "Epoch 2/3\n",
            "1486/1486 [==============================] - 566s 381ms/step - batch_loss: 1.6430\n",
            "Epoch 3/3\n",
            "1486/1486 [==============================] - 566s 381ms/step - batch_loss: 0.8756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8f0372d990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "1EttjYOqnZCC",
        "outputId": "a9c8c8a2-842c-441b-c17b-338336748131"
      },
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UvodSfypfz8"
      },
      "source": [
        "## Translate\n",
        "\n",
        "Now that the model is trained let's implement a function to execute the full `text => text` translation.\n",
        "\n",
        "For this the model needs to invert the `text => token IDs` mapping provided by the `output_text_processor`. It also needs to know the IDs for special tokens. This is all implemented in the constructor for the new class. The implementation of the actual translate method will follow.\n",
        "\n",
        "Overall this is similar to the training loop, except that the input to the decoder at each time step is a sample from the decoder's last prediction.\n",
        "\n",
        "### Implementing the translation loop.\n",
        "\n",
        "Here is a complete implementation of the text to text translation loop.\n",
        "\n",
        "This implementation collects the results into python lists, before using `tf.concat` to join them into tensors.\n",
        "\n",
        "This implementation statically unrolls the graph out to `max_length` iterations.\n",
        "This is okay with eager execution in python.\n",
        "\n",
        "If you want to export this model you'll need to wrap this method in a `tf.function`. This basic implementation has a few issues if you try to do that:\n",
        "\n",
        "1. The resulting graphs are very large and take a few seconds to build, save or load.\n",
        "2. You can't break from a statically unrolled loop, so it will always run `max_length` iterations, even if all the outputs are done. But even then it's marginally faster than eager execution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9POQukkRnY-s"
      },
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '<sos>']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string('<sos>')\n",
        "    self.end_token = index_from_string('<eos>')\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    return result_text\n",
        "\n",
        "  def sample(self, logits, temperature):\n",
        "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else: \n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "    return new_tokens\n",
        "\n",
        "  def translate(self,\n",
        "                  input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "      \n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      # If a sequence produces an `end_token`, set it `done`\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Convert the list of generates token ids to a list of strings.\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "  def tf_translate(self, input_text):\n",
        "    return self.translate(input_text)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIxPAHDCrCOd"
      },
      "source": [
        "### Translator instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANBupiLSnY7P"
      },
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3an-Uehti6c"
      },
      "source": [
        "### User input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg16VBZU7j68",
        "outputId": "d79a30a2-fc02-4aca-c13f-a284ab4cce5a"
      },
      "source": [
        "inp[500:505], targ[500:505], "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['lâchez-vous !',\n",
              "  'à terre !',\n",
              "  'au sol !',\n",
              "  'descends.',\n",
              "  \"va voir ailleurs si j'y suis\\u202f!\"],\n",
              " ['get down.', 'get down.', 'get down.', 'get down.', 'get lost!'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIJWejsbzSBo",
        "outputId": "cd8aefaf-0ebf-404c-d13f-91ead82da9c4"
      },
      "source": [
        "while True:\n",
        "  text = input(\"Enter a french sentence:\\n\")\n",
        "  if text.lower() == \"exit\":\n",
        "    break\n",
        "  text_cleaned = tf_lower_and_split_punct(text).numpy().decode()\n",
        "  input_text = tf.constant([text_cleaned])\n",
        "  print()\n",
        "  result = translator.translate(input_text)\n",
        "  result = result['text'][0].numpy().decode()\n",
        "  print(f\"French: > {text}\")\n",
        "  print(f\"English: < {result}\")\n",
        "  print()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a french sentence:\n",
            "lâchez-vous !\n",
            "\n",
            "French: > lâchez-vous !\n",
            "English: < beautiful speed .\n",
            "\n",
            "Enter a french sentence:\n",
            "Va\n",
            "\n",
            "French: > Va\n",
            "English: < lets do it right up .\n",
            "\n",
            "Enter a french sentence:\n",
            "va voir ailleurs si j'y suis\\u202f!\n",
            "\n",
            "French: > va voir ailleurs si j'y suis\\u202f!\n",
            "English: < go down to it until i come out .\n",
            "\n",
            "Enter a french sentence:\n",
            "exit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}