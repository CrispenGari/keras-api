{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_NMT_Transformers_Custom_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrF1N_UoWCek"
      },
      "source": [
        "### Nueral Machine Translation `(NMT)` with Transformers for language understanding.\n",
        "\n",
        "We are going to use the prevoious notebook as the base to load our own custom dataset to do NMT with transformers. We are foing to perform fench to english translation, so were are going to load the dataset from our google drive\n",
        "\n",
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344J2ke9VtKK",
        "outputId": "0e938c2d-5b26-4ed0-bc95-704d51e700a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1XvtAV7uV82J",
        "outputId": "d889d056-c82e-4207-faf4-3d88b8524922"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import string, sys, os, time, collections, pathlib, re\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwqY3gylM4KZ"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NjV_lBbMqDb",
        "outputId": "d9a33fc1-4ba5-42da-da4e-93896d4bbe81"
      },
      "source": [
        "base_path = \"/content/drive/My Drive/NLP Data/seq2seq/data/eng-fra.txt\"\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jaO1Jh3KM3XE",
        "outputId": "d5c31484-7de0-47ac-ffae-ee56b88bcd4d"
      },
      "source": [
        "lines = open(base_path, encoding=\"utf8\").read().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Go.\\tVa !'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZTb_XaiNI4O"
      },
      "source": [
        "We are going to normalize our corpus so for that we are going to use the following functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IfoNVZPNIZX"
      },
      "source": [
        "import unicodedata, re\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) \\\n",
        "                  if unicodedata.category(c) != \"Mn\")\n",
        "    \n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([!.?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!]+\", \" \", s)\n",
        "  return s"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1cEowGgNIRZ"
      },
      "source": [
        "en = []\n",
        "fr = []\n",
        "for line in lines:\n",
        "  try:\n",
        "    e, f = line.split(\"\\t\")\n",
        "    en.append(normalizeString(e))\n",
        "    fr.append(normalizeString(f))\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnNjXXmWNIO2",
        "outputId": "e3bffefb-c677-44c0-c25d-fca6bdbe6b26"
      },
      "source": [
        "len(en), len(fr)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(135842, 135842)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNOKLyDDeLkb",
        "outputId": "94f6de58-37b6-42e2-fb1c-288da233b8c3"
      },
      "source": [
        "input_data = fr\n",
        "target_data = en\n",
        "print(en[:5])\n",
        "print(fr[:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['go .', 'run !', 'run !', 'wow !', 'fire !']\n",
            "['va !', 'cours !', 'courez !', 'ca alors !', 'au feu !']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLtAqJqUecTZ"
      },
      "source": [
        "#### Tokenizing the data.\n",
        "Next, let's see how to prepare the data for our model. It is very simple and the steps are the following:\n",
        "\n",
        "* Create the vocabulary from the corpus using Subword tokenization, breaking words into â€œsubword unitsâ€ - strings of characters like ing or eau - that allow the downstream model to make intelligent decisions on words it doesnâ€™t recognize.\n",
        "* Calculate the maximum length of the input and output sequences.\n",
        "* Tokenize the data, convert the raw text into a sequence of integers. Once we define the vocabulary, we use the encode method to get the token for every word in the corpus.\n",
        "* Remove sentences longer that the max length defined.\n",
        "* Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvF0WW8jg8Ac"
      },
      "source": [
        "# Parameters for our model\n",
        "INPUT_COLUMN = 'input'\n",
        "TARGET_COLUMN = 'target'\n",
        "NUM_SAMPLES = 80000 #40000\n",
        "MAX_VOCAB_SIZE = 2**14\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 10  # Number of epochs to train for.\n",
        "MAX_LENGTH = 15"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2cjGijheLfC"
      },
      "source": [
        "def subword_tokenize(corpus, vocab_size, max_length):\n",
        "  # Create the vocabulary using Subword tokenization\n",
        "  tokenizer_corpus = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus, target_vocab_size=vocab_size  \n",
        "  )\n",
        "  # Get the final vocab size, adding the eos and sos tokens\n",
        "  num_words = tokenizer_corpus.vocab_size + 2\n",
        "  # Set eos and sos token\n",
        "  sos_token = [num_words-2]\n",
        "  eos_token = [num_words-1]\n",
        "  # Tokenize the corpus\n",
        "  sentences = [sos_token + tokenizer_corpus.encode(sentence) + eos_token\n",
        "          for sentence in corpus ]\n",
        "  # Identify the index of the sentences longer than max length\n",
        "  idx_to_remove = [count for count, sent in enumerate(sentences)\n",
        "                 if len(sent) > max_length]\n",
        "  \n",
        "  #  Pad the sentences\n",
        "  sentences = keras.preprocessing.sequence.pad_sequences(\n",
        "      sentences, value=0, padding=\"post\", maxlen=max_length\n",
        "  )\n",
        "  return sentences, tokenizer_corpus, num_words, sos_token, eos_token, idx_to_remove\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AifzyxAoeLZ2"
      },
      "source": [
        "# Tokenize and pad the input sequences\n",
        "encoder_inputs, tokenizer_inputs, num_words_inputs, sos_token_input, eos_token_input, del_idx_inputs= subword_tokenize(input_data, \n",
        "                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)\n",
        "# Tokenize and pad the outputs sequences\n",
        "decoder_outputs, tokenizer_outputs, num_words_output, sos_token_output, eos_token_output, del_idx_outputs = subword_tokenize(target_data, \n",
        "                                                                                                        MAX_VOCAB_SIZE, MAX_LENGTH)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM0JKDuDhJGg"
      },
      "source": [
        "\n",
        "### Check the tokenize function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0834Du3eLXZ",
        "outputId": "ecd80082-bd82-47aa-8364-1bae0169c6a3"
      },
      "source": [
        "# Check the tokenize function\n",
        "print(\"*************** encoder inputs **************\")\n",
        "print(encoder_inputs[:5], sos_token_input, eos_token_input)\n",
        "print(\"*************** decoder inputs **************\")\n",
        "print(decoder_outputs[:5], sos_token_output, eos_token_output)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** encoder inputs **************\n",
            "[[16167  2197    34 16168     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [16167  2006    34 16168     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [16167 15168 16033    34 16168     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [16167    63  2584    34 16168     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [16167    49   754    34 16168     0     0     0     0     0     0     0\n",
            "      0     0     0]] [16167] [16168]\n",
            "*************** decoder inputs **************\n",
            "[[18677   148     1 18678     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [18677  1264   115 18678     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [18677  1264   115 18678     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [18677  4501   115 18678     0     0     0     0     0     0     0     0\n",
            "      0     0     0]\n",
            " [18677   925   115 18678     0     0     0     0     0     0     0     0\n",
            "      0     0     0]] [18677] [18678]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV-liE7JhSBE"
      },
      "source": [
        "### Checking the vocabularies sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLJQduAseLTD",
        "outputId": "b98b9a74-432b-4e1b-fff6-877a2e9358e9"
      },
      "source": [
        "print('Size of Input Vocabulary: ', num_words_inputs)\n",
        "print('Size of Output Vocabulary: ', num_words_output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Input Vocabulary:  16169\n",
            "Size of Output Vocabulary:  18679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PDA4HxvhXL-"
      },
      "source": [
        "### Creating batch data generator\n",
        "Create a batch data generator:\n",
        "\n",
        "We want to train the model on batches, group of sentences, so we need to create a Dataset using the ``tf.data`` library and the function ``batch_on_slices`` on the input and output sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArXQovVfhzYM"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (encoder_inputs, decoder_outputs)\n",
        ").shuffle(len(input_data), reshuffle_each_iteration=True).batch(\n",
        "    BATCH_SIZE, drop_remainder=True).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcQVXup7hyvv"
      },
      "source": [
        "### Building a Transformer.\n",
        "\n",
        "What is a transformer?\n",
        "\n",
        "> _\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization â€¦ the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\"_ - **\"Attention is all you need\"** paper.\n",
        "\n",
        "The Transformer model extract features for each word using a self-attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. And no recurrent units are used to obtain this features, they are just weighted sum and activations, so they can be very parallelizable and efficient.\n",
        "\n",
        "\n",
        "### Self-attention: The Fundermentals operations.\n",
        "\n",
        "_\"Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let's call the input vectors ð±1,ð±2,â€¦ð±t and the corresponding output vectors ð²1,ð²2,â€¦,ð²t. The vectors all have dimension k. To produce output vector ð²i, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product.\"_\n",
        "\n",
        "#### Transformers from scratch by Peter Bloem\n",
        "\n",
        "In the self-attention mechanism of our model we need to introduce three elements: Queries, Values and Keys.\n",
        "\n",
        "Queries, Keys and Values\n",
        "\n",
        "Every input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. In every role, it is compared to the others vectors to get its own output yi (Query), to get the j-th output yj (Key) and to compute each output vector once the weights have been established (Value).\n",
        "\n",
        "To obtain this roles, we need three weight matrices of dimensions k x k and compute three linear transformation for each xi:\n",
        "\n",
        "![img](https://github.com/edumunozsala/Transformer-NMT/raw/d08ad5c2a251cc7e8521d278b2819551667c67a4/images/query_key_value.png)\n",
        "\n",
        "These three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a **\"self-attention\"**.\n",
        "\n",
        "### Scale dot-product Attention\n",
        "> _The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values._- **\"Attention is all you need\"** paper\n",
        "\n",
        "Then we use the Q, K and V matrices to calculate the attention scores. **The scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position.** That is, the dot product of the query vector with the key vector of the respective word we're scoring. So, for position 1 we calculate the dot product (.) of q1 and k1, then q1 . k2, q1 . k3,â€¦ \n",
        "\n",
        "Next we apply the \"scaled\" factor to have more stable gradients. The softmax function can not work properly with large values, resulting in vanishing the gradient and slow down the learning. After \"softmaxing\" we multiply by the Value matrix to keep the values of the words we want to focus on and minimizing or removing the values for the irrelevant words (its value in V matrix should be very small).\n",
        "\n",
        "The formula for these operations is:\n",
        "![img](https://render.githubusercontent.com/render/math?math=Attention%28Q%2C%20K%2C%20V%20%29%20%3D%20%5Ctext%7Bsoftmax%7D%5Cleft%28%5Cdfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%5Cright%29V&mode=inline)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1ZzX39DlWZT"
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  # Calculate the dot product, QK_transpose\n",
        "  product = tf.matmul(queries, keys, transpose_b=True)\n",
        "  # Get the scale factor\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "  # Apply the scale factor to the dot product\n",
        "  scaled_product = product/tf.math.sqrt(keys_dim)\n",
        "  # Apply mask when it is required\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "  # dot product with values\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "  return attention"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNwvjBoYmaCj"
      },
      "source": [
        "### Multi-Head Attention\n",
        "In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same result even if two sentences contain the same words in different order. Instead we would like to attent to different segments of the words. We can give the self attention greater power of discrimination, **by combining serveral self attention heads**, dividing the words vectors into a fixed number (h, number of heads) of chunks and then self-attention is applied on the corresponding chinks using the Q, K and V sub-matrices.\n",
        "\n",
        "This produce h different output matrices of scores.\n",
        "\n",
        "![img](https://github.com/edumunozsala/Transformer-NMT/raw/d08ad5c2a251cc7e8521d278b2819551667c67a4/images/dor_product_multihead.PNG)\n",
        "\n",
        "But the next layer (the Feed-Forward layer) is expecting just one matrix, a vector for each word, so after calculating the dot product of every head, we concat the output matrices and multiply them by an additional weights matrix $W_O$. This final matrix captures information from all the attention heads.\n",
        "\n",
        "$MultihHead(Q, K, V ) = \\text{Concat}(head_1,...,head_n)W^O$\n",
        "\n",
        "where $head_i=Attention(QW_i^Q,QW_i^K,QW_i^V)$ and $i$ is the head index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq4wDNjnlWVD"
      },
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "  def __init__(self, n_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.n_heads == 0\n",
        "    # Calculate the dimension of every head or projection\n",
        "    self.d_head = self.d_model // self.n_heads\n",
        "    # Set the weight matrices for Q, K and V\n",
        "    self.query_lin = keras.layers.Dense(units=self.d_model)\n",
        "    self.key_lin = keras.layers.Dense(units=self.d_model)\n",
        "    self.value_lin = keras.layers.Dense(units=self.d_model)\n",
        "    # Set the weight matrix for the output of the multi-head attention W0\n",
        "    self.final_lin = keras.layers.Dense(units=self.d_model)\n",
        "\n",
        "  def split_proj(self, inputs, batch_size):\n",
        "    # inputs: (batch_size, seq_length, d_model)\n",
        "    # Set the dimension of the projections\n",
        "    shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.n_heads,\n",
        "                 self.d_head)\n",
        "    splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "    return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    # Get the batch size\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "    # Set the Query, Key and Value matrices\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values)\n",
        "    # Split Q, K y V between the heads or projections\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "    # Apply the scaled dot product\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "    # Get the attention scores\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "    # Concat the h heads or projections\n",
        "    concat_attention = tf.reshape(attention,\n",
        "                                  shape=(batch_size, -1, self.d_model))\n",
        "    # Apply W0 to get the output of the multi-head attention\n",
        "    outputs = self.final_lin(concat_attention)\n",
        "    return outputs\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-fCTKbTqeju"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "We mentioned briefly that the order of the words in the sentence is an issue to solve in this model, because the network and the self-attention mechanism is permutation invariant. If we shuffle up the words in the input sentence, we get the same solutions. We need to create a representation of the position of the word in the sentence and add it to the word embedding.\n",
        "\n",
        "> _To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, so that the two can be summed. There are many choices of positional encodings._ - **\"Attention is all you need** paper\n",
        "\n",
        "So, we apply a function to map the position in the sentence to real valued vector. The network will learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. It would requiere sentences of all accepted positions during training but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered.\n",
        "\n",
        "In the paper a sinusoidal function is applied:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvBHfmgIlWSh"
      },
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "      # pos: (seq_length, 1) i: (1, d_model)\n",
        "      angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "      return pos * angles # (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "      # input shape batch_size, seq_length, d_model\n",
        "      seq_length = inputs.shape.as_list()[-2]\n",
        "      d_model = inputs.shape.as_list()[-1]\n",
        "      # Calculate the angles given the input\n",
        "      angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "      # Calculate the positional encodings\n",
        "      angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "      angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "      # Expand the encodings with a new dimension\n",
        "      pos_encoding = angles[np.newaxis, ...]\n",
        "      \n",
        "      return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo7oWyS4rqlS"
      },
      "source": [
        "### The Encoder\n",
        "Now that all the main pieces of the model have been described we can introduce the encoder components\n",
        "* Positional encoding: Add the position encoding to the input embedding (our input words are transformed to embedding vectors). _\"The same weight matrix is shared between the two embedding layers (encoder and decoder) and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by square root of the model dimension\"_ [1], ${\\sqrt{d_{model}}}$.\n",
        "\n",
        "* N = 6, identical layers, containing two sub-layers: a multi-head self-attention mechanism, and a fully connected feed-forward network. This FC layer is applied to each position separately and identically and consists of two linear transformations with a ReLU activation in between. But it is applied position-wise to the input, which means that the same neural network is applied to every single \"token\" vector belonging to the sentence sequence.\n",
        "\n",
        "$$FFN(x)= max(0,xW_1+b_1)W_2+b_2$$\n",
        "* There is a residual connection around each sub-layer (attention and FC network) followed by a layer normalization.\n",
        "\n",
        "\n",
        "_Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only_. - Peter Bloem, **\"Transformers from scratch\"**\n",
        "\n",
        "The next figure will show the components detailed:\n",
        "\n",
        "![img](https://github.com/edumunozsala/Transformer-NMT/raw/d08ad5c2a251cc7e8521d278b2819551667c67a4/images/encoder.PNG)\n",
        "\n",
        "Keep in mind that **only the vector from the last layer (6-th) is sent to the decoder**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIdnVVzjlWQZ"
      },
      "source": [
        "class EncoderLayer(keras.layers.Layer):\n",
        "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Hidden units of the feed forward component\n",
        "        self.FFN_units = FFN_units\n",
        "        # Set the number of projectios or heads\n",
        "        self.n_heads = n_heads\n",
        "        # Dropout rate\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # Build the multihead layer\n",
        "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = keras.layers.Dropout(rate=self.dropout_rate)\n",
        "        # Layer Normalization\n",
        "        self.norm_1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Fully connected feed forward layer\n",
        "        self.ffn1_relu = keras.layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.ffn2 = keras.layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = keras.layers.Dropout(rate=self.dropout_rate)\n",
        "        # Layer normalization\n",
        "        self.norm_2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        # Forward pass of the multi-head attention\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # Call to the residual connection and layer normalization\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        # Call to the FC layer\n",
        "        outputs = self.ffn1_relu(attention)\n",
        "        outputs = self.ffn2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        # Call to residual connection and the layer normalization\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QQ3Qgp2lWMU"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 FFN_units,\n",
        "                 n_heads,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        # The embedding layer\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = keras.layers.Dropout(rate=dropout_rate)\n",
        "        # Stack of n layers of multi-head attention and FC\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        n_heads,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(n_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        # Get the embedding vectors\n",
        "        outputs = self.embedding(inputs)\n",
        "        # Scale the embeddings by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Positional encodding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        # Call the stacked layers\n",
        "        for i in range(self.n_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JtxllL6s-P6"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder share some components with the encoder but they are used in a different way to take into account the encoder output.\n",
        "\n",
        "* Positional encoding: Similar that the one in the encoder\n",
        "* N=6 identical layers, containing 3 three sublayers. First, the Masked Multi-head attention or masked causal attention to prevent positions from attending to subsequent positions, hiding those features that belong to future states of the sequence. \"This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i\" [1]. It is implemented setting to âˆ’âˆž the values corresponding to the forbidden states in the softmax layer of the dot-product attention modules. The second component or \"encoder-decoder attention\" performs multi-head attention over the output of the decoder, the Key and Value vectors come from the output of the encoder but the queries come from the previous decoder layer. This allows every position in the decoder to attend over all positions in the input sequence [1]. And finally the fully-connected network.\n",
        "\n",
        "* The residual connection and layer normalization around each sub-layer, similar to the encoder.\n",
        "\n",
        "![img](https://github.com/edumunozsala/Transformer-NMT/raw/d08ad5c2a251cc7e8521d278b2819551667c67a4/images/decoder.PNG)\n",
        "\n",
        "At the end of the N stacked decoders, the linear layer, a fully-connected network, transforms the stacked outputs to a much larger vector, the logits. The softmax layer then turns those scores (logits) into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuaj3DmMlWJE"
      },
      "source": [
        "class DecoderLayer(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, n_heads, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention, causal attention\n",
        "        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = keras.layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention, encoder-decoder attention \n",
        "        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_2 = keras.layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.ffn1_relu = keras.layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.ffn2 = keras.layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = keras.layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # Call the masked causal attention\n",
        "        attention = self.multi_head_causal_attention(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        # Residual connection and layer normalization\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        # Call the encoder-decoder attention\n",
        "        attention_2 = self.multi_head_enc_dec_attention(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        # Residual connection and layer normalization\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        # Call the Feed forward\n",
        "        outputs = self.ffn1_relu(attention_2)\n",
        "        outputs = self.ffn2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        # Residual connection and layer normalization\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t9c-Cu4lWGN"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 FFN_units,\n",
        "                 n_heads,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        # Embedding layer\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding layer\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = keras.layers.Dropout(rate=dropout_rate)\n",
        "        # Stacked layers of multi-head attention and feed forward\n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        n_heads,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(n_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # Get the embedding vectors\n",
        "        outputs = self.embedding(inputs)\n",
        "        # Scale by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # Positional encodding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        # Call the stacked layers\n",
        "        for i in range(self.n_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQHVNUI2tpS0"
      },
      "source": [
        "### Transformer Model\n",
        "Once we have defined our components and created the encoder, the decoder and the linear-softmax final layer, we join the pieces to form our model, the Transformer.\n",
        "\n",
        "![img](https://github.com/edumunozsala/Transformer-NMT/raw/d08ad5c2a251cc7e8521d278b2819551667c67a4/images/transformer_architecture.PNG)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXg0s8IrlWEb"
      },
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 n_layers,\n",
        "                 FFN_units,\n",
        "                 n_heads,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        # Build the encoder\n",
        "        self.encoder = Encoder(n_layers,\n",
        "                               FFN_units,\n",
        "                               n_heads,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        # Build the decoder\n",
        "        self.decoder = Decoder(n_layers,\n",
        "                               FFN_units,\n",
        "                               n_heads,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        # build the linear transformation and softmax function\n",
        "        self.last_linear = keras.layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
        "        # Create the mask for padding\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        # Create the mask for the causal attention\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # Create the padding mask for the encoder\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        # Create the mask for the causal attention\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        # Create the mask for the encoder-decoder attention\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        # Call the encoder\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        # Call the decoder\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        # Call the Linear and Softmax functions\n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDIyEml-uKO6"
      },
      "source": [
        "It is worth mentioning that we create 3 masks, each of which will allow us:\n",
        "\n",
        "* Encoder mask: It is a padding mask to discard the pad tokens from the attention calculation.\n",
        "* Decoder mask 1: this mask is a union of the padding mask and the look ahead mask which will help the causal attention to discard the tokens \"in the future\". We take the maximum value between the padding mask and the look ahead one.\n",
        "* Decoder mask 2: it is the padding mask and is applied in the encoder-decoder attention layer.\n",
        "As you can see then we call the encoder, the decoder and the final linear-softmax layer to get the predicted output from our Transformer model.\n",
        "\n",
        "### Training the Transformer model\n",
        "Now that we have described in detail the components in the paper we are ready to implement them and train a transformer model on a NMT problem. It is a toy problem for educational purposes.\n",
        "\n",
        "We need to create a custom loss function to mask the padding tokens and we define the Adam optimizer described in the paper, with beta1 = 0.9, beta2 = 0.98 and epsilon= 10e-9. And then we create a scheduler to vary the learning rate over the training process according to:\n",
        "\n",
        "$lrate = d_{model}^{-0.5}*min(step\\_num^{-0.5}, step\\_num*warmup\\_steps^{-1.5})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEIV5TFNlWB7"
      },
      "source": [
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQI_SpJ7uWxt"
      },
      "source": [
        "And that's all, we have all the necessary elements to train our model using an usual loop for sequence-to-sequence tasks:\n",
        "\n",
        "* For every iteration on the batch generator that produce batch size inputs and outputs\n",
        "* Get the input sequence from 0 to length-1 and the actual outputs from 1 to length, the next word expected at every sequence step.\n",
        "* Call the transformer to get the predictions\n",
        "* Calculate the loss function between the real outputs and the predictions\n",
        "* Apply the gradients to update the weights in the model\n",
        "* Calculate the mean loss and the accuracy for the batch data\n",
        "* Show some results and save the model in every epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiZ2wLf5lV9g"
      },
      "source": [
        "def fit(dataset, transformer, n_epochs, print_every=50):\n",
        "  ''' Train the transformer model for n_epochs using the data generator dataset'''\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  # In every epoch\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    # Reset the losss and accuracy calculations\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    # Get a batch of inputs and targets\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        # Set the decoder inputs\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        # Set the target outputs, right shifted\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Call the transformer and get the predicted output\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            # Calculate the loss\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        # Update the weights and optimizer\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        # Save and store the metrics\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % print_every == 0:\n",
        "            losses.append(train_loss.result())\n",
        "            accuracies.append(train_accuracy.result())\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    # Checkpoint the model on every epoch        \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} in {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\n",
        "\n",
        "  return losses, accuracies\n",
        "  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcUxBay7umEa"
      },
      "source": [
        "\n",
        "Setting the hyperparameters and parameters of the model and training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHnWQTOUlV57"
      },
      "source": [
        "# Set hyperparamters for the model\n",
        "D_MODEL = 512 # 512\n",
        "N_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "N_HEADS = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k2eJgjduqTJ"
      },
      "source": [
        "Now we define and create all the elements to train the model and evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYfjVUT0ure-"
      },
      "source": [
        "# Clean the session\n",
        "tf.keras.backend.clear_session()\n",
        "# Create the Transformer model\n",
        "transformer = Transformer(vocab_size_enc=num_words_inputs,\n",
        "                          vocab_size_dec=num_words_output,\n",
        "                          d_model=D_MODEL,\n",
        "                          n_layers=N_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          n_heads=N_HEADS,\n",
        "                          dropout_rate=DROPOUT_RATE)\n",
        "\n",
        "# Define a categorical cross entropy loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "# Define a metric to store the mean loss of every epoch\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# Define a matric to save the accuracy in every epoch\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "# Create the scheduler for learning rate decay\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "# Create the Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY4F0Tqouvte"
      },
      "source": [
        "It is very useful to checkpoint and save our model during training. Training can take a lot of time and we can restore the model for future training or use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odo-yL4VurcG"
      },
      "source": [
        "#Create the Checkpoint \n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, 'ckpt/', max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Last checkpoint restored.\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxrYk0vQurZ1",
        "outputId": "a9e50888-567a-4442-9a74-d5cea6ff5c87"
      },
      "source": [
        "losses, accuracies = fit(dataset, transformer, EPOCHS, 100)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.8194 Accuracy 0.0000\n",
            "Epoch 1 Batch 100 Loss 5.6208 Accuracy 0.0538\n",
            "Epoch 1 Batch 200 Loss 5.1831 Accuracy 0.0699\n",
            "Epoch 1 Batch 300 Loss 4.6925 Accuracy 0.0933\n",
            "Epoch 1 Batch 400 Loss 4.2875 Accuracy 0.1158\n",
            "Epoch 1 Batch 500 Loss 3.9896 Accuracy 0.1353\n",
            "Epoch 1 Batch 600 Loss 3.7547 Accuracy 0.1519\n",
            "Epoch 1 Batch 700 Loss 3.5670 Accuracy 0.1656\n",
            "Epoch 1 Batch 800 Loss 3.4112 Accuracy 0.1772\n",
            "Epoch 1 Batch 900 Loss 3.2785 Accuracy 0.1877\n",
            "Epoch 1 Batch 1000 Loss 3.1635 Accuracy 0.1970\n",
            "Epoch 1 Batch 1100 Loss 3.0617 Accuracy 0.2051\n",
            "Epoch 1 Batch 1200 Loss 2.9733 Accuracy 0.2126\n",
            "Epoch 1 Batch 1300 Loss 2.8938 Accuracy 0.2196\n",
            "Epoch 1 Batch 1400 Loss 2.8210 Accuracy 0.2258\n",
            "Epoch 1 Batch 1500 Loss 2.7554 Accuracy 0.2315\n",
            "Epoch 1 Batch 1600 Loss 2.6955 Accuracy 0.2369\n",
            "Epoch 1 Batch 1700 Loss 2.6404 Accuracy 0.2420\n",
            "Epoch 1 Batch 1800 Loss 2.5864 Accuracy 0.2467\n",
            "Epoch 1 Batch 1900 Loss 2.5391 Accuracy 0.2510\n",
            "Epoch 1 Batch 2000 Loss 2.4951 Accuracy 0.2550\n",
            "Epoch 1 Batch 2100 Loss 2.4541 Accuracy 0.2588\n",
            "Saving checkpoint for epoch 1 in ckpt/ckpt-1\n",
            "Time for 1 epoch: 1082.4298751354218 secs\n",
            "\n",
            "Starting epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.5677 Accuracy 0.3214\n",
            "Epoch 2 Batch 100 Loss 1.5040 Accuracy 0.3454\n",
            "Epoch 2 Batch 200 Loss 1.4969 Accuracy 0.3484\n",
            "Epoch 2 Batch 300 Loss 1.4905 Accuracy 0.3505\n",
            "Epoch 2 Batch 400 Loss 1.4728 Accuracy 0.3530\n",
            "Epoch 2 Batch 500 Loss 1.4636 Accuracy 0.3544\n",
            "Epoch 2 Batch 600 Loss 1.4598 Accuracy 0.3562\n",
            "Epoch 2 Batch 700 Loss 1.4466 Accuracy 0.3579\n",
            "Epoch 2 Batch 800 Loss 1.4377 Accuracy 0.3592\n",
            "Epoch 2 Batch 900 Loss 1.4295 Accuracy 0.3602\n",
            "Epoch 2 Batch 1000 Loss 1.4202 Accuracy 0.3617\n",
            "Epoch 2 Batch 1100 Loss 1.4114 Accuracy 0.3630\n",
            "Epoch 2 Batch 1200 Loss 1.4034 Accuracy 0.3640\n",
            "Epoch 2 Batch 1300 Loss 1.3974 Accuracy 0.3650\n",
            "Epoch 2 Batch 1400 Loss 1.3924 Accuracy 0.3660\n",
            "Epoch 2 Batch 1500 Loss 1.3864 Accuracy 0.3669\n",
            "Epoch 2 Batch 1600 Loss 1.3811 Accuracy 0.3678\n",
            "Epoch 2 Batch 1700 Loss 1.3766 Accuracy 0.3685\n",
            "Epoch 2 Batch 1800 Loss 1.3719 Accuracy 0.3692\n",
            "Epoch 2 Batch 1900 Loss 1.3673 Accuracy 0.3699\n",
            "Epoch 2 Batch 2000 Loss 1.3633 Accuracy 0.3704\n",
            "Epoch 2 Batch 2100 Loss 1.3598 Accuracy 0.3708\n",
            "Saving checkpoint for epoch 2 in ckpt/ckpt-2\n",
            "Time for 1 epoch: 953.6725041866302 secs\n",
            "\n",
            "Starting epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.0270 Accuracy 0.3761\n",
            "Epoch 3 Batch 100 Loss 1.1647 Accuracy 0.3926\n",
            "Epoch 3 Batch 200 Loss 1.1669 Accuracy 0.3926\n",
            "Epoch 3 Batch 300 Loss 1.1725 Accuracy 0.3917\n",
            "Epoch 3 Batch 400 Loss 1.1734 Accuracy 0.3930\n",
            "Epoch 3 Batch 500 Loss 1.1740 Accuracy 0.3930\n",
            "Epoch 3 Batch 600 Loss 1.1692 Accuracy 0.3939\n",
            "Epoch 3 Batch 700 Loss 1.1649 Accuracy 0.3944\n",
            "Epoch 3 Batch 800 Loss 1.1647 Accuracy 0.3950\n",
            "Epoch 3 Batch 900 Loss 1.1603 Accuracy 0.3957\n",
            "Epoch 3 Batch 1000 Loss 1.1589 Accuracy 0.3962\n",
            "Epoch 3 Batch 1100 Loss 1.1553 Accuracy 0.3967\n",
            "Epoch 3 Batch 1200 Loss 1.1530 Accuracy 0.3973\n",
            "Epoch 3 Batch 1300 Loss 1.1495 Accuracy 0.3977\n",
            "Epoch 3 Batch 1400 Loss 1.1474 Accuracy 0.3981\n",
            "Epoch 3 Batch 1500 Loss 1.1441 Accuracy 0.3986\n",
            "Epoch 3 Batch 1600 Loss 1.1409 Accuracy 0.3992\n",
            "Epoch 3 Batch 1700 Loss 1.1369 Accuracy 0.3997\n",
            "Epoch 3 Batch 1800 Loss 1.1339 Accuracy 0.4002\n",
            "Epoch 3 Batch 1900 Loss 1.1302 Accuracy 0.4007\n",
            "Epoch 3 Batch 2000 Loss 1.1271 Accuracy 0.4010\n",
            "Epoch 3 Batch 2100 Loss 1.1238 Accuracy 0.4014\n",
            "Saving checkpoint for epoch 3 in ckpt/ckpt-3\n",
            "Time for 1 epoch: 932.2165117263794 secs\n",
            "\n",
            "Starting epoch 4\n",
            "Epoch 4 Batch 0 Loss 0.9817 Accuracy 0.4107\n",
            "Epoch 4 Batch 100 Loss 0.9840 Accuracy 0.4183\n",
            "Epoch 4 Batch 200 Loss 0.9712 Accuracy 0.4198\n",
            "Epoch 4 Batch 300 Loss 0.9708 Accuracy 0.4202\n",
            "Epoch 4 Batch 400 Loss 0.9752 Accuracy 0.4209\n",
            "Epoch 4 Batch 500 Loss 0.9742 Accuracy 0.4208\n",
            "Epoch 4 Batch 600 Loss 0.9776 Accuracy 0.4204\n",
            "Epoch 4 Batch 700 Loss 0.9774 Accuracy 0.4203\n",
            "Epoch 4 Batch 800 Loss 0.9809 Accuracy 0.4205\n",
            "Epoch 4 Batch 900 Loss 0.9816 Accuracy 0.4206\n",
            "Epoch 4 Batch 1000 Loss 0.9818 Accuracy 0.4210\n",
            "Epoch 4 Batch 1100 Loss 0.9805 Accuracy 0.4214\n",
            "Epoch 4 Batch 1200 Loss 0.9798 Accuracy 0.4215\n",
            "Epoch 4 Batch 1300 Loss 0.9797 Accuracy 0.4216\n",
            "Epoch 4 Batch 1400 Loss 0.9790 Accuracy 0.4218\n",
            "Epoch 4 Batch 1500 Loss 0.9787 Accuracy 0.4221\n",
            "Epoch 4 Batch 1600 Loss 0.9787 Accuracy 0.4222\n",
            "Epoch 4 Batch 1700 Loss 0.9780 Accuracy 0.4221\n",
            "Epoch 4 Batch 1800 Loss 0.9771 Accuracy 0.4220\n",
            "Epoch 4 Batch 1900 Loss 0.9771 Accuracy 0.4223\n",
            "Epoch 4 Batch 2000 Loss 0.9759 Accuracy 0.4226\n",
            "Epoch 4 Batch 2100 Loss 0.9749 Accuracy 0.4227\n",
            "Saving checkpoint for epoch 4 in ckpt/ckpt-4\n",
            "Time for 1 epoch: 916.8218710422516 secs\n",
            "\n",
            "Starting epoch 5\n",
            "Epoch 5 Batch 0 Loss 0.7335 Accuracy 0.4710\n",
            "Epoch 5 Batch 100 Loss 0.8700 Accuracy 0.4362\n",
            "Epoch 5 Batch 200 Loss 0.8712 Accuracy 0.4364\n",
            "Epoch 5 Batch 300 Loss 0.8716 Accuracy 0.4362\n",
            "Epoch 5 Batch 400 Loss 0.8758 Accuracy 0.4364\n",
            "Epoch 5 Batch 500 Loss 0.8776 Accuracy 0.4362\n",
            "Epoch 5 Batch 600 Loss 0.8803 Accuracy 0.4359\n",
            "Epoch 5 Batch 700 Loss 0.8820 Accuracy 0.4361\n",
            "Epoch 5 Batch 800 Loss 0.8880 Accuracy 0.4360\n",
            "Epoch 5 Batch 900 Loss 0.8888 Accuracy 0.4356\n",
            "Epoch 5 Batch 1000 Loss 0.8891 Accuracy 0.4358\n",
            "Epoch 5 Batch 1100 Loss 0.8886 Accuracy 0.4358\n",
            "Epoch 5 Batch 1200 Loss 0.8880 Accuracy 0.4359\n",
            "Epoch 5 Batch 1300 Loss 0.8889 Accuracy 0.4360\n",
            "Epoch 5 Batch 1400 Loss 0.8884 Accuracy 0.4360\n",
            "Epoch 5 Batch 1500 Loss 0.8879 Accuracy 0.4362\n",
            "Epoch 5 Batch 1600 Loss 0.8871 Accuracy 0.4362\n",
            "Epoch 5 Batch 1700 Loss 0.8883 Accuracy 0.4362\n",
            "Epoch 5 Batch 1800 Loss 0.8892 Accuracy 0.4363\n",
            "Epoch 5 Batch 1900 Loss 0.8882 Accuracy 0.4363\n",
            "Epoch 5 Batch 2000 Loss 0.8886 Accuracy 0.4364\n",
            "Epoch 5 Batch 2100 Loss 0.8893 Accuracy 0.4364\n",
            "Saving checkpoint for epoch 5 in ckpt/ckpt-5\n",
            "Time for 1 epoch: 983.9613447189331 secs\n",
            "\n",
            "Starting epoch 6\n",
            "Epoch 6 Batch 0 Loss 0.8287 Accuracy 0.4743\n",
            "Epoch 6 Batch 100 Loss 0.7838 Accuracy 0.4518\n",
            "Epoch 6 Batch 200 Loss 0.7935 Accuracy 0.4499\n",
            "Epoch 6 Batch 300 Loss 0.7922 Accuracy 0.4501\n",
            "Epoch 6 Batch 400 Loss 0.7961 Accuracy 0.4496\n",
            "Epoch 6 Batch 500 Loss 0.7967 Accuracy 0.4497\n",
            "Epoch 6 Batch 600 Loss 0.8040 Accuracy 0.4495\n",
            "Epoch 6 Batch 700 Loss 0.8066 Accuracy 0.4492\n",
            "Epoch 6 Batch 800 Loss 0.8084 Accuracy 0.4489\n",
            "Epoch 6 Batch 900 Loss 0.8120 Accuracy 0.4489\n",
            "Epoch 6 Batch 1000 Loss 0.8148 Accuracy 0.4486\n",
            "Epoch 6 Batch 1100 Loss 0.8173 Accuracy 0.4482\n",
            "Epoch 6 Batch 1200 Loss 0.8187 Accuracy 0.4480\n",
            "Epoch 6 Batch 1300 Loss 0.8194 Accuracy 0.4480\n",
            "Epoch 6 Batch 1400 Loss 0.8199 Accuracy 0.4478\n",
            "Epoch 6 Batch 1500 Loss 0.8210 Accuracy 0.4477\n",
            "Epoch 6 Batch 1600 Loss 0.8220 Accuracy 0.4477\n",
            "Epoch 6 Batch 1700 Loss 0.8234 Accuracy 0.4477\n",
            "Epoch 6 Batch 1800 Loss 0.8232 Accuracy 0.4478\n",
            "Epoch 6 Batch 1900 Loss 0.8225 Accuracy 0.4479\n",
            "Epoch 6 Batch 2000 Loss 0.8219 Accuracy 0.4478\n",
            "Epoch 6 Batch 2100 Loss 0.8225 Accuracy 0.4478\n",
            "Saving checkpoint for epoch 6 in ckpt/ckpt-6\n",
            "Time for 1 epoch: 915.140415430069 secs\n",
            "\n",
            "Starting epoch 7\n",
            "Epoch 7 Batch 0 Loss 0.7953 Accuracy 0.4565\n",
            "Epoch 7 Batch 100 Loss 0.7300 Accuracy 0.4608\n",
            "Epoch 7 Batch 200 Loss 0.7443 Accuracy 0.4596\n",
            "Epoch 7 Batch 300 Loss 0.7439 Accuracy 0.4597\n",
            "Epoch 7 Batch 400 Loss 0.7448 Accuracy 0.4594\n",
            "Epoch 7 Batch 500 Loss 0.7497 Accuracy 0.4590\n",
            "Epoch 7 Batch 600 Loss 0.7491 Accuracy 0.4591\n",
            "Epoch 7 Batch 700 Loss 0.7551 Accuracy 0.4584\n",
            "Epoch 7 Batch 800 Loss 0.7575 Accuracy 0.4586\n",
            "Epoch 7 Batch 900 Loss 0.7589 Accuracy 0.4582\n",
            "Epoch 7 Batch 1000 Loss 0.7580 Accuracy 0.4585\n",
            "Epoch 7 Batch 1100 Loss 0.7592 Accuracy 0.4583\n",
            "Epoch 7 Batch 1200 Loss 0.7599 Accuracy 0.4584\n",
            "Epoch 7 Batch 1300 Loss 0.7621 Accuracy 0.4581\n",
            "Epoch 7 Batch 1400 Loss 0.7641 Accuracy 0.4579\n",
            "Epoch 7 Batch 1500 Loss 0.7646 Accuracy 0.4577\n",
            "Epoch 7 Batch 1600 Loss 0.7652 Accuracy 0.4575\n",
            "Epoch 7 Batch 1700 Loss 0.7660 Accuracy 0.4574\n",
            "Epoch 7 Batch 1800 Loss 0.7675 Accuracy 0.4576\n",
            "Epoch 7 Batch 1900 Loss 0.7681 Accuracy 0.4575\n",
            "Epoch 7 Batch 2000 Loss 0.7695 Accuracy 0.4574\n",
            "Epoch 7 Batch 2100 Loss 0.7714 Accuracy 0.4572\n",
            "Saving checkpoint for epoch 7 in ckpt/ckpt-7\n",
            "Time for 1 epoch: 922.2866609096527 secs\n",
            "\n",
            "Starting epoch 8\n",
            "Epoch 8 Batch 0 Loss 0.6597 Accuracy 0.4665\n",
            "Epoch 8 Batch 100 Loss 0.6910 Accuracy 0.4705\n",
            "Epoch 8 Batch 200 Loss 0.6881 Accuracy 0.4711\n",
            "Epoch 8 Batch 300 Loss 0.6894 Accuracy 0.4699\n",
            "Epoch 8 Batch 400 Loss 0.6988 Accuracy 0.4693\n",
            "Epoch 8 Batch 500 Loss 0.7012 Accuracy 0.4684\n",
            "Epoch 8 Batch 600 Loss 0.7045 Accuracy 0.4682\n",
            "Epoch 8 Batch 700 Loss 0.7094 Accuracy 0.4678\n",
            "Epoch 8 Batch 800 Loss 0.7116 Accuracy 0.4670\n",
            "Epoch 8 Batch 900 Loss 0.7117 Accuracy 0.4673\n",
            "Epoch 8 Batch 1000 Loss 0.7143 Accuracy 0.4670\n",
            "Epoch 8 Batch 1100 Loss 0.7160 Accuracy 0.4669\n",
            "Epoch 8 Batch 1200 Loss 0.7181 Accuracy 0.4664\n",
            "Epoch 8 Batch 1300 Loss 0.7196 Accuracy 0.4660\n",
            "Epoch 8 Batch 1400 Loss 0.7217 Accuracy 0.4659\n",
            "Epoch 8 Batch 1500 Loss 0.7227 Accuracy 0.4657\n",
            "Epoch 8 Batch 1600 Loss 0.7230 Accuracy 0.4655\n",
            "Epoch 8 Batch 1700 Loss 0.7258 Accuracy 0.4653\n",
            "Epoch 8 Batch 1800 Loss 0.7273 Accuracy 0.4652\n",
            "Epoch 8 Batch 1900 Loss 0.7288 Accuracy 0.4650\n",
            "Epoch 8 Batch 2000 Loss 0.7293 Accuracy 0.4649\n",
            "Epoch 8 Batch 2100 Loss 0.7300 Accuracy 0.4648\n",
            "Saving checkpoint for epoch 8 in ckpt/ckpt-8\n",
            "Time for 1 epoch: 878.410551071167 secs\n",
            "\n",
            "Starting epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.7484 Accuracy 0.4676\n",
            "Epoch 9 Batch 100 Loss 0.6511 Accuracy 0.4753\n",
            "Epoch 9 Batch 200 Loss 0.6489 Accuracy 0.4747\n",
            "Epoch 9 Batch 300 Loss 0.6594 Accuracy 0.4749\n",
            "Epoch 9 Batch 400 Loss 0.6655 Accuracy 0.4736\n",
            "Epoch 9 Batch 500 Loss 0.6728 Accuracy 0.4726\n",
            "Epoch 9 Batch 600 Loss 0.6757 Accuracy 0.4723\n",
            "Epoch 9 Batch 700 Loss 0.6780 Accuracy 0.4722\n",
            "Epoch 9 Batch 800 Loss 0.6799 Accuracy 0.4721\n",
            "Epoch 9 Batch 900 Loss 0.6807 Accuracy 0.4719\n",
            "Epoch 9 Batch 1000 Loss 0.6830 Accuracy 0.4722\n",
            "Epoch 9 Batch 1100 Loss 0.6841 Accuracy 0.4719\n",
            "Epoch 9 Batch 1200 Loss 0.6851 Accuracy 0.4720\n",
            "Epoch 9 Batch 1300 Loss 0.6863 Accuracy 0.4717\n",
            "Epoch 9 Batch 1400 Loss 0.6884 Accuracy 0.4715\n",
            "Epoch 9 Batch 1500 Loss 0.6895 Accuracy 0.4716\n",
            "Epoch 9 Batch 1600 Loss 0.6911 Accuracy 0.4713\n",
            "Epoch 9 Batch 1700 Loss 0.6929 Accuracy 0.4712\n",
            "Epoch 9 Batch 1800 Loss 0.6938 Accuracy 0.4712\n",
            "Epoch 9 Batch 1900 Loss 0.6957 Accuracy 0.4711\n",
            "Epoch 9 Batch 2000 Loss 0.6969 Accuracy 0.4709\n",
            "Epoch 9 Batch 2100 Loss 0.6983 Accuracy 0.4708\n",
            "Saving checkpoint for epoch 9 in ckpt/ckpt-9\n",
            "Time for 1 epoch: 946.9391837120056 secs\n",
            "\n",
            "Starting epoch 10\n",
            "Epoch 10 Batch 0 Loss 0.7627 Accuracy 0.4710\n",
            "Epoch 10 Batch 100 Loss 0.6267 Accuracy 0.4840\n",
            "Epoch 10 Batch 200 Loss 0.6228 Accuracy 0.4831\n",
            "Epoch 10 Batch 300 Loss 0.6316 Accuracy 0.4817\n",
            "Epoch 10 Batch 400 Loss 0.6381 Accuracy 0.4814\n",
            "Epoch 10 Batch 500 Loss 0.6446 Accuracy 0.4801\n",
            "Epoch 10 Batch 600 Loss 0.6502 Accuracy 0.4795\n",
            "Epoch 10 Batch 700 Loss 0.6536 Accuracy 0.4786\n",
            "Epoch 10 Batch 800 Loss 0.6528 Accuracy 0.4782\n",
            "Epoch 10 Batch 900 Loss 0.6540 Accuracy 0.4781\n",
            "Epoch 10 Batch 1000 Loss 0.6559 Accuracy 0.4775\n",
            "Epoch 10 Batch 1100 Loss 0.6585 Accuracy 0.4769\n",
            "Epoch 10 Batch 1200 Loss 0.6590 Accuracy 0.4770\n",
            "Epoch 10 Batch 1300 Loss 0.6605 Accuracy 0.4768\n",
            "Epoch 10 Batch 1400 Loss 0.6636 Accuracy 0.4766\n",
            "Epoch 10 Batch 1500 Loss 0.6650 Accuracy 0.4766\n",
            "Epoch 10 Batch 1600 Loss 0.6665 Accuracy 0.4763\n",
            "Epoch 10 Batch 1700 Loss 0.6672 Accuracy 0.4763\n",
            "Epoch 10 Batch 1800 Loss 0.6675 Accuracy 0.4763\n",
            "Epoch 10 Batch 1900 Loss 0.6686 Accuracy 0.4761\n",
            "Epoch 10 Batch 2000 Loss 0.6698 Accuracy 0.4761\n",
            "Epoch 10 Batch 2100 Loss 0.6714 Accuracy 0.4759\n",
            "Saving checkpoint for epoch 10 in ckpt/ckpt-10\n",
            "Time for 1 epoch: 971.6906070709229 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjgZlZomvpCq"
      },
      "source": [
        "### Visualizing training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "TiNHfdsjurXm",
        "outputId": "b7cb4ff2-0ef1-4e5f-bd08-2df8d4787afc"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "# plot some data\n",
        "ax1.plot(losses, label='loss')\n",
        "#plt.plot(results.history['val_loss'], label='val_loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "# accuracies\n",
        "ax2.plot(accuracies, label='acc')\n",
        "#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\n",
        "ax2.set_title('Training Accuracy')\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noRn0gicvwDt"
      },
      "source": [
        "### Model Inference\n",
        "\n",
        "We have seen the scores obtained after training but what we are interested in making predictions and see how the model works with new sentences. The predict function will input a tokenize sentence to the model and return the predicted new sentence, in our example, a translation from english to spanish.\n",
        "\n",
        "* Tokenize the input sentence to a sequence of tokens\n",
        "* Set the initial output sequence to the SOS token\n",
        "* Until we reach the max length or the eos token is returned by the model\n",
        "* Get the next word predicted. The model returns the logits, remember that the softmax function is applied in the loss calculation.\n",
        "* Get the index in the vocabulary of the word with the highest probability\n",
        "* Concat the next word predicted to the output sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q33I7OpCurT7"
      },
      "source": [
        "def predict(inp_sentence, tokenizer_in, tokenizer_out, target_max_len):\n",
        "    # Tokenize the input sequence using the tokenizer_in\n",
        "    inp_sentence = sos_token_input + tokenizer_in.encode(inp_sentence) + eos_token_input\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    # Set the initial output sentence to sos\n",
        "    out_sentence = sos_token_output\n",
        "    # Reshape the output\n",
        "    output = tf.expand_dims(out_sentence, axis=0)\n",
        "\n",
        "    # For max target len tokens\n",
        "    for _ in range(target_max_len):\n",
        "        # Call the transformer and get the logits \n",
        "        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
        "        # Extract the logists of the next word\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # The highest probability is taken\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        # Check if it is the eos token\n",
        "        if predicted_id == eos_token_output:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        # Concat the predicted word to the output sequence\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_W9Kge8wA96"
      },
      "source": [
        "And finally our last function receives a sentence in english, calls the transformer to translate it to spanish and shows the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlxug7y4urQl"
      },
      "source": [
        "def translate(sentence):\n",
        "  # Get the predicted sequence for the input sentence\n",
        "  output = predict(sentence, tokenizer_inputs, tokenizer_outputs, MAX_LENGTH).numpy()\n",
        "  # Transform the sequence of tokens to a sentence\n",
        "  predicted_sentence = tokenizer_outputs.decode(\n",
        "      [i for i in output if i < sos_token_output]\n",
        "  )\n",
        "  return predicted_sentence\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsyFsYGEurNi",
        "outputId": "118a0b55-b169-46c4-e9c5-400c774e4811"
      },
      "source": [
        "\n",
        "for e, f in zip(en[200: 210], fr[200: 210]):\n",
        "  print(\"Input sentence: {}\".format(f))\n",
        "  predicted_sentence = translate(f)\n",
        "  print(\"Predicted sentence: {}\".format(predicted_sentence))\n",
        "  print(\"Real sentence: {}\".format(e))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: appelle tom .\n",
            "Predicted sentence: call tom .\n",
            "Real sentence: call tom .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: appelez tom .\n",
            "Predicted sentence: call tom .\n",
            "Real sentence: call tom .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: courage !\n",
            "Predicted sentence: have a nerve !\n",
            "Real sentence: cheer up !\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: detends toi !\n",
            "Predicted sentence: loosen up .\n",
            "Real sentence: cool off !\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: menottez le .\n",
            "Predicted sentence: put it up .\n",
            "Real sentence: cuff him .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: avance !\n",
            "Predicted sentence: get early .\n",
            "Real sentence: drive on .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: avancez !\n",
            "Predicted sentence: drive on .\n",
            "Real sentence: drive on .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: continue a rouler !\n",
            "Predicted sentence: keep driving .\n",
            "Real sentence: drive on .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: continuez a rouler !\n",
            "Predicted sentence: keep driving .\n",
            "Real sentence: drive on .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: lache toi !\n",
            "Predicted sentence: get away  .\n",
            "Real sentence: get down !\n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU8fKSxbVUPZ",
        "outputId": "dbc7660f-1ebe-4fb2-abae-e87004dcb569"
      },
      "source": [
        "for e, f in zip(en[1000: 1010], fr[1000: 1010]):\n",
        "  print(\"Input sentence: {}\".format(f))\n",
        "  predicted_sentence = translate(f)\n",
        "  print(\"Predicted sentence: {}\".format(predicted_sentence))\n",
        "  print(\"Real sentence: {}\".format(e))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: je le suppose .\n",
            "Predicted sentence: i suppose so .\n",
            "Real sentence: i guess so .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j imagine .\n",
            "Predicted sentence: i m being lazy .\n",
            "Real sentence: i guess so .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j ai recu de l aide .\n",
            "Predicted sentence: i got help .\n",
            "Real sentence: i had help .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je te deteste .\n",
            "Predicted sentence: i hate you .\n",
            "Real sentence: i hate you .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je te hais .\n",
            "Predicted sentence: i hate you .\n",
            "Real sentence: i hate you .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j en ai un .\n",
            "Predicted sentence: i have one .\n",
            "Real sentence: i have one .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j en ai une .\n",
            "Predicted sentence: i have one .\n",
            "Real sentence: i have one .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j ai gagne .\n",
            "Predicted sentence: i won .\n",
            "Real sentence: i have won .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je l ai emporte .\n",
            "Predicted sentence: i won .\n",
            "Real sentence: i have won .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je l aide .\n",
            "Predicted sentence: i m asking him .\n",
            "Real sentence: i help him .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-rIGl5fVYxy",
        "outputId": "a13c23c4-4be1-48ad-d7ed-68d61bbf12c2"
      },
      "source": [
        "for e, f in zip(en[3000: 3010], fr[3000: 3010]):\n",
        "  print(\"Input sentence: {}\".format(f))\n",
        "  predicted_sentence = translate(f)\n",
        "  print(\"Predicted sentence: {}\".format(predicted_sentence))\n",
        "  print(\"Real sentence: {}\".format(e))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: il me faut partir .\n",
            "Predicted sentence: i need to go .\n",
            "Real sentence: i have to go .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: il me faut m en aller .\n",
            "Predicted sentence: i need to go .\n",
            "Real sentence: i have to go .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je dois partir .\n",
            "Predicted sentence: i must go .\n",
            "Real sentence: i have to go .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je dois m en aller .\n",
            "Predicted sentence: i must go .\n",
            "Real sentence: i have to go .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j entends de la musique .\n",
            "Predicted sentence: i m hearing music .\n",
            "Real sentence: i hear music .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j ai entendu ca .\n",
            "Predicted sentence: i heard that .\n",
            "Real sentence: i heard that .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j ai donne un coup de main .\n",
            "Predicted sentence: i gave a hand .\n",
            "Real sentence: i helped out .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j honore cela .\n",
            "Predicted sentence: i m dying about that .\n",
            "Real sentence: i honor that .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: je l etreignis .\n",
            "Predicted sentence: i hugged her .\n",
            "Real sentence: i hugged her .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: j ai improvise .\n",
            "Predicted sentence: i m coming away .\n",
            "Real sentence: i improvised .\n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA1P6pmsVhMR",
        "outputId": "f8eb21ec-c641-4980-ebe7-25c8db6c736c"
      },
      "source": [
        "for e, f in zip(en[4000: 4010], fr[4000: 4010]):\n",
        "  print(\"Input sentence: {}\".format(f))\n",
        "  predicted_sentence = translate(f)\n",
        "  print(\"Predicted sentence: {}\".format(predicted_sentence))\n",
        "  print(\"Real sentence: {}\".format(e))\n",
        "  print()\n",
        "  print(\"*\"*100)\n",
        "  print()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence: ai je du talent ?\n",
            "Predicted sentence: did i have talented ?\n",
            "Real sentence: am i talented ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: des questions ?\n",
            "Predicted sentence: are any questions ?\n",
            "Real sentence: any questions ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: de quelconques questions ?\n",
            "Predicted sentence: any questions ?\n",
            "Real sentence: any questions ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: autre chose ?\n",
            "Predicted sentence: is something else ?\n",
            "Real sentence: anything else ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: sont elles occupees ?\n",
            "Predicted sentence: are they busy ?\n",
            "Real sentence: are they busy ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: sont ils occupees ?\n",
            "Predicted sentence: are they busy ?\n",
            "Real sentence: are they busy ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: c est des flics ?\n",
            "Predicted sentence: is this cops ?\n",
            "Real sentence: are they cops ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: sont ils flics ?\n",
            "Predicted sentence: are they cops ?\n",
            "Real sentence: are they cops ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: sont ils des flics ?\n",
            "Predicted sentence: are they cops ?\n",
            "Real sentence: are they cops ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "Input sentence: etes vous flic ?\n",
            "Predicted sentence: are you a cop ?\n",
            "Real sentence: are you a cop ?\n",
            "\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQoXJbf3VqC6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}