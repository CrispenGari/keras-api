{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Text_Genereation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcsSRWh3bH3K"
      },
      "source": [
        "### Text Generation RNN\n",
        "This notebook is a follow up for text generation using a character based RNN. We will be working on the dataset of Shakespeare's writting from Andrej Karpathy.[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "### Model\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n",
        "\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FxU9lq-ebGZs",
        "outputId": "f28418fc-f1fd-45ff-9038-0a34a2c2b099"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os, time\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw0WHzKSdR-r"
      },
      "source": [
        "### Data.\n",
        "I've already downloaded the `shakespeare.txt` file and uploaded it on my google colab. Next I've to mount the drives and declare the paths.\n",
        "\n",
        "You can load the data directy from an online url by doing this:\n",
        "\n",
        "```py\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl_k3FsibGWF",
        "outputId": "fc088b40-6145-4b57-c524-b0d0ed0b1172"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-Ml3sHPbGTZ",
        "outputId": "6dc60e12-f779-4448-d595-0d7a3b512074"
      },
      "source": [
        "file_path = '/content/drive/My Drive/NLP Data/text-gen/shakespeare.txt'\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_L8q4x4exBC"
      },
      "source": [
        "### Reading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfCoQEjDbGQN",
        "outputId": "181e2004-520d-456f-b57a-e2130076cae0"
      },
      "source": [
        "text = open(file_path, 'rb').read().decode(encoding=\"utf-8\")\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i4M_jeEfAk7"
      },
      "source": [
        "### Checking some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc9mNPl7bGM6",
        "outputId": "27b45fcb-3ee8-4ea6-967f-cc49e5acbe7c"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JO8MsIsfGgM"
      },
      "source": [
        "### Character vocab size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAHjhbffbGKU",
        "outputId": "b759e92e-500c-43e8-e1d3-90e4cd3cfe83"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNS_38HtfQU6"
      },
      "source": [
        "### Text Processing\n",
        "\n",
        "1. Vectorizing the text.\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `preprocessing.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBBJcBXgbGHF",
        "outputId": "e5e52d40-83c4-46a4-8bac-fc86b064db70"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeAdhxgkfuvi"
      },
      "source": [
        "### Creating the ` preprocessing.StringLookup` layer.\n",
        "The `StringLookup` maps strings from a vocabulary to integer indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuOgj6nZbGEF"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_348pppgmm8"
      },
      "source": [
        "It converts form tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axMu1lg1bGBl",
        "outputId": "c64843bd-3430-4d5c-81e0-236210f65f7e"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxZVKooAgyr6"
      },
      "source": [
        "Since the goal is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `preprocessing.StringLookup(..., invert=True)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpXCPuNGbF-0"
      },
      "source": [
        "chars_from_ids = preprocessing.StringLookup(\n",
        "  vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None  \n",
        ")\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBXRYolqhQhy"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKHHj3T3hM7r",
        "outputId": "04d8e5ad-42fa-4738-8d82-5072a900b378"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDsEqQ6Chl5Z"
      },
      "source": [
        "### We can join the strings by using the `tf.strins.reduce_join` function.\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS59AMglhMxm",
        "outputId": "413f1f28-87dc-474a-8664-126360fad016"
      },
      "source": [
        "tf.strings.reduce_join(chars[0], axis=-1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'abcdefg'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GMjFu6Ph7Jj"
      },
      "source": [
        "So let's create a helper function that will join the characters by taking id's and returns us a word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLxTwzXBbF8K"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(\n",
        "      chars_from_ids(ids), axis=-1\n",
        "  ).numpy()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzBqcsKebF5L",
        "outputId": "68f181aa-9c4b-47ba-ec89-c8da7f4f01ef"
      },
      "source": [
        "text_from_ids(ids)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHmPyN3riYUj"
      },
      "source": [
        "### The prediction task.\n",
        "We are trying to train a model that will learn, given a character or a sequence of character what is the most probable next character to occur in the sequence.\n",
        "\n",
        "The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "### Creating training examples targets.\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvuJaZCHa7d5",
        "outputId": "9c00db1a-14cf-4f7d-8ce0-6735038f323d"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xM9eMb1hjBx",
        "outputId": "0b37e28a-a8c5-4a9a-8119-e70ed5085c7f"
      },
      "source": [
        "text_from_ids(all_ids[:10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'First Citi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jd5D1AtjtU1"
      },
      "source": [
        "### Create the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1O-kCNDhi_J"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    all_ids\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOKRVzVqj3k3"
      },
      "source": [
        "### Checking if it worsk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkE9nWxihi73",
        "outputId": "3393cef9-ff25-4172-f471-2d40b771ceaf"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx5D1e1ahi5C"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sipYRJLShizY"
      },
      "source": [
        "### Batching our dataset\n",
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58IExy9pkubH",
        "outputId": "e236b81b-20cb-449e-d060-c34c565de3e7"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbZRE6lckm-H"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsYGSx9Hhiwo",
        "outputId": "beed21fb-0d36-40c2-dc45-6e99c4f34d46"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DVvWxpLlPpc"
      },
      "source": [
        "For training you'll need a dataset of (`input`, `label`) pairs. Where input and `label` are sequences. At each time step the input is the current character and the `label` is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J2hc1FMhiuS"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RDbPmZfhiq6",
        "outputId": "ed190a08-3992-4c78-b9ea-cd843abf364b"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyxLBBWYmBYd"
      },
      "source": [
        "### Now we should create our new dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxSFtcBhiny"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_e8lXqfmIKk",
        "outputId": "2a65f92b-3c24-49ae-fb15-7a252cf0ca96"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example))\n",
        "  print(\"Target:\", text_from_ids(target_example))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIFhCOqHmSw_"
      },
      "source": [
        "### Creating batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oph4xHHmMKg"
      },
      "source": [
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG50wxFWmlrb",
        "outputId": "24e6c869-c0da-4b59-99bc-fbd8f5a94141"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(\n",
        "    BUFFER_SIZE\n",
        ").batch(BATCH_SIZE,\n",
        "        drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2GQNFnDm7NV"
      },
      "source": [
        "### Building the Model.\n",
        "\n",
        "We are going to use the `Keras Subclassing` to create our model this is because of flexebility. This model has three layers:\n",
        "1. The Embedding layer \n",
        "  * The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "\n",
        "2. The GRU\n",
        "  * A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)\n",
        "3. THe Dense \n",
        "  * The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7ZYB72Emz5o",
        "outputId": "cd1db20f-12ab-49e9-b4a1-69ad9b0030da"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "vocab_size"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQ7JMRsrFjT"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               embedding_dim,\n",
        "               rnn_units\n",
        "               ):\n",
        "    super(Model, self).__init__()\n",
        "    self.embedding = keras.layers.Embedding(\n",
        "        vocab_size, embedding_dim\n",
        "    )\n",
        "    self.gru = keras.layers.GRU(rnn_units,\n",
        "                                return_sequences=True,\n",
        "                                return_state=True\n",
        "                                )\n",
        "    self.dense = keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None,return_state=False, training=False):\n",
        "    x = self.embedding(inputs, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V53IyJFzsSDH"
      },
      "source": [
        "### Creating the model instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EHPNBDHsRbX"
      },
      "source": [
        "model = Model(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CueT7Qw6snK2"
      },
      "source": [
        "### Visualization\n",
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the `log-likelihood` of the next character:\n",
        "\n",
        "![img](https://www.tensorflow.org/text/tutorials/images/text_generation_training.png)\n",
        "\n",
        "### Trying the model\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8waN3GTsYT-",
        "outputId": "5c8e1eef-62d0-4e70-c949-7e2cacbdb841"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgVDKHBntoNG"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCe5AMxpsYtR",
        "outputId": "5a3edcd4-a35b-433b-8e30-ea40fe78a400"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqvDYSkAuCGa"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECNI_SeYuNE5"
      },
      "source": [
        "### Model training\n",
        "\n",
        "At this point the problem can be treated as a classification problem, predicting the next character.\n",
        "\n",
        "### Optimizer and Loss function\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "For the optimizer we are going to use the `Adam` with defaults params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iiXhL0ft3Rq"
      },
      "source": [
        "criterion = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7zl8H1wvIT9"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=criterion, metrics=[\"acc\"])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwABDpa1vVoC"
      },
      "source": [
        "### Configuring checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQVYN1yvUnu"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE9MUgy-vcM6",
        "outputId": "c16bfafa-e2a5-4e1b-d650-61b6a9925fcd"
      },
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, \n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 27s 139ms/step - loss: 2.7272 - acc: 0.2777\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.9897 - acc: 0.4179\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.7141 - acc: 0.4914\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.5519 - acc: 0.5344\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.4525 - acc: 0.5599\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.3843 - acc: 0.5765\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.3318 - acc: 0.5897\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.2875 - acc: 0.6008\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.2465 - acc: 0.6113\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.2067 - acc: 0.6222\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.1673 - acc: 0.6330\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 1.1263 - acc: 0.6443\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 1.0834 - acc: 0.6566\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.0374 - acc: 0.6706\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.9889 - acc: 0.6855\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.9378 - acc: 0.7019\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.8848 - acc: 0.7185\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.8313 - acc: 0.7358\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.7782 - acc: 0.7535\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.7291 - acc: 0.7696\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.6844 - acc: 0.7836\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.6437 - acc: 0.7964\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.6086 - acc: 0.8074\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.5770 - acc: 0.8174\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 25s 139ms/step - loss: 0.5518 - acc: 0.8247\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.5309 - acc: 0.8312\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.5134 - acc: 0.8358\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.4967 - acc: 0.8406\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.4852 - acc: 0.8435\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 0.4731 - acc: 0.8471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "353XzOguvuX2"
      },
      "source": [
        "### Visualizing the model training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Dc8rzZCRvt9G",
        "outputId": "18e92296-d26a-4b5b-e272-bd23be834fbe"
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(title=\"model training history\")\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEICAYAAACgQWTXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xUd73/8ddnO9t736X33jEFSCGQXjXVSLwmRo0a9aq5V38meuONN16vXSPGaGIKiaaSBkETAQ11wwK7QCDAsr33Pjvf3x/nLAyEZXfZ2Z32eT4e5zFnZs6c+RwmefPle77ne8QYg1JKKf8Q5OkClFJKuY+GulJK+RENdaWU8iMa6kop5Uc01JVSyo9oqCullB/RUFcjRkT+JCIPD3DbYyJy6TDWcruIbHD3tudQR5/HKSIXisjB4fhe5b801JXPGcxfDn0xxjxjjLnM3du6kzFmszFmcn/bichDIvL0SNSkvJ+GuvI7IhLi6Rp8if55+RcNdXUKuzvgmyKyR0RaReQPIpImIm+JSLOIbBSRBJftrxGRAhFpEJH3RGSqy3tzRSTP/tzzQMRp33WViOy2P/svEZk1gPruAW4HviUiLSKyzqXub4vIHqBVREJE5AER+cj+/kIRud5lP6tFZIvLcyMi94rIIbueX4uInMO2wSLyExGpEZGjInKfvf3ZgnOO/efdKCLPi0iEva/lIlLi8r3fFpFS+3gOisglIrIK+E/gZvvPI9/eNlNEXhOROhE5LCJ3u+znIRH5q4g8LSJNwAMi0iYiSS7bzBORahEJ7e83UV7GGKOLLicW4BiwFUgDsoAqIA+YixXKfwcetLedBLQCK4BQ4FvAYSDMXoqAr9nv3QR0Aw/bn51r73sxEAx8xv7ucJc6Lu2jxj/17ue0uncDOcAo+7VPAplYjZeb7Voz7PdWA1tcPm+A14F4IBeoBladw7b3AoVANpAAbLS3DznLn/d2u85EYD9wr/3ecqDEXp8MFAOZ9vMxwHh7/SHg6dP2uwn4jf2bzbFrvNhl+27gOvvPZhTwJvAFl8//FPilp/971GXwi7bU1Zn80hhTaYwpBTYD24wxHxhjOoCXsQIZrKB8wxjzjjGmG/hfrIA4D1iCFeY/M8Z0G2P+Cuxw+Y57gN8ZY7YZY3qMMU8CnfbnztUvjDHFxph2AGPMX4wxZcYYpzHmeeAQsOgsn/+RMabBGHMceBcrDAe77aeAnxtjSowx9cCPBlh3mTGmDljXx/f2AOHANBEJNcYcM8Z8dKadiUgOcD7wbWNMhzFmN/A4cKfLZu8bY16x/2zagSeBO+zPBwO3An8eQO3Ky2ioqzOpdFlvP8PzaHs9E6s1DoAxxonVmsyy3ys1xrjOGFfksj4a+IbdfdEgIg1YrezMIdRd7PpERO506d5pAGYAyWf5fIXLehsnj3Mw22aeVscpNZ3r9xpjDgP3Y7Wyq0RkrYj09WeVCdQZY5pdXivC+l36qutVrL8wxmL9y6vRGLN9ALUrL6OhroaiDCucAbD7lXOAUqAcyOrta7bluqwXAz80xsS7LJHGmOcG8L19TS164nURGQ38HrgPSDLGxAP7AOnjs+5SjtX10ivHXTs2xjxrjLkA68/cAP/T+9Zpm5YBiSIS4/JaLtbvcmJ3p+27A3gBq7X+abSV7rM01NVQvABcaZ+wCwW+gdWF8i/gfcABfEVEQkXkBk7t+vg9cK+ILBZLlIhceVoQ9aUSGNfPNlFYwVUNICJ3YbXUh9sLwFdFJEtE4oFvu2OnIjJZRC4WkXCgA+tfTE777UpgjIgEARhjirF+g0dEJMI+Af1vQH/DHp/COn9wDRrqPktDXZ0zY8xBrJbdL4Ea4GrgamNMlzGmC7gBKyTqsPrfX3L57E7gbuBXQD3WCdbVA/zqP2B1FTSIyCt91FYI/ATrL5dKYCbwz8Ed4Tn5PbAB2AN8gHUC0oHVJz4U4Vj98zVY3TWpwH/Y7/3FfqwVkTx7/Vask6llWOdBHjTGbDzbFxhj/on1F0WeMabobNsq7yWndnkqpdxJRC4HHjPGjO53Yy8gIn8HnjXGPO7pWtS50Za6Um4kIqNE5Ap7nHwW8CBWS9nrichCYB7wvKdrUedOQ10p9xLg+1hdSh9gjTv/nkcrGgAReRJrTP39p42aUT5Gu1+UUsqPaEtdKaX8iMcm8klOTjZjxozx1NcrpZRP2rVrV40xJqWv9z0W6mPGjGHnzp2e+nqllPJJInLW4aba/aKUUn5EQ10ppfyIhrpSSvkRveOJUsqndXd3U1JSQkdHh6dLcauIiAiys7MJDR3cfUo01JVSPq2kpISYmBjGjBnDqZOC+i5jDLW1tZSUlDB27NhBfVa7X5RSPq2jo4OkpCS/CXQAESEpKemc/vWhoa6U8nn+FOi9zvWYfC7UD1Y088hb+2nu6PZ0KUop5XV8LtSL69r43T+O8GGlzjmklPIO0dFnu/PhyPK5UJ+cbt0Y50CFhrpSSp3O50I9O2EU0eEhHNRQV0p5GWMM3/zmN5kxYwYzZ87k+eetqenLy8tZunQpc+bMYcaMGWzevJmenh5Wr159Ytuf/vSnbqnB54Y0igiT0qK1pa6U+pjvryugsKzJrfuclhnLg1dPH9C2L730Ert37yY/P5+amhoWLlzI0qVLefbZZ1m5ciXf+c536Onpoa2tjd27d1NaWsq+ffsAaGhocEu9PtdSB5iSEcvBimZ0LnillDfZsmULt956K8HBwaSlpbFs2TJ27NjBwoUL+eMf/8hDDz3E3r17iYmJYdy4cRw5coQvf/nLvP3228TGxrqlBp9rqQNMSY/h2W3HqWzqJD0uwtPlKKW8xEBb1CNt6dKlbNq0iTfeeIPVq1fz9a9/nTvvvJP8/HzWr1/PY489xgsvvMATTzwx5O/yyZb65DTrZOn+Cvf+M0sppYbiwgsv5Pnnn6enp4fq6mo2bdrEokWLKCoqIi0tjbvvvpvPfe5z5OXlUVNTg9Pp5MYbb+Thhx8mLy/PLTX4aEvd+mfKwYpmLpqc6uFqlFLKcv311/P+++8ze/ZsRIRHH32U9PR0nnzySX784x8TGhpKdHQ0Tz31FKWlpdx11104nU4AHnnkEbfU4LF7lC5YsMAM5SYZS/77b3xifBI/vXmOG6tSSvma/fv3M3XqVE+XMSzOdGwisssYs6Cvz/hk9wtY49V1BIxSSp3KZ0N9SnoMH1W10N3j9HQpSinlNXw31DNi6Opxcqym1dOlKKWU1/DZUJ+cZp0s1S4YpZQ6yWdDfXxqFMFBotMFKKWUC58N9fCQYMYlR3FAx6orpdQJ/Ya6iOSIyLsiUigiBSLy1TNss1xEGkVkt718b3jKPZWOgFFKqVMN5OIjB/ANY0yeiMQAu0TkHWNM4WnbbTbGXOX+Evs2JT2G1/eU09LpIDrcJ6+jUkopt+q3pW6MKTfG5NnrzcB+IGu4CxsI1ytLlVLKU6677jrmz5/P9OnTWbNmDQBvv/028+bNY/bs2VxyySUAtLS0cNdddzFz5kxmzZrFiy++6PZaBtW8FZExwFxg2xne/oSI5ANlwL8bYwqGXF0/em+YcbCimfmjE4b765RS3u6tB6Bir3v3mT4TLv/RWTd54oknSExMpL29nYULF3Lttddy9913s2nTJsaOHUtdXR0A//Vf/0VcXBx791o11tfXu7dWBhHqIhINvAjcb4w5/exkHjDaGNMiIlcArwATz7CPe4B7AHJzc8+56F4nb5ihJ0uVUp7zi1/8gpdffhmA4uJi1qxZw9KlSxk7diwAiYmJAGzcuJG1a9ee+FxCgvsbowMKdREJxQr0Z4wxL53+vmvIG2PeFJHfiEiyMabmtO3WAGvAmvtlSJWjN8xQSp2mnxb1cHjvvffYuHEj77//PpGRkSxfvpw5c+Zw4MCBEa8FBjb6RYA/APuNMf/Xxzbp9naIyCJ7v7XuLLQvk9NjOaA3zFBKeUhjYyMJCQlERkZy4MABtm7dSkdHB5s2beLo0aMAJ7pfVqxYwa9//esTnx2O7peBjFM/H/g0cLHLkMUrROReEbnX3uYmYJ/dp/4L4BYzQik7JT2GxvZuKps6R+LrlFLqFKtWrcLhcDB16lQeeOABlixZQkpKCmvWrOGGG25g9uzZ3HzzzQB897vfpb6+nhkzZjB79mzeffddt9fTb/eLMWYLIP1s8yvgV+4qajB6T5YeqGjSuyAppUZceHg4b7311hnfu/zyy095Hh0dzZNPPjms9fjsFaW9priMgFFKqUDn86EeHxlGemyEhrpSSuEHoQ46XYBSgc4fB0qc6zH5RahPSY/hcFULDr1hhlIBJyIigtraWr8KdmMMtbW1REQM/jyhX0yYMjndumHG0ZpWJqbFeLocpdQIys7OpqSkhOrqak+X4lYRERFkZ2cP+nN+E+pg3TBDQ12pwBIaGnriyk3lJ90vE1Kj9YYZSimFn4T6yRtmaKgrpQKbX4Q6WF0wByt1Yi+lVGDzm1Cfkh5DcV07LZ0OT5eilFIe4zehPtm+YcaHldoFo5QKXH4T6jpdgFJK+VGoZ8WPIiosmAPl2q+ulApcfhPqQUHCJJ0uQCkV4Pwm1MG6EfXBSr1hhlIqcPlZqMfQ0NZNVbPeMEMpFZj8KtRdpwtQSqlA5FehfnIEjJ4sVUoFJr8K9fjIMNJiw7WlrpQKWH4V6mBdhHSgXENdKRWY/C7Up6THcLhab5ihlApMfhnqXQ4nx2pbPV2KUkqNOL8LdR0Bo5QKZH4X6nrDDKVUIPO7UA8PCWas3jBDKRWg/C7Uwb5hhoa6UioA+WWoT0mL4XhdG616wwylVIDxy1DvPVl6UG+YoZQKMH4Z6lMzrLsgaReMUirQ+GWo994wQ0NdKRVo/DLUT94wQyf2UkoFFr8MdbCuLD1YoTfMUEoFln5DXURyRORdESkUkQIR+eoZthER+YWIHBaRPSIyb3jKHbjJaTHUt3VTrTfMUEoFkIG01B3AN4wx04AlwJdEZNpp21wOTLSXe4DfurXKczA53TpZqhchKaUCSb+hbowpN8bk2evNwH4g67TNrgWeMpatQLyIZLi92kGYcmIOGO1XV0oFjkH1qYvIGGAusO20t7KAYpfnJXw8+BGRe0Rkp4jsrK6uHlylg5QQZd0wI7+4cVi/RymlvMmAQ11EooEXgfuNMefU/DXGrDHGLDDGLEhJSTmXXQzK5TMyWF9QQVlD+7B/l1JKeYMBhbqIhGIF+jPGmJfOsEkpkOPyPNt+zaPuXjoOgN9vPuLhSpRSamQMZPSLAH8A9htj/q+PzV4D7rRHwSwBGo0x5W6s85xkxY/i2jlZrN1eTF1rl6fLUUqpYTeQlvr5wKeBi0Vkt71cISL3isi99jZvAkeAw8DvgS8OT7mDd++ycbR39/Cnfx71dClKKTXsQvrbwBizBZB+tjHAl9xVlDtNTIvhsmlp/Olfx7hn2Xiiw/s9ZKWU8ll+e0Wpqy9eNIGmDgfPbivydClKKTWsAiLU5+TEc974JB7ffJROR4+ny1FKqWETEKEO8MXlE6hq7uSlPI8PylFKqWETMKF+/oQkZmXH8bt/fESPUyf5Ukr5p4AJdRHhC8vGc6y2jTf3eny0pVJKDYuACXWAldPTGZcSxW/e+0in5FVK+aWACvWgIOHeZePZX97EPz4c3rlnlFLKEwIq1AGum5NFRlwEv3nvI0+XopRSbhdwoR4WEsTdF45j+9E6dhXVebocpZRyq4ALdYBbFuWQEBnKb97V1rpSyr8EZKhHhoWw+ryx/O1Ald5EQynlVwIy1AE+c95oosKC+a32rSul/EjAhnp8ZBi3Lc5lXX4Zx2vbPF2OUkq5RcCGOsDnLhxHSFAQazZra10p5R8COtTTYiO4YV4WL+wsoaq5w9PlKKXUkAV0qAN8ftl4HD1OnthyzNOlKKXUkAV8qI9NjuLymRk8vbWI6uZOT5ejlFJDEvChDnD/JRNxOJ186dk8unucni5HKaXOmYY61i3v/ufGWWw/WscP39jv6XKUUuqc6Q07bdfOyWJvSSOPbznK9MxYPrkgx9MlKaXUoGlL3cUDl0/h/AlJfOeVfeQXN3i6HKWUGjQNdRchwUH88tZ5pESH8/k/79ITp0opn6OhfprEqDDW3DmfhvYuvvRMHl0OPXGqlPIdGupnMD0zjkdvms32Y3U8/Eahp8tRSqkB0xOlfbhmdib7ShtZs+kIMzLj+NRCPXGqlPJ+2lI/i2+tnMwFE5L57iv7+OB4vafLUUqpfmmon4V14nQuaXHh3Pv0Lp0fRinl9TTU+5EQFcbv7lhAU7uDLz6tJ06VUt5NQ30ApmXG8uNPzmJnUT0/eL3A0+UopVSf9ETpAF01K5O9pY387h9HGJMUxb9dMBYR8XRZSil1Cm2pD8K3Vk5hxbQ0Hn5jP994IZ+2LoenS1JKqVP0G+oi8oSIVInIvj7eXy4ijSKy216+5/4yvUNwkPDYHfO5/9KJvLy7lOt+/U8OV7V4uiyllDphIC31PwGr+tlmszFmjr38YOhlea/gIOH+Syfx1GcXUdPSxbW/2sJr+WWeLksppYABhLoxZhNQNwK1+JQLJ6bwxlcuYEpGLF957gMefHUfnY4eT5ellApw7upT/4SI5IvIWyIyva+NROQeEdkpIjurq6vd9NWekxE3irX3LOHuC8fy5PtFfOqx9ympb/N0WUqpAOaOUM8DRhtjZgO/BF7pa0NjzBpjzAJjzIKUlBQ3fLXnhQYH8Z0rp/HYHfM4Ut3Klb/Ywt8PVHq6LKVUgBpyqBtjmowxLfb6m0CoiCQPuTIfs2pGBuu+fAFZ8aP47J928ujbB3DorfGUUiNsyKEuIuliD9gWkUX2PmuHul9fNCY5ipe+eB63LMzhN+99xO2Pb+NgRbOny1JKBZB+Lz4SkeeA5UCyiJQADwKhAMaYx4CbgC+IiANoB24xxphhq9jLRYQG86MbZ7FgTCLff62AVT/fxHVzsvjapZPITYr0dHlKKT8nnsrfBQsWmJ07d3rku0dKfWsXj236iD/98xg9TsMti3L48sUTSYuN8HRpSikfJSK7jDEL+nxfQ334VTZ18Mu/H2Lt9mKCg4TV543h3mXjSYgK83RpSikfo6HuRY7XtvGzjR/y8u5SosNCuHvpOD57wViiw3UKHqXUwGioe6EPK5v5yYaDrC+oJDEqjC8uH88dS0YTERrs6dKUUl5OQ92L7S5u4CcbDrL5UA0JkaHcOC+b2xbnMi4l2tOlKaW8lIa6D9h6pJan3j/GhoJKHE7DJ8YlcfuSXC6blk5YiE6kqZQ6qb9Q185cL7BkXBJLxiVR1dzBX3aW8Nz249z37AckR4dx0/wcbluUq8MhlVIDoi11L+R0GjYdqubZbcf524EqepyGCycmc/viXC6ZmkZosLbelQpU2v3i4yoaO3h+RzFrdxynvLGDlJhwrpyZwdWzM5mXG693X1IqwGio+wlHj5P3Dlbzws5i3vuwmi6Hk6z4UVw5K4OrZmUwMytOA16pAKCh7oeaO7p5p7CS1/eUs/lQNd09htFJkVw1K4OrZmUyJT1GA14pP6Wh7uca27pZX1DBuj1l/OujWnqchvEpUVw1K5MrZ2UwMTVaA14pP6KhHkBqWzp5u6CCdfllbDtahzEwNjmKy6alcdn0dObmxBMUpAGvlC/TUA9QVU0dbCisZH1BBe9/VIvDaUiNCWfFtDRWTk9nybgkHQOvlA/SUFc0tnfz3sEq1hdU8N7Batq6eoiJCOHiKamsnJ7OskkpROn8M0r5BA11dYqO7h62HKphfUEFG/dXUt/WTVhIEBdMSObSqWlcOjWVVJ0aWCmvpaGu+uTocbLjWD0bCit4p7CSkvp2AGbnxLNiaiorpqUzKU1PtCrlTTTU1YAYY/iwsoV3Cit4Z38V+cUNAOQkjuLSqWmsmJbGwjGJejWrUh6moa7OSVVTB387UMU7hZVsOVxDl8NJbEQIF01J5ZKpaSyblELcqFBPl6lUwNFQV0PW1uVg86Ea3ims5N0DVdS2dhESJCwam8glU9NYMTVNJxxTaoRoqCu36nEadhfXs3F/FRsLKzlU1QLAxNRoLp1mnWidk5NAsI6HV2pYaKirYVVU28rG/VX8bX8l24/W4XAakqLCuGhKKpdNS2PppBS9o5NSbqShrkZMY3s3//iwmo2Flbx7sIrmDgejQoNZNimFlTPSuHhyGnGR2g+v1FBoqCuP6HI42Xa0lvUFFWwoqKSquZOQIGHJuCRWTk9jxbR00uN0PLxSg6WhrjzO6TTklzSwvqCSDQUVHKlpBazx8Cunp7Fqerrel1WpAdJQV17ncFXziYDPL2kEYEp6DFfMzOCKmRlMSNWAV6ovGurKq5U1tPP2vgre3FvOzqJ6ACalRZ8I+ElpMR6uUCnvoqGufEZFYwdv7yvnzb0V7Ciypg6ekBrNFTPSuWJWBpPT9OYfSmmoK59U1dTB2wVWC3770TqcBsalRHHN7EyunZPF2OQoT5eolEdoqCufV91s3fzj9fwyth+zWvCzs+O4dk4WV83OIDVGR9GowKGhrvxKWUM7r+8p45UPyigsbyJI4PwJyVwzO5NVM9KJidBx8Mq/aagrv3WosplXd5fxan4pxXXthIcEccnUVK6dk8XyySmEh+iVrMr/DDnUReQJ4Cqgyhgz4wzvC/Bz4AqgDVhtjMnrrzANdeUuxhjyjjfw2u5SXt9TTm1rF/GRoVwzO5Mb52UzKztOT7Aqv+GOUF8KtABP9RHqVwBfxgr1xcDPjTGL+ytMQ10NB0ePk82Ha3gpr5QNBRV0OpxMTI3mxvnZXD83izS9q5PycW7pfhGRMcDrfYT674D3jDHP2c8PAsuNMeVn26eGuhpuje3dvLGnnBfzSthVVE+QwNJJKdw4L5sV09J0ojHlk/oLdXfcbTgLKHZ5XmK/dtZQV2q4xY0K5bbFudy2OJcj1S28lFfKS3klfPm5D4iNCOGq2ZncND+buTnx2j2j/MaI3kJeRO4B7gHIzc0dya9WAW5cSjT/vnIyX18xifeP1PLirhJeyivh2W3HmZQWzc0Lc7lhbhYJUWGeLlWpIdHuFxWwWjodvJ5fxtodxewubiAsOIiVM9K5ZWEOnxiXRJDe6EN5oZHofnkNuE9E1mKdKG3sL9CV8gbR4SHcsiiXWxblcqCiibXbi3n5g1LW5ZeRmxjJzQtz+OT8bFL15KryIQMZ/fIcsBxIBiqBB4FQAGPMY/aQxl8Bq7CGNN5ljOm3Ca4tdeWNOrp7WF9QwXPbj7P1SB3BQcJFk1O5dVEOyyalEBIc5OkSVYDTi4+UOkdHa1p5fkcxf91VQk1LJxlxEdyyMJdbFuXo0EjlMRrqSg1Rd4+Tv+2v4pltRWw+VENwkLBiahq3L8nl/PHJ2veuRtRI9Kkr5ddCg4NYNSOdVTPSOVbTynPbj/PCzmLeLqhgTFIkty3O5ZPzc3TkjPIK2lJX6hx0dPfw9r4KntlWxI5j9YSFBHHlzAzuWJLLvNwEHfeuho12vyg1zA5WNPPMtiJeyiulpdPBlPQYbl8ymuvnZhEdrv8YVu6loa7UCGntdLAuv4w/by2ioKyJqLBgrp+XxR1LRjMlPdbT5Sk/oaGu1AgzxrC7uIGntx7n9T1ldDqcLBidwB1LRnP5zHSdElgNiYa6Uh7U0NbFX3eV8My24xytaSUxKoxPLsjm9kWjyU2K9HR5CsAY6OmC7jbo7rAeHR3Q3W4tPZ3Q47C26ekCp8t67+vObnB0WdueeLSXU17rsvY9+xZYdPc5laujX5TyoPjIMD534Tg+e/5Y/vlRDU9vLeLxzUdZs+kIyyalcMfi0Vw0JZVgHRbZN2OsIOxsga5m+7Hl48+7Wq3F0QmOdiug+3x0Ce3uNsBNjdvgMAgOh5C+HsMhIhZChu86B22pKzXCKho7eG77cdbuOE5lUyeZcRHcuiiXmxfl+Of9Vo2BziZoq4W2upOPHY3Q2Wy919lkr7ssHS6vm56BfVdwGISMgtAIKzhDIuz1UWd+DI2E0FH2dpEnX3N9HhwOwaF2YIe6rIdBUMip60HDf8Wxdr8o5aWsi5oqeXrrcbYcriEkSLhsehp3LB7NJ8Ynee+wyB4HtNdBaw20VttLDbTV2I+u4V1rbet09L2/kAgIj4HwWPsxBiLiTq6Hx0BYtMtj9Jmfh0VDsP93PmioK+UDjta08uy2Iv6yq4SGtm7GJUdx2+JcbpqfTXzkCFzUZIwVxC0V0FwBLZUnH1sqocUO77Yaa7szdVdIEIxKhKhk6zEyESKT7MV1PQlGJVhLWLTVNaEGTENdKR/S0d3Dm3vLeXprEXnHGwgPCeKqWZncviT33G/m0dUGzeXQVApNZSeX5vJTA9zZ/fHPhsVATBpEp1lhHJViL8nWEpl88rVR8RCkI3uGm4a6Uj6qsKyJZ7YV8coHpbR29TAxNZqb5mdz/bysk33vPQ5oLoOG49BQbD02lUBTuR3epdDR8PGdR8RBTCbEpFtLdNqZH8OiRvagVb801JXyZc4eWmuOs3XXLvbv34ejrogcqWF6ZAO5wbWMaq9ATj+JGJ0GMRkQmwWxGRCbaa27vqZh7bN0SKNS3q67HeqPQd1RqD966mPDcaKc3VwCXAKYEKE5LIUjXUmsd4yhJngxabmTmDljJmPHT4W4bGvYnApYGupKDTdjoKXKCu5TFju4WypO3T48FhLGQPoMmHo1JI61nsfnIrHZxIaEMdNpaDhUzcZdJbxTUEnXh06mZpRx0/wgrp2TSXK0Bnug0u4XpdyhswUaiqw+7fqijwe4o91lY7G6QRJGQ4Id2IljrfXEsdaokEGcEG1o62Jdfhl/2VXCnpJGggTOn5DM1bMyWTk9nbjIULceqvIs7VNXyh262+0TkUXWUl90aoi31526fVi0HdijrdBOGHMywONzhq2L5GBFM+vyy1i3p4yi2jZCg4WlE1O4enYml05L01kj/YCGulID4eiExhKrVd1w3F6KTq63VJ66fXAYxOdC/GjrMWG0vT7aWo9MGlRr292MMewtbWRdfhmv7ymnvLGD8JAgLpmaytWzMrloSioRoTr80BdpqCvVe2FNY7E1xK+x1Br211hirSbyNRUAAAzCSURBVDcUWWO2XQWFWCcde0O7N8B7wzs6bUQuCXcHp9Ow63g96/LLeHNvOTUtXUSFBbNiWhqrZqSzdFIKkWHagvcVGurKv/UGdlOpywU29mOjHdxNZaf1aWO1tGOzzhDcdqs7JsMvL6Rx9DjZeqSOdfllrC+soKGtm/CQIC6cmMxl09K5ZGoqSXqS1atpqCvfZIw1815L1amXrLteDdkb4D2dp31YrFCOyzoZ3HHZp65HJvtMS3u4OHqcbD9Wx4aCSt4prKS0oZ0ggQVjErlsWhorp6eTk6jTA3sbDXXlXbpaT04A1VJlrfcGd0ulvV5hPXa3ffzzweH2BTW9F9Nknlxi7MfotICY2MmdjDEUlDWxobCSDQUVHKhoBmBKegyXTU/n4impzMyK0ymCvYCGuhpeji6X2flqoLX25MRPrdUnJ4LqXc4U1GBdth6dDtGpLpeq2+vRqfZ7adbEUN46e6EfOV7bxobCCjYUVrLzWB1OA3GjQjlvfBIXTEzmwgkpepMPD9FQV4PjGtKt1dbUqSfWTw/tWuhsPPN+JNie9Cnl1CU65bRJoVIgKtWat1p5pbrWLrYcrmHLoWq2HKqhrLEDgNzESDvgkzlvfLKOhx8hGuqBzBirZezaim6rPW3e69pT3z9bSJ+Ypc9+jOydqS/pZEj3vhYRH/B91v7IGMORmla2HKph86Eath6ppaXTQZDAzOx4LpyQzOJxiczLTSBKx8QPCw11f3OiJe1yc4KzrTs6zryfoFA7jJOtLg3XkHYN56gUazsNaXUG3T1O8osb2Hyohs2HqskvaaTHaQgOEmZkxbF4bCKLxiSycEyituTdREPdV/R026M8Ku15ru25rpsrXNbLP37lYq/gMKsb40SXhksr2rVF3Rvk4bHaN63crqXTQV5RPduP1rH9aB27ixvo6nEiAlPSY62QH2uFfEqMDp08Fxrq3qKn2xozfeIS8+OnrrdU8rG7yUjwyZOGMRkn576OOkO/dHiMhrTyOh3dPewubjgR8ruK6mnvtqYKzkkcxfSMOGZkxTI9M47pWbH+eY9WN9Opd0daRyNU7IXyfKgstC87L7LGVBvnye0k2BpHHT8aJlxqjZ0+JbwzrMD2wwtgVOCICA1mybgkloxLAqzumn2ljWw/Wsee0kYKSht5u+DkLJWpMeFMz4xlRlYc0zOtsM9OGOW992v1QhrqQ9FWB+W7rQDvXeqOnHw/Os2axGn0eR+fIyQ2S8dSq4ATGhzE3NwE5uYmnHituaObwrImCsqa2FfWSEFpE5sO1dDjtP7lGhsRwoTUaCakRjMxNebEelb8KIJ03PzHaPfLYNQdhf3roHgblO+BxuMn34vPhYzZ9jIXMmZZ46uVUoPW0d3DgYpmCsoaKSxr4nBVCx9Vt1DT0nVim4jQIManWAE/wX4cmxJFTkKkX4+8cUv3i4isAn4OBAOPG2N+dNr7q4EfA6X2S78yxjx+ThV7m9qPoPBVKHzFaokDJI6HnIWw6HNWiKfPskaQKKXcIiI0mDk58czJiT/l9Ya2Lg5XtXCoqoXD9rLzWD2v7i47Zbvk6DByEyNPLklRJ9ZTY8L9uoXfb0tdRIKBD4EVQAmwA7jVGFPoss1qYIEx5r6BfrFXt9RrDlshXviK1T8OkDUfpl0H066x5sRWSnmN1k4HR6pbKaprpai2jeK6No7bS1lDO06XmAsPCSI7YRQ5iZHkJESSkzjKfrSee/vQS3e01BcBh40xR+wdrgWuBQrP+ilfU/3hyRZ55T7rteyFcNkPrSCPz/VsfUqpPkWFhzAzO46Z2XEfe6/L4aSsof1EyB+va+N4bRvF9W3kFdXT1OE4ZfuYiBCyEyLJsYM/LTac5GjXJYzEqDBCgr3zuo2BhHoWUOzyvARYfIbtbhSRpVit+q8ZY4pP30BE7gHuAcjN9ZKQrP0I3vkeHHjdep6zGFY+YgV5XLZna1NKDVlYSBBjkqMYkxx1xvcb27sprmujpL6dknqrlV9c387RmlY2Haqmo9v5sc+IQEJkGElRYVbQx1hhnxITTor9PCU6nNSY8BH/C8BdZxPWAc8ZYzpF5PPAk8DFp29kjFkDrAGr+8VN331u2uvhHz+G7WusC3eW/yfM+7Q1y59SKmDEjQolLiuOGVkfb+UbY2judFDb0kVNSyc1zZ3UtHZZjy2dJ17fW9JATUsXLZ2Oj+1DhBPh3xv6l01PY9WMjGE5noGEeimQ4/I8m5MnRAEwxtS6PH0ceHTopQ2Tnm7Y+Ud47xEr2OfeARf/P4hJ83RlSikvIyLERoQSGxHK2D5a+q7au3qoaemkqrmT6uZOqu2/CKpb7OfNnRypbmV8avSw1TyQUN8BTBSRsVhhfgtwm+sGIpJhjOm9H9g1wH63VukOxsChDbDhu1DzIYy5EFb+tzX0UCml3GBUWLB1wtWDNxfpN9SNMQ4RuQ9YjzWk8QljTIGI/ADYaYx5DfiKiFwDOIA6YPUw1jx4lQWw/jtw5F1rOOItz8Hky/WyeqWU3/Hvi49aquHdhyHvKWtulGUPwMLPQUjY8H6vUkoNk8Cd+6W+CJ5YBa1VsOgeWPZtvUBIKeX3/DPUW6rgz9dZN4i4++/WVZ9KKRUA/C/U2xvgzzdY84/f+aoGulIqoPhXqHe1wbM3Q/UBuO15yFnk6YqUUmpE+U+oO7rghTutGRQ/+UeYcImnK1JKqRHnH6HudMIrX4DD78DVP4fp13u6IqWU8gjvnJFmMIyBt74J+/4Klz4E81d7uCCllPIc3w/1d/8bdjwO538VLviap6tRSimP8u1Qf/83sOlRmHcnXPp9T1ejlFIe57uhvvtZWP8fMO1auOpnesm/Ukrhq6F+4A149T4YdxHc8HsICvZ0RUop5RV8L9SPboa/3AWZc+HmpyEk3NMVKaWU1/C9UI9KhjHnw+1/gfDhm5NYKaV8ke+NU0+dCp9+2dNVKKWUV/K9lrpSSqk+aagrpZQf0VBXSik/oqGulFJ+RENdKaX8iIa6Ukr5EQ11pZTyIxrqSinlR8QY45kvFqkGis7x48lAjRvL8Qb+dkz+djzgf8fkb8cD/ndMZzqe0caYlL4+4LFQHwoR2WmMWeDpOtzJ347J344H/O+Y/O14wP+O6VyOR7tflFLKj2ioK6WUH/HVUF/j6QKGgb8dk78dD/jfMfnb8YD/HdOgj8cn+9SVUkqdma+21JVSSp2BhrpSSvkRnwt1EVklIgdF5LCIPODpetxBRI6JyF4R2S0iOz1dz2CJyBMiUiUi+1xeSxSRd0TkkP2Y4MkaB6uPY3pIRErt32m3iFzhyRoHQ0RyRORdESkUkQIR+ar9uk/+Tmc5Hl/+jSJEZLuI5NvH9H379bEiss3OvOdFJOys+/GlPnURCQY+BFYAJcAO4FZjTKFHCxsiETkGLDDG+ORFEyKyFGgBnjLGzLBfexSoM8b8yP7LN8EY821P1jkYfRzTQ0CLMeZ/PVnbuRCRDCDDGJMnIjHALuA6YDU++Dud5Xg+he/+RgJEGWNaRCQU2AJ8Ffg68JIxZq2IPAbkG2N+29d+fK2lvgg4bIw5YozpAtYC13q4poBnjNkE1J328rXAk/b6k1j/w/mMPo7JZxljyo0xefZ6M7AfyMJHf6ezHI/PMpYW+2movRjgYuCv9uv9/ka+FupZQLHL8xJ8/Ie0GWCDiOwSkXs8XYybpBljyu31CiDNk8W40X0issfunvGJrorTicgYYC6wDT/4nU47HvDh30hEgkVkN1AFvAN8BDQYYxz2Jv1mnq+Fur+6wBgzD7gc+JL9T3+/Yaw+Pt/p5+vbb4HxwBygHPiJZ8sZPBGJBl4E7jfGNLm+54u/0xmOx6d/I2NMjzFmDpCN1TMxZbD78LVQLwVyXJ5n26/5NGNMqf1YBbyM9WP6ukq737O3/7PKw/UMmTGm0v6fzgn8Hh/7nex+2heBZ4wxL9kv++zvdKbj8fXfqJcxpgF4F/gEEC8iIfZb/Waer4X6DmCifTY4DLgFeM3DNQ2JiETZJ3oQkSjgMmDf2T/lE14DPmOvfwZ41YO1uEVv+Nmux4d+J/sk3B+A/caY/3N5yyd/p76Ox8d/oxQRibfXR2ENCNmPFe432Zv1+xv51OgXAHuI0s+AYOAJY8wPPVzSkIjIOKzWOUAI8KyvHZOIPAcsx5omtBJ4EHgFeAHIxZpi+VPGGJ858djHMS3H+me9AY4Bn3fpj/ZqInIBsBnYCzjtl/8Tqx/a536nsxzPrfjubzQL60RoMFaD+wVjzA/sjFgLJAIfAHcYYzr73I+vhbpSSqm++Vr3i1JKqbPQUFdKKT+ioa6UUn5EQ10ppfyIhrpSSvkRDXWllPIjGupKKeVH/j8OICCwK8BxOQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuIOmTbAwIga"
      },
      "source": [
        "### Generating text\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as we execute it.\n",
        "\n",
        "![img](https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png)\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n",
        "\n",
        "The following makes a single step prediction:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LkS72AtweFT"
      },
      "source": [
        "class OneStep(keras.Model):\n",
        "  def __init__(self,\n",
        "               model,\n",
        "               chars_from_ids, \n",
        "               ids_from_chars,\n",
        "               temperature=1.0\n",
        "               ):\n",
        "    super(OneStep, self).__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    ## Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
        "    )\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "  \n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYvCLc0ByX1o"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl1iKjExyhAC"
      },
      "source": [
        "Let's run it in a loop to generate some text. Looking at the generated text, we'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifiULZN6y60K",
        "outputId": "cee08569-9c5f-4d8b-a6ce-c3eb33629c80"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['VOLUMNIA:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOLUMNIA:\n",
            "I would thou hear's.\n",
            "\n",
            "YORK:\n",
            "I thank you, gentlemen, what say'st\n",
            "thou, and by me with your servant?\n",
            "\n",
            "VINCENTIO:\n",
            "Carrior, let us see't.\n",
            "\n",
            "FROTH:\n",
            "Be petition; yea, and me, better, Biondello,\n",
            "But did return for my son Lucentio his\n",
            "wandering and-dance. Where Merbridou\n",
            "Must help them, and not to question; you\n",
            "will not be aged banish'd. Let him be grain'd:\n",
            "Therefore be made and herbson that was much, in piercing steps,\n",
            "Be telling of thy long-seding arms: her face,\n",
            "Came to the bill that way home to thee;\n",
            "But what o' that?\n",
            "\n",
            "FLORIZEL:\n",
            "And he, my soul seen had my father's son:\n",
            "Peace, I have set my life and like to do\n",
            "this thichey and the wheeling of that. Who's thee,\n",
            "Very fool.\n",
            "\n",
            "Second Henry:\n",
            "Be pit on 't: then, I say, I come:\n",
            "And now forthwith never were as fair and loat,\n",
            "'Twixt six and now believe them.'\n",
            "But what learning is the tempest there?\n",
            "\n",
            "LADY ANNE:\n",
            "I hope, shall stop what you most along;\n",
            "Our general homes gone,\n",
            "And had her noble house; why Hast thou move our\n",
            "extermaid to England;\n",
            "Add war \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5746331214904785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJQTI8nL0rT8"
      },
      "source": [
        "To improve the model perfomance we may want to train for more epochs. Adding new layers, Maybe try to use LSTM.\n",
        "\n",
        "If we want the model to generate text faster the easiest thing we can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGf7r-vK0Re_",
        "outputId": "143a0db4-eb64-4f3e-e80a-8549f948ed30"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "for res in result:\n",
        "  print(res.numpy().decode('utf-8'))\n",
        "\n",
        "print('\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "My lord, I make a surpate none of your cousin\n",
            "Men drudge alas, alarums! Who's there, ho?\n",
            "\n",
            "VINCENTIO:\n",
            "But what's his transfor natures?\n",
            "\n",
            "CORIOLANUS:\n",
            "My guilt be done! Then, as I am inward.\n",
            "\n",
            "BIANCA:\n",
            "And live, my lord,\n",
            "We shall be mercy, and I enter'd in my\n",
            "tomend on' that hath made a shame.\n",
            "\n",
            "ANGELO:\n",
            "Well; the very risor of your adversary\n",
            "As you the man that have my woes told;\n",
            "You that my heart as I. Ajove are you in words?\n",
            "\n",
            "GREMIO:\n",
            "But freeder,\n",
            "That, by ghass fly to his mouth, 'shall,\n",
            "And, for thou forgest what I be still and make him\n",
            "do what I wish for Rome! the prince your father\n",
            "To make a hurdy Kater his easily's royal,\n",
            "And grans many axe until we thought these fairest chairful seas:\n",
            "What I have said, Brakening battle strive,\n",
            "In this o' the Aufiance in the malmsey'd\n",
            "With heaven unwilling tongue.\n",
            "\n",
            "SLY:\n",
            "Welcome, Kate; for I have put meet be.\n",
            "\n",
            "PETRUCHIO:\n",
            "Be patient. What's the matter,\n",
            "The life to close a sisterhood of his.\n",
            "\n",
            "KATHARINA:\n",
            "Minion tapest in thy honey would\n",
            "I were a quarter'll \n",
            "ROMEO:\n",
            "More affections, inferior to resign\n",
            "Her body is banish'd, for ne'er against him:\n",
            "I had rather close by my order.\n",
            "\n",
            "Provost:\n",
            "Away, your fur, and made them both.\n",
            "\n",
            "JULIET:\n",
            "God-den, thou hast a child, whose good citizens\n",
            "Would prove hourly supper in his throne:\n",
            "He hath berew them both they blood, all dream'd\n",
            "Which will muster seize how he should produce\n",
            "With love, and rear in Hermione;\n",
            "And triburly Hedwards hath made by the\n",
            "next debt on us.\n",
            "\n",
            "First Citizen:\n",
            "Ye're lack oppose you: let him all have disparl'd me.\n",
            "\n",
            "VIRGILIA:\n",
            "Beseech your mercy, what would you hear\n",
            "Themselves hold the narrow seas;\n",
            "The angry bear for from the book of life,\n",
            "And he no more i' the villain: I have heard\n",
            "one oun one, he is a last ring. Doth my honest not\n",
            "come unto my noble feed I have hound,\n",
            "Or else the austereness of my faith of Jusais unto\n",
            "Oxford, which I do disinherit.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Measurely unpersolute to my side,\n",
            "And be a parlous malf a place dangerous\n",
            "To wife for Edward weeps much once restore\n",
            "To knock him f\n",
            "ROMEO:\n",
            "My lord, I like it well, my lord,\n",
            "And one that would disorder him to him:\n",
            "An ear o'erposier, they are proud confess,\n",
            "Because my poor beak to do it, hastily\n",
            "gets be. We sweet my rage.\n",
            "\n",
            "GLOUCESTER:\n",
            "Well, give me the master, hull a merry man to turn.\n",
            "\n",
            "JULIET:\n",
            "O sound!--I can. I sit in him\n",
            "throng: alan! wouldst thou go together!\n",
            "\n",
            "KING EDWARD IV:\n",
            "Nay, he is. I have no less than never Warwick\n",
            "As he is beauty men, you should not broke his\n",
            "true, unlawfully moved withal.\n",
            "\n",
            "BONA:\n",
            "My husband--God be in health command,\n",
            "Which eyes of one may drink a thousand leave\n",
            "To have him high me granted lend the bowels of their hearts.\n",
            "\n",
            "WARWICK:\n",
            "O, you be not, but Marcius coming home?\n",
            "\n",
            "VOLUMNIA:\n",
            "At enmity's mess: yes, you have been a noble and\n",
            "another, as ourselves, our house, whose heart\n",
            "think out of such a sovereign and the enryay's boldness,\n",
            "And we disdain them going.\n",
            "\n",
            "ANTIGONUS:\n",
            "And he's admarence,\n",
            "Colverse power in her son, and hase't to do\n",
            "What conduct times for thee to her:\n",
            "I'll in blood, they know what\n",
            "ROMEO:\n",
            "Sir, are you mock'd to do unprovided.\n",
            "\n",
            "Second Citizen:\n",
            "Correction, stand you wot: 'tis an inquire yet the injury\n",
            "I thank you, consorted to the entom.\n",
            "\n",
            "VIRLINIUS:\n",
            "I am out.\n",
            "\n",
            "VINCENTIO:\n",
            "Lucentio is it were true.\n",
            "\n",
            "PROSPERO:\n",
            "Mock redeed,\n",
            "but, I say, well haughty I entreat me hanged.\n",
            "Dinst thou move too much affection,\n",
            "Like too lambs intiged, and barl traitors in my love.\n",
            "Shief most accuse; I mean to us or latue.\n",
            "\n",
            "PETRUCHIO:\n",
            "Nay, I thank God all this world, I would make him\n",
            "To righ above before his enemies.\n",
            "Mock morning the never former lies;\n",
            "and, as he is a quit out of thy country's breast,\n",
            "Which face to have leisuned, usurp'd,\n",
            "Her father only not to quench it.\n",
            "\n",
            "GLOUCESTER:\n",
            "I cannot, sir, in a sweet floods, 'tis the fistiembo\n",
            "A' offects his rock.\n",
            "\n",
            "CAPULET:\n",
            "O heavens' lost for ever.\n",
            "\n",
            "HERMIONE:\n",
            "Signior Harry, until thou must be long.\n",
            "May it please you, sir. By that leasons\n",
            "To grat me in? another, my dear hard,\n",
            "That ever made grossly done to her,\n",
            "With cares sound: whither the Lord Stateou?\n",
            "\n",
            "\n",
            "ROMEO:\n",
            "Signior Baptista, who comest thou hast\n",
            "Dissolute all things swoil'd in him\n",
            "that hold itself plained head and heard-belock,\n",
            "Which let it be more retire.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "In God's name, take anointed ill. What, is't not war?\n",
            "\n",
            "MENENIUS:\n",
            "A bloody time well enough to us.\n",
            "Your best of York he is but boon, and yet the general lives,\n",
            "Strikes thy bashadies' malicianally began,\n",
            "For in your breast wish'd to behold him toward.\n",
            "\n",
            "YORK:\n",
            "I hope he will be talking; and make them no opecline,\n",
            "How do I from your victoriest:\n",
            "'Romeo is banished!' Thursday, tell me,\n",
            "For I will summer a tay as young so dealt?\n",
            "O place where on eive mans, stand be a wife\n",
            "To him and pound our services, to each content\n",
            "To win the friendly brothers in Ludue.\n",
            "Cousin Angoly comes to hear it; I\n",
            "whither it be persiace the kingdoms.\n",
            "\n",
            "GRUMIO:\n",
            "I would they be tentellest of your means?\n",
            "I am no little both in this sea-stoo,\n",
            "And brought to light thee with anon. Lady:\n",
            "Allas for the fair soul--\n",
            "\n",
            "Nurse:\n",
            "\n",
            "JULIENCE:\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What\n",
            "\n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.358760595321655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la8Y4UJh1yl2"
      },
      "source": [
        "### Export the generator model.\n",
        "This single-step model can easily be saved and restored, allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_jXkUbE1Mc0",
        "outputId": "55a34ce7-53c5-425b-9fab-4d4ece0c48b1"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fd37ad00810>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDMXfOWk1-aj"
      },
      "source": [
        "### Testing the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSxMrPdM14x6",
        "outputId": "496cb60d-5abc-4946-ab7d-c6672c8be92f"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "There is no grain as prestrable to my soul.\n",
            "Go, fall another, I must have stain'd with rebits\n",
            "And i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA_0byzZ2VFA"
      },
      "source": [
        "### Advanced Custom training\n",
        "\n",
        "The above training procedure is simple, but does not give we much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that we've seen how to run the model manually next we'll implement the training loop. This gives a starting point if, for example, we want to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. we can learn more about this approach by reading the eager execution guide.\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "Calculate the updates and apply them to the model using the optimizer.\n",
        "The basic procedure is:\n",
        "\n",
        "* Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "* Calculate the updates and apply them to the model using the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1tCxkKx2AOg"
      },
      "source": [
        "class CustomTraining(Model):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lbQSnEM2yOz"
      },
      "source": [
        "The above implementation of the `train_step` method follows Keras' `train_step` conventions. This is optional, but it allows you to change the behavior of the train step and still use keras' Model.compile and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq8LqvGh2p4h"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spVKOU5x22wY"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Hmwrnq24q1",
        "outputId": "54049c07-363d-45db-ed00-fcd1282f9488"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 27s 136ms/step - loss: 2.7101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd379bb3410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg03CiKV2-YQ"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amlsZ6qo2566"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkjy7tLS3E3u"
      },
      "source": [
        "### Credits\n",
        "* [this tutorial](https://www.tensorflow.org/text/tutorials/text_generation)\n",
        "* [tensorflow docs](https://www.tensorflow.org/api_docs/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ7Nvemy3A7a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}