{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_LSTM_word-level-text-gen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBqPiQJFp_I3"
      },
      "source": [
        "### Word level text generation\n",
        "\n",
        "In this notebook we are going to learn how to perform a word level text generation in keras. In this notebook we are going to create a model that will be able to generate news headlines using [New York Times Comment and Headlines](https://www.kaggle.com/aashita/nyt-comments) dataset wich i got from kaggle.\n",
        "\n",
        "I've already downloaded the dataset and loaded so I will load it from here eaisly.\n",
        "\n",
        "### Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J74qtHCJXqa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732edcc4-8598-445e-c2c5-481687e282fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx36fbJirBxm"
      },
      "source": [
        "### Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eBk1Zrxtp7xj",
        "outputId": "e8a564dc-7ed4-4194-d091-91e7c4ea3c53"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import os, time, math, string\n",
        "\n",
        "np.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.19.5'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljvqyz3rhd4"
      },
      "source": [
        "#### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lug0yJVdp7up",
        "outputId": "9a98293d-02a4-495b-990b-4718c0f6c1a0"
      },
      "source": [
        "base_path = '/content/drive/My Drive/NLP Data/text-gen/New York Times Comments'\n",
        "os.path.exists(base_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu7ZBabYp7qP",
        "outputId": "d078e15e-33f8-4bf1-f63a-7bfe9417fe90"
      },
      "source": [
        "%%time\n",
        "all_headlines = []\n",
        "for filename in os.listdir(base_path):\n",
        "  if 'Articles' in filename:\n",
        "    df = pd.read_csv(os.path.join(base_path, filename))\n",
        "    all_headlines.extend(list(\n",
        "        df.headline.values\n",
        "    ))\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "print(len(all_headlines))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8603\n",
            "CPU times: user 92.4 ms, sys: 17.2 ms, total: 110 ms\n",
            "Wall time: 185 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7mB3D3gp7n-",
        "outputId": "72613e61-b710-434a-ff4e-dd446d5c06f4"
      },
      "source": [
        "all_headlines[:15]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
              " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
              " 'The New Noma, Explained',\n",
              " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
              " 'Is School a Place for Self-Expression?',\n",
              " 'Commuter Reprogramming',\n",
              " 'Ford Changed Leaders, Looking for a Lift. It’s Still Looking.',\n",
              " 'Romney Failed to Win at Utah Convention, But Few Believe He’s Doomed',\n",
              " 'Chain Reaction',\n",
              " 'He Forced the Vatican to Investigate Sex Abuse. Now He’s Meeting With Pope Francis.',\n",
              " 'In Berlin, artists find a home',\n",
              " 'The Right Stuff',\n",
              " 'Jimmy Carter Knows What North Korea Wants',\n",
              " 'The Truth Is Out There',\n",
              " 'New Jersey Ruling Could Reignite Battle Over Church-State Separation']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQXMR5VvteYG"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "The first step that we are going to do for data preperation is text-clening. We are going to remove punctuations and lowercase all words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0-vorPap7je",
        "outputId": "10503450-be34-4d8a-d66c-064ed99384ad"
      },
      "source": [
        "def clean_text(txt):\n",
        "  txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "  txt = txt.encode('utf8').decode('ascii', 'ignore')\n",
        "  return txt\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "corpus[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
              " 'epa to unveil a new rule its effect less science in policymaking',\n",
              " 'the new noma explained',\n",
              " 'how a bag of texas dirt  became a times tradition',\n",
              " 'is school a place for selfexpression',\n",
              " 'commuter reprogramming',\n",
              " 'ford changed leaders looking for a lift its still looking',\n",
              " 'romney failed to win at utah convention but few believe hes doomed',\n",
              " 'chain reaction',\n",
              " 'he forced the vatican to investigate sex abuse now hes meeting with pope francis']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUaQthWUxpq2"
      },
      "source": [
        "### Generating Sequence of N-grams Tokens\n",
        "Language modelling requires a sequence input data, as given a sequence of word-tokens the aim is to predict the next word.\n",
        "\n",
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9bm_bWsp7hR"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "  input_sequences = []\n",
        "  for line in corpus:\n",
        "    token_list= tokenizer.texts_to_sequences([\n",
        "        line\n",
        "    ])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      input_sequences.append(n_gram_sequence)\n",
        "  return input_sequences, total_words"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwJX110hp7Vu",
        "outputId": "4aa0888c-57bb-4f35-e1d8-ef043c9b5398"
      },
      "source": [
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:28]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[391, 915],\n",
              " [391, 915, 2484],\n",
              " [391, 915, 2484, 5166],\n",
              " [391, 915, 2484, 5166, 664],\n",
              " [391, 915, 2484, 5166, 664, 78],\n",
              " [391, 915, 2484, 5166, 664, 78, 7],\n",
              " [391, 915, 2484, 5166, 664, 78, 7, 2],\n",
              " [391, 915, 2484, 5166, 664, 78, 7, 2, 475],\n",
              " [391, 915, 2484, 5166, 664, 78, 7, 2, 475, 11],\n",
              " [391, 915, 2484, 5166, 664, 78, 7, 2, 475, 11, 5167],\n",
              " [331, 3],\n",
              " [331, 3, 5168],\n",
              " [331, 3, 5168, 2],\n",
              " [331, 3, 5168, 2, 12],\n",
              " [331, 3, 5168, 2, 12, 1011],\n",
              " [331, 3, 5168, 2, 12, 1011, 22],\n",
              " [331, 3, 5168, 2, 12, 1011, 22, 736],\n",
              " [331, 3, 5168, 2, 12, 1011, 22, 736, 332],\n",
              " [331, 3, 5168, 2, 12, 1011, 22, 736, 332, 412],\n",
              " [331, 3, 5168, 2, 12, 1011, 22, 736, 332, 412, 5],\n",
              " [331, 3, 5168, 2, 12, 1011, 22, 736, 332, 412, 5, 5169],\n",
              " [1, 12],\n",
              " [1, 12, 5170],\n",
              " [1, 12, 5170, 3366],\n",
              " [14, 2],\n",
              " [14, 2, 3367],\n",
              " [14, 2, 3367, 4],\n",
              " [14, 2, 3367, 4, 562]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CT-qh9xzgZU"
      },
      "source": [
        "### How ngrams works?\n",
        "\n",
        "```\n",
        "[1, 12] -> this is\n",
        "[1, 12, 5170] -> this is a\n",
        "[1, 12, 5170, 3366]-> this is a dog\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P7DqzUJz6H1"
      },
      "source": [
        "### Padding sequences and generating target.\n",
        "\n",
        "Sequences must be of the same length. Ww will create n-grams sequences as predictors and the next word of the n-grams as label.\n",
        "\n",
        "```\n",
        "they -> are (label)\n",
        "they are -> standing (label)\n",
        "they are standing -> at (label)\n",
        "they are standing at -> the (label)\n",
        "they are standing at the -> door (label)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU0uHnKFp7Sk"
      },
      "source": [
        "def generate_padded_sequences(input_sequences):\n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, \n",
        "                                            padding='pre'))\n",
        "  predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "  label = keras.utils.to_categorical(label, num_classes=total_words)\n",
        "  return predictors, label, max_sequence_len"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcC7Lk59p7P9"
      },
      "source": [
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIaO1FaR3I5B"
      },
      "source": [
        "### LSTM for text generation\n",
        "\n",
        "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
        "\n",
        "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
        "\n",
        "LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3fROykH3uNU",
        "outputId": "c4d0ac90-1bad-414f-eb41-3094027ff3fe"
      },
      "source": [
        "predictors"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,    0,    0,  391],\n",
              "       [   0,    0,    0, ...,    0,  391,  915],\n",
              "       [   0,    0,    0, ...,  391,  915, 2484],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 3044,    4,   91],\n",
              "       [   0,    0,    0, ...,    4,   91,   13],\n",
              "       [   0,    0,    0, ...,   91,   13,    1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCb7l--1k6S",
        "outputId": "d272ba5d-5b28-4d2d-c15e-4f9f805f8556"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "  input_len = max_sequence_len - 1\n",
        "  model = keras.Sequential(name=\"text-generator\")\n",
        "  \n",
        "  # Add Input Embedding Layer\n",
        "  model.add(keras.layers.Embedding(total_words, 100,\n",
        "                                    input_length=input_len))\n",
        "  \n",
        "  # Add Hidden Layer 1 - LSTM Layer\n",
        "  model.add(keras.layers.LSTM(100))\n",
        "  model.add(keras.layers.Dropout(0.1))\n",
        "  \n",
        "  # Add Output Layer\n",
        "  model.add(keras.layers.Dense(total_words, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam', metrics=[\"acc\"])\n",
        "  return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"text-generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 23, 100)           1126500   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 11265)             1137765   \n",
            "=================================================================\n",
            "Total params: 2,344,665\n",
            "Trainable params: 2,344,665\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TLnGvTU17zy",
        "outputId": "5410c8aa-5369-457a-e3fc-42d709bcb164"
      },
      "source": [
        "model.fit(predictors, label,\n",
        "          epochs=5, verbose=1,\n",
        "          shuffle=True,\n",
        "          validation_split=.1\n",
        "          )"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1457/1457 [==============================] - 25s 16ms/step - loss: 7.8566 - acc: 0.0383 - val_loss: 7.6306 - val_acc: 0.0541\n",
            "Epoch 2/5\n",
            "1457/1457 [==============================] - 22s 15ms/step - loss: 7.3522 - acc: 0.0544 - val_loss: 7.5598 - val_acc: 0.0657\n",
            "Epoch 3/5\n",
            "1457/1457 [==============================] - 22s 15ms/step - loss: 7.0050 - acc: 0.0701 - val_loss: 7.5182 - val_acc: 0.0840\n",
            "Epoch 4/5\n",
            "1457/1457 [==============================] - 22s 15ms/step - loss: 6.6334 - acc: 0.0905 - val_loss: 7.5503 - val_acc: 0.0914\n",
            "Epoch 5/5\n",
            "1457/1457 [==============================] - 22s 15ms/step - loss: 6.2566 - acc: 0.1102 - val_loss: 7.6219 - val_acc: 0.0945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8dba85f290>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr1CJw1K57c-"
      },
      "source": [
        "### Generating text.\n",
        "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyYlmp1d4vET"
      },
      "source": [
        "def generate_text(seed_text, max_len, model, max_sequence_len):\n",
        "  for _ in range(max_len):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list],\n",
        "                               maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list)\n",
        "    predicted = np.argmax(predicted, axis=1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "  return seed_text.title()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuKb9rEy6P4D",
        "outputId": "0273516a-b5af-434c-bee9-182affb87f72"
      },
      "source": [
        "print (generate_text(\"united states\", 5, model, max_sequence_len))\n",
        "print (generate_text(\"preident trump\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"donald trump\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"india and china\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"new york\", 4, model, max_sequence_len))\n",
        "print (generate_text(\"science and technology\", 5, model, max_sequence_len))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "United States And The New York Times\n",
            "Preident Trump And The New York\n",
            "Donald Trump And The New York\n",
            "India And China And The New York\n",
            "New York Today The New York\n",
            "Science And Technology A New York Times Is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4bLFWRT6PfM"
      },
      "source": [
        "### Conclusion\n",
        "We have leant how to use LSTM for text generation by expanding [this kaggle](https://www.kaggle.com/garidziracrispen/beginners-guide-to-text-generation-using-lstms) notebook. Next we will try to implement this idea and try to create a model that will be a little bit accurate than this one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU0YRvOjBSN_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}