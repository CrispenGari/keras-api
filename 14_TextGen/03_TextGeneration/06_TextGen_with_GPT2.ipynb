{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_TextGen_with_GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skE9g37QJHki"
      },
      "source": [
        "###  Generating text GPT-2\n",
        "In this notebook we are going to learn how to use GPT-2 model to generate text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydMiKSzRJtsa"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3WUVNsWI83S"
      },
      "source": [
        "from transformers import TFOpenAIGPTLMHeadModel, OpenAIGPTTokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4zZ-rs6Kd5D"
      },
      "source": [
        "### GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "631c6af52ac940eba80431b955cc0e32"
          ]
        },
        "id": "dNzvmJmtJiNc",
        "outputId": "e5bcfc9e-5a2f-4db3-def8-ce049e52eb41"
      },
      "source": [
        "gpttokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "gpt = TFOpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "631c6af52ac940eba80431b955cc0e32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
            "\n",
            "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjIhfj_xJpCo",
        "outputId": "9ce14311-fa5b-44ea-987c-fcfa5353adbd"
      },
      "source": [
        "input_ids = gpttokenizer.encode('Robotics is the ', return_tensors='tf')\n",
        "print(input_ids)\n",
        "greedy_output = gpt.generate(input_ids, max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(gpttokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[5846 9259  544  481]], shape=(1, 4), dtype=int32)\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "robotics is the only way to get to the surface. \" \n",
            " \" i'm not sure i understand. \" \n",
            " \" the first thing we have to do is find a way to get to the surface. \" \n",
            " \" but how? \" \n",
            " \" we have to find a way to get to the surface. \" \n",
            " \" but how? \" \n",
            " \" we have to find a way to get to the surface. \" \n",
            " \" but how? \" \n",
            " \" we have to find a way to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fnEcaIzKWZY"
      },
      "source": [
        "### GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GbIOIy8KZpn",
        "outputId": "9bc0d7fa-b48a-43ab-dd51-ab3794b324e0"
      },
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "gpt2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "gpt2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", \n",
        "                                         pad_token_id=gpt2tokenizer.eos_token_id)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFSWvtKIKH8J",
        "outputId": "d1d5109f-71f8-4b92-fa33-36b71b0d83f7"
      },
      "source": [
        "\n",
        "# encode context the generation is conditioned on\n",
        "input_ids = gpt2tokenizer.encode('Robotics is the ', return_tensors='tf')\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = gpt2.generate(input_ids, max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 50 * '-')\n",
        "print(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "--------------------------------------------------\n",
            "Robotics is the vernacular of the future.\n",
            "\n",
            "The future is not a future where robots are going to be able to do anything. It's a future where robots are going to be able to do anything.\n",
            "\n",
            "The future is a future where robots are going to be able to do anything.\n",
            "\n",
            "The future is a future where robots are going to be able to do anything.\n",
            "\n",
            "The future is a future where robots are going to be able to do anything.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIH2IxJqKpQS"
      },
      "source": [
        "Ref:\n",
        "\n",
        "https://github.com/PacktPublishing/Advanced-NLP-with-TensorFlow-2/blob/main/chapter5-nlg-with-transformer-gpt/text-generation-with-GPT-2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZeW79CaKOdv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}