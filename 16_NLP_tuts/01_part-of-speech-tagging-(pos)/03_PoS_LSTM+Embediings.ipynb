{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_PoS_LSTM+Embediings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1pYbAVBbuno"
      },
      "source": [
        "### PoS - Part of Speech Tagging \n",
        "\n",
        "As promised in the previous notebook, In this notebook we are going to show how we can use the `LSTM` layer together with the `Embedding` layer using `word2vec` vectors as the weigths of the embedding layer.\n",
        "\n",
        "\n",
        "**Note:** The rest of the notebokk will remain the same as the previous notebook the only difference is that this time around we are going to load the data from a file called `pos.csv`.\n",
        "\n",
        "### Part of Speech Tagging (PoS)\n",
        "\n",
        "This is a process of classifying words into their part of speech.\n",
        "\n",
        "### Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nNWG8ks1buYp",
        "outputId": "e2128340-e309-4764-8672-2cea4bf0ae25"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import brown, treebank, conll2000\n",
        "\n",
        "import os, time, re, string, random, nltk\n",
        "tf.__version__"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLC_AJA3eqHg"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "We are going to load the data from a `csv` file named `pos.csv` from our google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKjUB7ZSkI3F",
        "outputId": "a0fc7ade-e7aa-432e-f638-18a2068c2acf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghTccUg8zOtd",
        "outputId": "dd8d419f-b9e3-439c-9fb0-ebb0e41b1414"
      },
      "source": [
        "file_path = \"/content/drive/My Drive/NLP Data/pos-datasets/english/pos.csv\"\n",
        "os.path.exists(file_path)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "Kc95-6KpbuTN",
        "outputId": "db802835-1450-4ddd-decf-6d8e6b8d6c3e"
      },
      "source": [
        "dataframe = pd.read_csv(file_path)\n",
        "dataframe.head(5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Fulton County Grand Jury said Friday an in...</td>\n",
              "      <td>DET NOUN NOUN ADJ NOUN VERB NOUN DET NOUN ADP ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The jury further said in term-end presentments...</td>\n",
              "      <td>DET NOUN ADV VERB ADP NOUN NOUN ADP DET NOUN A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The September-October term jury had been charg...</td>\n",
              "      <td>DET NOUN NOUN NOUN VERB VERB VERB ADP NOUN ADJ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>`` Only a relative handful of such reports was...</td>\n",
              "      <td>. ADV DET ADJ NOUN ADP ADJ NOUN VERB VERB . . ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The jury said it did find that many of Georgia...</td>\n",
              "      <td>DET NOUN VERB PRON VERB VERB ADP ADJ ADP NOUN ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence                                               tags\n",
              "0  The Fulton County Grand Jury said Friday an in...  DET NOUN NOUN ADJ NOUN VERB NOUN DET NOUN ADP ...\n",
              "1  The jury further said in term-end presentments...  DET NOUN ADV VERB ADP NOUN NOUN ADP DET NOUN A...\n",
              "2  The September-October term jury had been charg...  DET NOUN NOUN NOUN VERB VERB VERB ADP NOUN ADJ...\n",
              "3  `` Only a relative handful of such reports was...  . ADV DET ADJ NOUN ADP ADJ NOUN VERB VERB . . ...\n",
              "4  The jury said it did find that many of Georgia...  DET NOUN VERB PRON VERB VERB ADP ADJ ADP NOUN ..."
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLkq2eHS2dDl"
      },
      "source": [
        "### Dataset creation\n",
        "\n",
        "Now in our dataset we a sentence of words and a sentence of tags, which each tag correspont to a word in a sentence of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rabLeUClevj",
        "outputId": "25ad368b-0112-4d4d-bc68-b96bbba1dd9f"
      },
      "source": [
        "X = dataframe.sentence.values\n",
        "y = dataframe.tags.values\n",
        "\n",
        "print(X[0])\n",
        "print(y[0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "DET NOUN NOUN ADJ NOUN VERB NOUN DET NOUN ADP NOUN ADJ NOUN NOUN VERB . DET NOUN . ADP DET NOUN VERB NOUN .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZOcw6zhsOXt"
      },
      "source": [
        "X = list(map(lambda x: x.split(\" \"), list(X)))\n",
        "y = list(map(lambda x: x.split(\" \"), list(y)))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4yznSIt6ke-"
      },
      "source": [
        "### Data Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc08l1pL512i"
      },
      "source": [
        "words = []\n",
        "tags = []\n",
        "\n",
        "for sentence in X:\n",
        "  for word in sentence:\n",
        "    words.append(word.lower())\n",
        "\n",
        "for tag in y:\n",
        "  for t in tag:\n",
        "    tags.append(t.lower())\n",
        "num_words = len(set(words))\n",
        "num_tags = len(set(tags))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVxRolH_ncnL",
        "outputId": "d7fe8566-bf56-458c-bcf4-617da9a09a6b"
      },
      "source": [
        "set(tags)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.',\n",
              " 'adj',\n",
              " 'adp',\n",
              " 'adv',\n",
              " 'conj',\n",
              " 'det',\n",
              " 'noun',\n",
              " 'num',\n",
              " 'pron',\n",
              " 'prt',\n",
              " 'verb',\n",
              " 'x'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAMPJWaX7K8W",
        "outputId": "50c8cbab-c780-4058-af80-f261151768fd"
      },
      "source": [
        "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
        "print(\"Vocabulary size: {}\".format(num_words))\n",
        "print(\"Total number of tags: {}\".format(num_tags))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tagged sentences: 72202\n",
            "Vocabulary size: 59448\n",
            "Total number of tags: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDpN_kwt7OQt"
      },
      "source": [
        "### Checking examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDzxH69N7Lbn",
        "outputId": "c04f7e50-1f53-44e8-c568-144c92871ef4"
      },
      "source": [
        "print(\"Sample x: \", X[0], \"\\n\")\n",
        "print(\"Sample y: \", y[0], \"\\n\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample x:  ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
            "\n",
            "Sample y:  ['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRzGBx8F7iPe"
      },
      "source": [
        "### Text vectorization\n",
        "\n",
        "We are going to use the `Tokenizer` class to encode text from sequences to sequence of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6I-rrUf7dSG"
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(split=' ')\n",
        "tokenizer.fit_on_texts(X)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4lGE2K47dKe"
      },
      "source": [
        "tag_tokenizer = keras.preprocessing.text.Tokenizer(split=' ')\n",
        "tag_tokenizer.fit_on_texts(y)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiAZNI-A8mr7",
        "outputId": "072511b1-652b-4e55-b12d-53345b98d0cf"
      },
      "source": [
        "tag_tokenizer.word_index"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 3,\n",
              " 'adj': 6,\n",
              " 'adp': 4,\n",
              " 'adv': 7,\n",
              " 'conj': 9,\n",
              " 'det': 5,\n",
              " 'noun': 1,\n",
              " 'num': 11,\n",
              " 'pron': 8,\n",
              " 'prt': 10,\n",
              " 'verb': 2,\n",
              " 'x': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NQpnq4G8vpd"
      },
      "source": [
        "Now we can convert our tokens to sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCC0iAXP7dHE"
      },
      "source": [
        "sentences_sequences = tokenizer.texts_to_sequences(X)\n",
        "tags_sequences = tag_tokenizer.texts_to_sequences(y)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2f-IPa-9OQ5"
      },
      "source": [
        "### Checking a single example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnXl0QB8ogJd",
        "outputId": "6cc85d42-f12e-4cf7-bd3f-17fc360ed1fe"
      },
      "source": [
        "len(tags_sequences[3]), len(sentences_sequences[3])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37, 37)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4WRXdu9dQX"
      },
      "source": [
        "Let's convert tag tokens back to word representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIifihSD7ZGe",
        "outputId": "d589ba09-d430-4ec1-f9b5-01d9ee984b9c"
      },
      "source": [
        "print(\"Y[0]: \", y[0])\n",
        "print(\"tags_sequences[0]: \", tags_sequences[0])\n",
        "print(\"sequences_to_tags[0]: \", \n",
        "      tag_tokenizer.sequences_to_texts([tags_sequences[0]]))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y[0]:  ['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n",
            "tags_sequences[0]:  [5, 1, 1, 6, 1, 2, 1, 5, 1, 4, 1, 6, 1, 1, 2, 3, 5, 1, 3, 4, 5, 1, 2, 1, 3]\n",
            "sequences_to_tags[0]:  ['det noun noun adj noun verb noun det noun adp noun adj noun noun verb . det noun . adp det noun verb noun .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsrBx8t-lNp"
      },
      "source": [
        "### Checking if the inputs and outputs have the same length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aqugfsQ-lAU",
        "outputId": "ff0833ff-7f5c-4f6e-b8ad-33e64b5528d3"
      },
      "source": [
        "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(tags_sequences, sentences_sequences)]\n",
        "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 sentences have disparate input-output lengths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCw0Qdi1-JZ6"
      },
      "source": [
        "### Padding sequences\n",
        "\n",
        "Since the sentences has various length we are going to pad the sequences of these sentences to the longest sentence. We will make sure that these sequences are padded to have the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHVBiNgU9-tT",
        "outputId": "da90ce33-e47e-4b8f-d8b2-43f545e013b8"
      },
      "source": [
        "lengths = [len(seq) for seq in tags_sequences]\n",
        "MAX_LENGTH = max(lengths)\n",
        "print(f\"Longest sentence: {MAX_LENGTH}\")\n",
        "\n",
        "MAX_LENGTH = 100 # we are going to set the max-length to 100"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest sentence: 271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-DrTROo_TIe"
      },
      "source": [
        "padded_sentences = keras.preprocessing.sequence.pad_sequences(\n",
        "    sentences_sequences,\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")\n",
        "padded_tags = keras.preprocessing.sequence.pad_sequences(\n",
        "    tags_sequences,\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4MBc7WpANsM"
      },
      "source": [
        "Checking the a single example of the padded sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4KpXgTnAM12",
        "outputId": "f894e4e4-f91c-4934-c1f1-a8db51c0d34f"
      },
      "source": [
        "print(padded_sentences[0], \"\\n\"*2)\n",
        "print(padded_tags[0])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    1  5731   778  2326  1842    39   853    34  1944     4 16831   379\n",
            "  1343  1523  1116    12    67   569    14     9    89 10208   252   205\n",
            "     3     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n",
            "\n",
            "[5 1 1 6 1 2 1 5 1 4 1 6 1 1 2 3 5 1 3 4 5 1 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eftEgKviEtHA"
      },
      "source": [
        "### One-hot Encode `padded_tags` labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCVNUjAMEw5z",
        "outputId": "08867860-fc8b-4796-df7f-372aa4bb0c49"
      },
      "source": [
        "padded_tags = keras.utils.to_categorical(padded_tags)\n",
        "padded_tags.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72202, 100, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75exlRtXFY_x"
      },
      "source": [
        "### Set's spliting.\n",
        "\n",
        "We are then going to split the data into 3 sets using the `sklearn` `train_test_split` method to split our data train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h3UyVxtFNv-"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   padded_sentences, padded_tags, random_state=42, test_size=.15\n",
        ")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "   padded_sentences, padded_tags, random_state=42, test_size=.15\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wOZEU8GKsR"
      },
      "source": [
        "### Counting examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq_DTy7XGMK0",
        "outputId": "2d47008e-c072-4209-c5f6-bde7168dbe14"
      },
      "source": [
        "print(\"training: \", len(X_train))\n",
        "print(\"testing: \", len(X_test))\n",
        "print(\"validation: \", len(X_valid))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training:  61371\n",
            "testing:  10831\n",
            "validation:  10831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTcpukkaGhxC"
      },
      "source": [
        "### A simple RNN with word Embeddings\n",
        "\n",
        "As mentioned previously we are going to make use of the `word2vec` so to download these vectors we are going to run the following code cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIzN8SinthvM"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "word2vec = api.load('word2vec-google-news-300')\n",
        "\n",
        "vec_king = word2vec['king']"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTtd1BlRxlDW",
        "outputId": "f1ce73f8-b9e1-44cf-be7c-5344449779c7"
      },
      "source": [
        "vec_king"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
              "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
              "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
              "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
              "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
              "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
              "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
              "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
              "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
              "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
              "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
              "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
              "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
              "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
              "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
              "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
              "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
              "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
              "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
              "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
              "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
              "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
              "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
              "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
              "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
              "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
              "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
              "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
              "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
              "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
              "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
              "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
              "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
              "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
              "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
              "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
              "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
              "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
              "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
              "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
              "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
              "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
              "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
              "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
              "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
              "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
              "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
              "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
              "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
              "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
              "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
              "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
              "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
              "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
              "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
              "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
              "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
              "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
              "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
              "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
              "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
              "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
              "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
              "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
              "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
              "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
              "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
              "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
              "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
              "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
              "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
              "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
              "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
              "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
              "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDawpXDsuXil"
      },
      "source": [
        "### Theory behind word vectors.\n",
        "\n",
        "Words with the simmilar meaning are closer to each other in the vector space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkGgag4QurV9"
      },
      "source": [
        "### Number of classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9azdoT9Ga8y",
        "outputId": "1952f491-f8ee-4ff3-f8d9-09a3721d86a5"
      },
      "source": [
        "n_classes = y_train.shape[-1]\n",
        "n_classes"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zKjEy6Fu13p"
      },
      "source": [
        "### Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9CL_y0UHgDj",
        "outputId": "644009c3-e56e-4bbf-834f-81becfa107d1"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "EMBEDDING_SIZE = 300 # each word in word2vec model is represented using a 300 dimensional vector\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "VOCAB_SIZE"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59449"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1kwKgGlu-LC"
      },
      "source": [
        "### Creating an embedding matrix that suits our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLAnPafovC2x"
      },
      "source": [
        "# Initialize the embedding weights with zeros\n",
        "embedding_weigths = np.zeros([\n",
        "   VOCAB_SIZE, EMBEDDING_SIZE\n",
        "])\n",
        "\n",
        "# Getting the string to integer mapping\n",
        "stoi = tokenizer.word_index\n",
        "\n",
        "# copying vectors from word2vec to our embedding matrix that suits our data\n",
        "\n",
        "for word, index in stoi.items():\n",
        "  try:\n",
        "    embedding_weigths[index, :] = word2vec[word]\n",
        "  except:\n",
        "    pass\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SigfzMFTwFWm"
      },
      "source": [
        "### Checking the dimention of our embedding weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGvTt3PnvCgC",
        "outputId": "f98079e1-700e-4f35-924c-6b56831b3ad5"
      },
      "source": [
        "print(\"Embeddings shape: {}\".format(embedding_weigths.shape))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (59449, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXzJeRv3wNhU"
      },
      "source": [
        "#### Checking a single word in our embedding weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrznxvIVwM5r",
        "outputId": "a7026e8c-4efe-40a0-e1b1-b640f895faa3"
      },
      "source": [
        "embedding_weigths[tokenizer.word_index['the']]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281,\n",
              "       -0.12060547,  0.03515625, -0.11865234,  0.04394531,  0.03015137,\n",
              "       -0.05688477, -0.07617188,  0.01287842,  0.04980469, -0.08496094,\n",
              "       -0.06347656,  0.00628662, -0.04321289,  0.02026367,  0.01330566,\n",
              "       -0.01953125,  0.09277344, -0.171875  , -0.00131989,  0.06542969,\n",
              "        0.05834961, -0.08251953,  0.0859375 , -0.00318909,  0.05859375,\n",
              "       -0.03491211, -0.0123291 , -0.0480957 , -0.00302124,  0.05639648,\n",
              "        0.01495361, -0.07226562, -0.05224609,  0.09667969,  0.04296875,\n",
              "       -0.03540039, -0.07324219,  0.03271484, -0.06176758,  0.00787354,\n",
              "        0.0035553 , -0.00878906,  0.0390625 ,  0.03833008,  0.04443359,\n",
              "        0.06982422,  0.01263428, -0.00445557, -0.03320312, -0.04272461,\n",
              "        0.09765625, -0.02160645, -0.0378418 ,  0.01190186, -0.01391602,\n",
              "       -0.11328125,  0.09326172, -0.03930664, -0.11621094,  0.02331543,\n",
              "       -0.01599121,  0.02636719,  0.10742188, -0.00466919,  0.09619141,\n",
              "        0.0279541 , -0.05395508,  0.08544922, -0.03686523, -0.02026367,\n",
              "       -0.08544922,  0.125     ,  0.14453125,  0.0267334 ,  0.15039062,\n",
              "        0.05273438, -0.18652344,  0.08154297, -0.01062012, -0.03735352,\n",
              "       -0.07324219, -0.07519531,  0.03613281, -0.13183594,  0.00616455,\n",
              "        0.05078125,  0.04516602,  0.0100708 , -0.15039062, -0.06005859,\n",
              "        0.05761719, -0.00692749,  0.01586914, -0.0213623 ,  0.10351562,\n",
              "       -0.00029182, -0.046875  , -0.01635742, -0.07861328, -0.06933594,\n",
              "        0.01635742, -0.03149414, -0.01373291, -0.03662109, -0.08886719,\n",
              "       -0.0480957 , -0.01318359, -0.07177734,  0.00588989, -0.04614258,\n",
              "        0.03979492,  0.10058594, -0.04931641,  0.07568359,  0.03881836,\n",
              "       -0.16699219, -0.09619141, -0.10107422,  0.02905273, -0.05786133,\n",
              "       -0.01928711, -0.04296875, -0.08398438, -0.01989746,  0.05151367,\n",
              "        0.00848389, -0.03613281, -0.14941406, -0.01855469, -0.03637695,\n",
              "       -0.07666016, -0.03955078, -0.06152344, -0.02001953,  0.04150391,\n",
              "        0.03686523, -0.07226562,  0.00592041, -0.06298828,  0.00738525,\n",
              "       -0.01586914,  0.01611328, -0.01452637,  0.00772095,  0.10107422,\n",
              "       -0.00558472,  0.01428223, -0.07617188,  0.05639648, -0.01293945,\n",
              "        0.03063965, -0.02490234, -0.09863281,  0.0324707 , -0.02807617,\n",
              "       -0.08105469,  0.02062988,  0.01611328, -0.04199219, -0.03491211,\n",
              "       -0.03759766,  0.05493164,  0.01373291,  0.02685547, -0.05859375,\n",
              "       -0.07177734, -0.12011719, -0.02282715, -0.1640625 , -0.00361633,\n",
              "       -0.05981445,  0.07080078, -0.07714844,  0.05175781, -0.04296875,\n",
              "       -0.04833984,  0.0300293 , -0.06591797, -0.03173828, -0.04882812,\n",
              "       -0.03491211,  0.05883789, -0.01464844,  0.18066406,  0.05688477,\n",
              "        0.05249023,  0.05786133,  0.11669922,  0.05200195, -0.0534668 ,\n",
              "        0.01867676, -0.015625  ,  0.00576782, -0.07324219, -0.11621094,\n",
              "        0.04052734,  0.0625    , -0.04321289,  0.01055908,  0.02172852,\n",
              "        0.04248047,  0.03271484,  0.04418945,  0.05761719,  0.02612305,\n",
              "       -0.01831055, -0.02697754, -0.00674438,  0.00509644, -0.11621094,\n",
              "        0.00364685,  0.05761719, -0.05957031, -0.08837891,  0.0135498 ,\n",
              "        0.04541016, -0.04638672, -0.0177002 , -0.0625    ,  0.03442383,\n",
              "       -0.02416992,  0.03088379,  0.09570312,  0.07958984,  0.03930664,\n",
              "        0.0279541 , -0.0859375 ,  0.08105469,  0.06640625, -0.00041962,\n",
              "       -0.06933594,  0.03588867, -0.03417969,  0.04492188, -0.00772095,\n",
              "       -0.00741577, -0.04760742,  0.01397705, -0.09960938,  0.0246582 ,\n",
              "       -0.09960938,  0.11474609,  0.03173828,  0.02209473,  0.07226562,\n",
              "        0.03686523,  0.02563477,  0.01367188, -0.02734375,  0.00592041,\n",
              "       -0.06738281,  0.05053711, -0.02832031, -0.04516602, -0.01733398,\n",
              "        0.02111816,  0.03515625, -0.04296875,  0.06640625,  0.12207031,\n",
              "        0.12353516,  0.0039978 ,  0.04516602, -0.01855469,  0.04833984,\n",
              "        0.04516602,  0.08691406,  0.02941895,  0.03759766,  0.03442383,\n",
              "       -0.07373047, -0.0402832 , -0.14648438, -0.02441406, -0.01953125,\n",
              "        0.0065918 , -0.0018158 , -0.01092529,  0.09326172,  0.06542969,\n",
              "        0.01843262, -0.09326172, -0.01574707, -0.07128906, -0.08935547,\n",
              "       -0.07128906, -0.03015137, -0.01300049,  0.01635742, -0.01831055,\n",
              "        0.01483154,  0.00500488,  0.00366211,  0.04760742, -0.06884766])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VV048hLwbCD"
      },
      "source": [
        "### Building an RNN using pretrained weigths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok0UE_ENG5fk",
        "outputId": "4a9aac29-119d-447d-9c7a-c6554f6617c3"
      },
      "source": [
        "rnn_model = keras.Sequential([\n",
        "    keras.layers.Embedding(\n",
        "      VOCAB_SIZE, EMBEDDING_SIZE, input_length=MAX_SEQUENCE_LENGTH,\n",
        "      trainable = True,\n",
        "      weights = [embedding_weigths]\n",
        "    ),\n",
        "    keras.layers.LSTM(64, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(\n",
        "        keras.layers.Dense(n_classes, activation=\"softmax\")\n",
        "    )\n",
        "], name=\"lstm_rnn\")\n",
        "\n",
        "rnn_model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_rnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 300)          17834700  \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100, 64)           93440     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 100, 13)           845       \n",
            "=================================================================\n",
            "Total params: 17,928,985\n",
            "Trainable params: 17,928,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icv5RR92LzDp"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHsE7LdWLm5A",
        "outputId": "5f1fe83f-82e6-4b42-8a6e-bdab53cd13d9"
      },
      "source": [
        "rnn_model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "rnn_model.fit(\n",
        "    X_train, y_train, batch_size=128, \n",
        "    epochs=2, validation_data=(X_valid, y_valid)\n",
        ")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "480/480 [==============================] - 233s 481ms/step - loss: 0.2691 - acc: 0.9347 - val_loss: 0.0428 - val_acc: 0.9880\n",
            "Epoch 2/2\n",
            "480/480 [==============================] - 231s 482ms/step - loss: 0.0314 - acc: 0.9902 - val_loss: 0.0282 - val_acc: 0.9903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb26f6d8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeXTShyfMRaC"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdRYYIj8MMOY",
        "outputId": "b097efe4-4695-4fff-bd51-27ab53b7ab51"
      },
      "source": [
        "rnn_model.evaluate(X_test, y_test, verbose=1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "339/339 [==============================] - 9s 28ms/step - loss: 0.0282 - acc: 0.9903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02823222056031227, 0.9903010129928589]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-723LG8Map9"
      },
      "source": [
        "### Model Inference\n",
        "\n",
        "Now we are ready to make predictions of our tags. We are going to perform the following steps in the `make_prediction` function.\n",
        "1. tokenize the sentence\n",
        "2. convert the tokenized sentence to integer representation\n",
        "3. padd the tokenized sentences and pass them to the model\n",
        "4. get the predictions and we convert the predictions back to `tags`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQl5z4JlPfQ9"
      },
      "source": [
        "sent = \"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\"\n",
        "tags = 'DET NOUN NOUN ADJ NOUN VERB NOUN DET NOUN ADP NOUN ADJ NOUN NOUN VERB . DET NOUN . ADP DET NOUN VERB NOUN .'.split(\" \")"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsZuVmDNMY_I"
      },
      "source": [
        "def tokenize_and_pad_sequences(sent):\n",
        "\n",
        "  if isinstance(sent, str):\n",
        "    tokens = sent.split(\" \")\n",
        "  else:\n",
        "    tokens = sent\n",
        "  tokens = [t.lower() for t in tokens]\n",
        "  sequences = tokenizer.texts_to_sequences([tokens])\n",
        "  padded_sequnces = keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=MAX_LENGTH,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\"\n",
        "  )\n",
        "  predictions = rnn_model.predict(padded_sequnces)\n",
        "  predictions = tf.argmax(predictions, axis=-1).numpy().astype(\"int32\")\n",
        "\n",
        "  return  tag_tokenizer.sequences_to_texts(predictions)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8redvXF4QVJU",
        "outputId": "c5e57c1f-f8f5-413e-ef62-b7679b276d92"
      },
      "source": [
        "pred_tags = tokenize_and_pad_sequences(sent)\n",
        "\n",
        "print(\"word\\t\\t\\ttag\\tpred-tag\\t\")\n",
        "print(\"-\"*40)\n",
        "for word, tag, pred_tag in zip(sent.split(\" \"), tags, pred_tags[0].split(\" \")):\n",
        "  print(f\"{word}\\t\\t\\t{tag}\\t{pred_tag.upper()}\\t\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word\t\t\ttag\tpred-tag\t\n",
            "----------------------------------------\n",
            "The\t\t\tDET\tDET\t\n",
            "Fulton\t\t\tNOUN\tNOUN\t\n",
            "County\t\t\tNOUN\tNOUN\t\n",
            "Grand\t\t\tADJ\tADJ\t\n",
            "Jury\t\t\tNOUN\tNOUN\t\n",
            "said\t\t\tVERB\tVERB\t\n",
            "Friday\t\t\tNOUN\tNOUN\t\n",
            "an\t\t\tDET\tDET\t\n",
            "investigation\t\t\tNOUN\tNOUN\t\n",
            "of\t\t\tADP\tADP\t\n",
            "Atlanta's\t\t\tNOUN\tNOUN\t\n",
            "recent\t\t\tADJ\tADJ\t\n",
            "primary\t\t\tNOUN\tADJ\t\n",
            "election\t\t\tNOUN\tNOUN\t\n",
            "produced\t\t\tVERB\tVERB\t\n",
            "``\t\t\t.\t.\t\n",
            "no\t\t\tDET\tDET\t\n",
            "evidence\t\t\tNOUN\tNOUN\t\n",
            "''\t\t\t.\t.\t\n",
            "that\t\t\tADP\tADP\t\n",
            "any\t\t\tDET\tDET\t\n",
            "irregularities\t\t\tNOUN\tNOUN\t\n",
            "took\t\t\tVERB\tVERB\t\n",
            "place\t\t\tNOUN\tNOUN\t\n",
            ".\t\t\t.\t.\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2aDq_jHUG5s"
      },
      "source": [
        "### Conclusion\n",
        "Our goal was achived to load the `word2vec` vectors in our embeding layer of as LSTM RNN model. In the next one we are going to have a look at how we can perform the same task using `Bi-LSTM`. So basically the model achitecture will remain the same, we are only going to change from using LSTM to the use of `Bi-LSTM` (Bidirectional LSTM) with `word2vec` vectors in our embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ezL-ErJzEra"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}