{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_NER_BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVPjkBs-fC65"
      },
      "source": [
        "---\n",
        "Author:                 **`Crispen Gari`**\n",
        "\n",
        "Topic:                  **`\"Named Entity Recognition\" (NER)`**\n",
        " \n",
        "Main:                   **`Natural Language Processing NLP`**\n",
        "\n",
        "Library:                **`TensorFlow (2.x)`**\n",
        "\n",
        "Programing Language:    **`Python`**\n",
        "\n",
        "Date:                   **`2021-09-20`**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugq5EA9reSY7"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "In this series of notebooks we are going to go thought what is called `NER` (Named Entity Recoginition) using tensorflow 2. We are going to use the [conll2003](https://www.clips.uantwerpen.be/conll2003/ner/) the english version in this notebook as our dataset. I've downloaded the data and uploaded it on my google drive so that it can be loaded here on google colab easily.\n",
        "\n",
        "\n",
        "### Mounting the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4WPwL73eQlf",
        "outputId": "f91a235a-e806-4cc1-fab6-5961a6395921"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqGm4SrJgcKT"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlWkokVagelD"
      },
      "source": [
        "import os, time, io\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import Counter\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X--9U85MgKQF"
      },
      "source": [
        "### Path to files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrTtq0-5eR6U",
        "outputId": "4132f024-399b-4af3-815b-6b0394fc83af"
      },
      "source": [
        "root = '/content/drive/My Drive/NLP Data/ner-CoNLL-2003'\n",
        "os.path.exists(root)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3spBmdlgvY8"
      },
      "source": [
        "### File structures\n",
        "We have three files in the `ner-CoNLL-2003` folder which are:\n",
        "\n",
        "1. train.txt\n",
        "2. valid.txt\n",
        "3. test.txt\n",
        "\n",
        "Eaxh file contains data of the following nature:\n",
        "\n",
        "```txt\n",
        "-DOCSTART- -X- -X- O\n",
        "\n",
        "CRICKET NNP B-NP O\n",
        "- : O O\n",
        "LEICESTERSHIRE NNP B-NP B-ORG\n",
        "TAKE NNP I-NP O\n",
        "OVER IN B-PP O\n",
        "AT NNP B-NP O\n",
        "TOP NNP I-NP O\n",
        "AFTER NNP I-NP O\n",
        "INNINGS NNP I-NP O\n",
        "VICTORY NN I-NP O\n",
        ". . O O\n",
        "\n",
        "```\n",
        "\n",
        "### Data preprocessing\n",
        "We are going to extract the words with their named entities into an array for example:\n",
        "\n",
        "```\n",
        "[\n",
        "  ['EU', 'BB-ORG'], ['TOP', 'O']...\n",
        "]\n",
        "```\n",
        "The following code helps us to create a single function that we can reuse for all our sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTApU5OneR3R"
      },
      "source": [
        "def split_text_label(filename):\n",
        "  f = open(os.path.join(root, filename))\n",
        "  split_labeled_text = []\n",
        "  sentence = []\n",
        "  for line in f:\n",
        "    if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "       if len(sentence) > 0:\n",
        "         split_labeled_text.append(sentence)\n",
        "         sentence = []\n",
        "       continue\n",
        "    splits = line.split(' ')\n",
        "    sentence.append([splits[0],splits[-1].rstrip(\"\\n\")])\n",
        "  if len(sentence) > 0:\n",
        "    split_labeled_text.append(sentence)\n",
        "    sentence = []\n",
        "  return split_labeled_text\n",
        "train = split_text_label( \"train.txt\")\n",
        "valid = split_text_label( \"valid.txt\")\n",
        "test = split_text_label( \"test.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33gdxxrleR0K",
        "outputId": "c1d43035-d66c-4f86-df04-d62d301071a7"
      },
      "source": [
        "valid[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['CRICKET', 'O'],\n",
              "  ['-', 'O'],\n",
              "  ['LEICESTERSHIRE', 'B-ORG'],\n",
              "  ['TAKE', 'O'],\n",
              "  ['OVER', 'O'],\n",
              "  ['AT', 'O'],\n",
              "  ['TOP', 'O'],\n",
              "  ['AFTER', 'O'],\n",
              "  ['INNINGS', 'O'],\n",
              "  ['VICTORY', 'O'],\n",
              "  ['.', 'O']],\n",
              " [['LONDON', 'B-LOC'], ['1996-08-30', 'O']],\n",
              " [['West', 'B-MISC'],\n",
              "  ['Indian', 'I-MISC'],\n",
              "  ['all-rounder', 'O'],\n",
              "  ['Phil', 'B-PER'],\n",
              "  ['Simmons', 'I-PER'],\n",
              "  ['took', 'O'],\n",
              "  ['four', 'O'],\n",
              "  ['for', 'O'],\n",
              "  ['38', 'O'],\n",
              "  ['on', 'O'],\n",
              "  ['Friday', 'O'],\n",
              "  ['as', 'O'],\n",
              "  ['Leicestershire', 'B-ORG'],\n",
              "  ['beat', 'O'],\n",
              "  ['Somerset', 'B-ORG'],\n",
              "  ['by', 'O'],\n",
              "  ['an', 'O'],\n",
              "  ['innings', 'O'],\n",
              "  ['and', 'O'],\n",
              "  ['39', 'O'],\n",
              "  ['runs', 'O'],\n",
              "  ['in', 'O'],\n",
              "  ['two', 'O'],\n",
              "  ['days', 'O'],\n",
              "  ['to', 'O'],\n",
              "  ['take', 'O'],\n",
              "  ['over', 'O'],\n",
              "  ['at', 'O'],\n",
              "  ['the', 'O'],\n",
              "  ['head', 'O'],\n",
              "  ['of', 'O'],\n",
              "  ['the', 'O'],\n",
              "  ['county', 'O'],\n",
              "  ['championship', 'O'],\n",
              "  ['.', 'O']],\n",
              " [['Their', 'O'],\n",
              "  ['stay', 'O'],\n",
              "  ['on', 'O'],\n",
              "  ['top', 'O'],\n",
              "  [',', 'O'],\n",
              "  ['though', 'O'],\n",
              "  [',', 'O'],\n",
              "  ['may', 'O'],\n",
              "  ['be', 'O'],\n",
              "  ['short-lived', 'O'],\n",
              "  ['as', 'O'],\n",
              "  ['title', 'O'],\n",
              "  ['rivals', 'O'],\n",
              "  ['Essex', 'B-ORG'],\n",
              "  [',', 'O'],\n",
              "  ['Derbyshire', 'B-ORG'],\n",
              "  ['and', 'O'],\n",
              "  ['Surrey', 'B-ORG'],\n",
              "  ['all', 'O'],\n",
              "  ['closed', 'O'],\n",
              "  ['in', 'O'],\n",
              "  ['on', 'O'],\n",
              "  ['victory', 'O'],\n",
              "  ['while', 'O'],\n",
              "  ['Kent', 'B-ORG'],\n",
              "  ['made', 'O'],\n",
              "  ['up', 'O'],\n",
              "  ['for', 'O'],\n",
              "  ['lost', 'O'],\n",
              "  ['time', 'O'],\n",
              "  ['in', 'O'],\n",
              "  ['their', 'O'],\n",
              "  ['rain-affected', 'O'],\n",
              "  ['match', 'O'],\n",
              "  ['against', 'O'],\n",
              "  ['Nottinghamshire', 'B-ORG'],\n",
              "  ['.', 'O']],\n",
              " [['After', 'O'],\n",
              "  ['bowling', 'O'],\n",
              "  ['Somerset', 'B-ORG'],\n",
              "  ['out', 'O'],\n",
              "  ['for', 'O'],\n",
              "  ['83', 'O'],\n",
              "  ['on', 'O'],\n",
              "  ['the', 'O'],\n",
              "  ['opening', 'O'],\n",
              "  ['morning', 'O'],\n",
              "  ['at', 'O'],\n",
              "  ['Grace', 'B-LOC'],\n",
              "  ['Road', 'I-LOC'],\n",
              "  [',', 'O'],\n",
              "  ['Leicestershire', 'B-ORG'],\n",
              "  ['extended', 'O'],\n",
              "  ['their', 'O'],\n",
              "  ['first', 'O'],\n",
              "  ['innings', 'O'],\n",
              "  ['by', 'O'],\n",
              "  ['94', 'O'],\n",
              "  ['runs', 'O'],\n",
              "  ['before', 'O'],\n",
              "  ['being', 'O'],\n",
              "  ['bowled', 'O'],\n",
              "  ['out', 'O'],\n",
              "  ['for', 'O'],\n",
              "  ['296', 'O'],\n",
              "  ['with', 'O'],\n",
              "  ['England', 'B-LOC'],\n",
              "  ['discard', 'O'],\n",
              "  ['Andy', 'B-PER'],\n",
              "  ['Caddick', 'I-PER'],\n",
              "  ['taking', 'O'],\n",
              "  ['three', 'O'],\n",
              "  ['for', 'O'],\n",
              "  ['83', 'O'],\n",
              "  ['.', 'O']]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AuIoU54nVJ9"
      },
      "source": [
        "### Building the vocabulary\n",
        "\n",
        "Next we are going to build the vocabulary for all unique words and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQeHUtCheRxL"
      },
      "source": [
        "labelSet = set()\n",
        "wordSet = set()\n",
        "# words and labels\n",
        "for data in [train, valid, test]:\n",
        "  for labeled_text in data:\n",
        "    for word, label in labeled_text:\n",
        "      labelSet.add(label)\n",
        "      wordSet.add(word.lower())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aygo3Opqvm-"
      },
      "source": [
        "We are going to create a word index maping. We are going to start with our entity labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_G_YtBoeRih"
      },
      "source": [
        "\n",
        "label2Idx = {}\n",
        "for label in sorted(list(labelSet), key=len):\n",
        "  label2Idx[label] = len(label2Idx)\n",
        "idx2Label = {v: k for k, v in label2Idx.items()}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9soXsCNo6Lgw",
        "outputId": "dbec37dd-67df-4088-8d78-00c8f71916a6"
      },
      "source": [
        "len(idx2Label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRfbMcBkrmAE"
      },
      "source": [
        "Word index mapping for the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu93mtc3eRfp",
        "outputId": "825a6643-b73b-42e9-9b70-ce37ffbbe3c0"
      },
      "source": [
        "word2Idx = {}\n",
        "word2Idx[\"<pad>\"] = 0 # padding token\n",
        "word2Idx[\"<unk>\"] = 1 # unknown token \n",
        "\n",
        "for word in wordSet:\n",
        "  word2Idx[word] = len(word2Idx)\n",
        "\n",
        "idx2Word = {v:k for k, v in word2Idx.items()}\n",
        "print(len(idx2Word))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vW6GQJLeRZ6"
      },
      "source": [
        "def createMatrices(data, word2Idx, label2Idx):\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  for split_labeled_text in data:\n",
        "     wordIndices = []\n",
        "     labelIndices = []\n",
        "     for word, label in split_labeled_text:\n",
        "       if word in word2Idx:\n",
        "          wordIdx = word2Idx[word]\n",
        "       elif word.lower() in word2Idx:\n",
        "          wordIdx = word2Idx[word.lower()]\n",
        "       else:\n",
        "          wordIdx = word2Idx['<unk>']\n",
        "       wordIndices.append(wordIdx)\n",
        "       labelIndices.append(label2Idx[label])\n",
        "     sentences.append(wordIndices)\n",
        "     labels.append(labelIndices)\n",
        "  return sentences, labels\n",
        "\n",
        "train_sents, train_labels = createMatrices(\n",
        "    train, word2Idx, label2Idx\n",
        ")\n",
        "valid_sents, valid_labels = createMatrices(\n",
        "    valid, word2Idx, label2Idx\n",
        ")\n",
        "test_sents, test_labels = createMatrices(\n",
        "    test, word2Idx, label2Idx\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hrn8RifzteM"
      },
      "source": [
        "#### Padding the sequences\n",
        "\n",
        "We are going to pad our sequences to have the same length of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4svjTq2Push_"
      },
      "source": [
        "def padding(sents, labels, max_len=100):\n",
        "  padded_sentences = keras.preprocessing.sequence.pad_sequences(sents, max_len,       \n",
        "  padding='post', truncating=\"post\")\n",
        "  padded_labels = keras.preprocessing.sequence.pad_sequences(labels, max_len,       \n",
        "  padding='post', truncating=\"post\")\n",
        "  return padded_sentences, padded_labels\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4SlsdgW0bta"
      },
      "source": [
        "train_features, train_labels = padding(train_sents, train_labels)\n",
        "valid_features, valid_labels = padding(valid_sents, valid_labels)\n",
        "test_features, test_labels = padding(test_sents, test_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlNeIY-803p4"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "We are going to load our word embedding, from a local file. Basically we are going to use the `glove.6B.100d` word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxk-KySU0p_5",
        "outputId": "acbbb9f9-e360-4434-d90d-3d44db9852e8"
      },
      "source": [
        "embedding_path = \"/content/drive/MyDrive/NLP Data/glove.6B/glove.6B.100d.txt\"\n",
        "os.path.exists(embedding_path)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGhtDvQ81V3a",
        "outputId": "0cefc8fe-19ac-40ae-fc90-0b344b9078a9"
      },
      "source": [
        "embedding_dict = dict()\n",
        "with open(embedding_path, encoding=\"utf8\") as glove:\n",
        "  for line in glove:\n",
        "    records = line.split();\n",
        "    word = records[0]\n",
        "    vectors = np.asarray(records[1: ], dtype=np.float32)\n",
        "    embedding_dict[word] = vectors\n",
        "\n",
        "print(len(embedding_dict))\n",
        "embedding_dict[\"what\"].shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLuOoi71b4_"
      },
      "source": [
        "### Embedding matrix\n",
        "We are then going to create an embedding matrix thate suits our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_oO0wxU1yGt"
      },
      "source": [
        "vocab_size = len(word2Idx)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUfOlN01Z3x"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "for word, index in word2Idx.items():\n",
        "  vector = embedding_dict.get(word)\n",
        "  if vector is not None:\n",
        "    embedding_matrix[index] = vector"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jDvv11C2M69"
      },
      "source": [
        "### Input Pipeline\n",
        "\n",
        "We are going to make use of the `tf.data.Dataset.from_tensor_slices` to create a dataset from tensor slices so that we will be able to batch and shuffle it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ZznfL92IXl"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = train_features.shape[0]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_features, train_labels)\n",
        ").shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE, \n",
        "                                                             drop_remainder=True)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (valid_features, valid_labels)\n",
        ").batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_features, test_labels)\n",
        ").batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHqtB6g33WBt"
      },
      "source": [
        "### Model\n",
        "We are going to build a Bi-Directional Long-Short Term Memory, aka BiLSTM model. We are going to use the keras subclassing API, feel free to use the sequential API it will work as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opqQ1IiA3PKQ"
      },
      "source": [
        "class NER(keras.Model):\n",
        "  def __init__(self, max_seq_len,\n",
        "               embedding_dim,\n",
        "               output_dim, weights):\n",
        "    super(NER, self).__init__()\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(\n",
        "        embedding_dim, 100,\n",
        "        weights = [weights],\n",
        "        trainable=True,\n",
        "        input_length=max_seq_len\n",
        "    )\n",
        "    self.bilstm = keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(128, \n",
        "                          dropout=.5,\n",
        "                          return_sequences=True\n",
        "        )\n",
        "    )\n",
        "    self.dropout = keras.layers.Dropout(rate=.5)\n",
        "    self.out = keras.layers.Dense(output_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dropout(self.embedding(x))\n",
        "    x = self.bilstm(x)\n",
        "    return self.dropout(self.out(x))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4Dogy_5XcE"
      },
      "source": [
        "model = NER(\n",
        "    max_seq_len= 100,\n",
        "    embedding_dim=len(word2Idx),\n",
        "    weights = embedding_matrix,\n",
        "    output_dim= len(idx2Label)\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg_1Q_RX7hCK"
      },
      "source": [
        "Now the model can be trained by calling the `.fit()` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfJJiN2M6rYH",
        "outputId": "6f791d55-d1ad-42c4-ef1b-59a4a62553b6"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True\n",
        "    ),\n",
        "     metrics = [\"acc\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_dataset, epochs = 10, validation_data = valid_dataset\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "219/219 [==============================] - 18s 66ms/step - loss: 0.8984 - acc: 0.9705 - val_loss: 0.0676 - val_acc: 0.9810\n",
            "Epoch 2/10\n",
            "219/219 [==============================] - 11s 52ms/step - loss: 0.8256 - acc: 0.9792 - val_loss: 0.0472 - val_acc: 0.9876\n",
            "Epoch 3/10\n",
            "219/219 [==============================] - 13s 60ms/step - loss: 0.8182 - acc: 0.9807 - val_loss: 0.0399 - val_acc: 0.9887\n",
            "Epoch 4/10\n",
            "219/219 [==============================] - 11s 52ms/step - loss: 0.8122 - acc: 0.9820 - val_loss: 0.0337 - val_acc: 0.9912\n",
            "Epoch 5/10\n",
            "219/219 [==============================] - 11s 52ms/step - loss: 0.8092 - acc: 0.9828 - val_loss: 0.0295 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "219/219 [==============================] - 11s 52ms/step - loss: 0.8074 - acc: 0.9835 - val_loss: 0.0277 - val_acc: 0.9926\n",
            "Epoch 7/10\n",
            "219/219 [==============================] - 12s 55ms/step - loss: 0.8030 - acc: 0.9839 - val_loss: 0.0245 - val_acc: 0.9936\n",
            "Epoch 8/10\n",
            "219/219 [==============================] - 13s 60ms/step - loss: 0.8022 - acc: 0.9843 - val_loss: 0.0233 - val_acc: 0.9939\n",
            "Epoch 9/10\n",
            "219/219 [==============================] - 12s 55ms/step - loss: 0.8009 - acc: 0.9846 - val_loss: 0.0222 - val_acc: 0.9942\n",
            "Epoch 10/10\n",
            "219/219 [==============================] - 11s 51ms/step - loss: 0.8007 - acc: 0.9849 - val_loss: 0.0212 - val_acc: 0.9945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fad003dfd50>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJWQOgkqAEZe"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0nnqXNq8DrF",
        "outputId": "fb3f37f9-b685-4445-ded2-2e2d515b59db"
      },
      "source": [
        "model.evaluate(test_dataset, verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53/53 [==============================] - 1s 13ms/step - loss: 0.0205 - acc: 0.9942\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02051875926554203, 0.9941627383232117]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAWftKIkAUbf"
      },
      "source": [
        "### Model inference / Making predictions\n",
        "\n",
        "Now our model is ready to make predictions. we are going to create a function called `predict_entities` which will make predictions for each word in a sentence and returns it's entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnwO5q8eByq3",
        "outputId": "de8a78fb-2090-4b7f-824d-c182f993157b"
      },
      "source": [
        "sent = [w for w in [t for t, i in train[0]]]\n",
        "labels = [w for w in [i for t, i in train[0]]]\n",
        "sent, labels"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
              " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fHXfuY8Dn2",
        "outputId": "bdb9759d-3952-48c0-cd0b-c1584c9e52b7"
      },
      "source": [
        "def pedict_entities(sent):\n",
        "  tokenized = sent.lower().split(\" \")\n",
        "  tokens = [word2Idx[token] for token in tokenized]\n",
        "  tokens_padded = keras.preprocessing.sequence.pad_sequences([tokens], 100,       \n",
        "  padding='post', truncating=\"post\")\n",
        "\n",
        "  predictions = model(tokens_padded)\n",
        "  predictions= tf.squeeze(tf.argmax(predictions, axis=-1))[:len(tokens)].numpy()\n",
        "  predicted_labels = [idx2Label[i] for i in predictions]\n",
        "  return tokenized, predicted_labels\n",
        "\n",
        "pedict_entities(\" \".join(sent))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
              " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3K0UEYuCR5U"
      },
      "source": [
        "sent = [w for w in [t for t, i in train[0]]]\n",
        "labels = [w for w in [i for t, i in train[0]]]\n",
        "pedict_entities(\" \".join(sent))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Ouc75bFD7W"
      },
      "source": [
        "### Making more predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXEfBLPAFJhz",
        "outputId": "f31fd832-50e1-4e62-e332-0cb806c3e6af"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"*\"*50)\n",
        "  sent = [w for w in [t for t, i in test[i]]]\n",
        "  labels = [w for w in [i for t, i in test[i]]] \n",
        "  tokenized, preds = pedict_entities(\" \".join(sent))\n",
        "  print(\"sentence: \", sent)\n",
        "  print(\"actual labels: \", labels)\n",
        "  print(\"predicted labels: \", preds)\n",
        "  print(\"*\"*50)\n",
        "  print()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "sentence:  ['\\t-DOCSTART-']\n",
            "actual labels:  ['O']\n",
            "predicted labels:  ['O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
            "actual labels:  ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['Nadim', 'Ladki']\n",
            "actual labels:  ['B-PER', 'I-PER']\n",
            "predicted labels:  ['B-PER', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06']\n",
            "actual labels:  ['B-LOC', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\n",
            "predicted labels:  ['B-LOC', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['Japan', 'began', 'the', 'defence', 'of', 'their', 'Asian', 'Cup', 'title', 'with', 'a', 'lucky', '2-1', 'win', 'against', 'Syria', 'in', 'a', 'Group', 'C', 'championship', 'match', 'on', 'Friday', '.']\n",
            "actual labels:  ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['But', 'China', 'saw', 'their', 'luck', 'desert', 'them', 'in', 'the', 'second', 'match', 'of', 'the', 'group', ',', 'crashing', 'to', 'a', 'surprise', '2-0', 'defeat', 'to', 'newcomers', 'Uzbekistan', '.']\n",
            "actual labels:  ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
            "predicted labels:  ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['China', 'controlled', 'most', 'of', 'the', 'match', 'and', 'saw', 'several', 'chances', 'missed', 'until', 'the', '78th', 'minute', 'when', 'Uzbek', 'striker', 'Igor', 'Shkvyrin', 'took', 'advantage', 'of', 'a', 'misdirected', 'defensive', 'header', 'to', 'lob', 'the', 'ball', 'over', 'the', 'advancing', 'Chinese', 'keeper', 'and', 'into', 'an', 'empty', 'net', '.']\n",
            "actual labels:  ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['Oleg', 'Shatskiku', 'made', 'sure', 'of', 'the', 'win', 'in', 'injury', 'time', ',', 'hitting', 'an', 'unstoppable', 'left', 'foot', 'shot', 'from', 'just', 'outside', 'the', 'area', '.']\n",
            "actual labels:  ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['The', 'former', 'Soviet', 'republic', 'was', 'playing', 'in', 'an', 'Asian', 'Cup', 'finals', 'tie', 'for', 'the', 'first', 'time', '.']\n",
            "actual labels:  ['O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['O', 'O', 'B-MISC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n",
            "**************************************************\n",
            "sentence:  ['Despite', 'winning', 'the', 'Asian', 'Games', 'title', 'two', 'years', 'ago', ',', 'Uzbekistan', 'are', 'in', 'the', 'finals', 'as', 'outsiders', '.']\n",
            "actual labels:  ['O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "**************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGb5mgHIF2la"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL4TIdKgFyDm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}