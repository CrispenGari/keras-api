{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_NER_Using_Transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00db2c82b75944e0b81b3e1c2b9ca842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee4bb9d758064ece8d723cfeac50e9a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2a8537e8459043c193fd198f5684abe5",
              "IPY_MODEL_31d1c438b8f14d82b9bc73c466189674",
              "IPY_MODEL_999fc35a69834e8db74f7edb4f3b48fa"
            ]
          }
        },
        "ee4bb9d758064ece8d723cfeac50e9a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a8537e8459043c193fd198f5684abe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e3012102f9c464b9b803fdd3b9a24df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01b6774eeb694baeab8f1704bd11aa4b"
          }
        },
        "31d1c438b8f14d82b9bc73c466189674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adedd9f70c884be4a02ca840fb6e911c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfd150350cb5412799e39ffbfd673907"
          }
        },
        "999fc35a69834e8db74f7edb4f3b48fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0b3340309bc43cb993622c730a428c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:00&lt;00:00, 40.72it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcbb7950e60847b6bee57c46706e347c"
          }
        },
        "1e3012102f9c464b9b803fdd3b9a24df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01b6774eeb694baeab8f1704bd11aa4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adedd9f70c884be4a02ca840fb6e911c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfd150350cb5412799e39ffbfd673907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0b3340309bc43cb993622c730a428c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcbb7950e60847b6bee57c46706e347c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2gcDjxJGGO9"
      },
      "source": [
        "---\n",
        "Author:                 **`Crispen Gari`**\n",
        "\n",
        "Topic:                  **`\"Named Entity Recognition\" (NER)`**\n",
        " \n",
        "Main:                   **`Natural Language Processing NLP`**\n",
        "\n",
        "Library:                **`TensorFlow (2.x)`**\n",
        "\n",
        "Programing Language:    **`Python`**\n",
        "\n",
        "Date:                   **`2021-09-20`**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhnAteBYGJCT"
      },
      "source": [
        "### NER (Named Entity Recognition) using transformers.\n",
        "\n",
        "In this notebook we are going to do NER using transformers. Basically we will be following [this](https://keras.io/examples/nlp/ner_transformers/) amazing tensorflow tutorial from keras examples.\n",
        "\n",
        "### Definition\n",
        "\n",
        "Named Entity Recognition (NER) is the process of identifying named entities in text. Example of named entities are: \"Person\", \"Location\", \"Organization\", \"Dates\" etc. NER is essentially a token classification task where every token is classified into one or more predetermined categories.\n",
        "\n",
        "The dataset that we will be working with is `HuggingFace` dataset so to download it we will run the following command.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wya4CulAF_vn",
        "outputId": "4968c622-aa8c-4666-b89d-e8df65147a67"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.8.1 huggingface-hub-0.0.17 multidict-5.1.0 xxhash-2.0.2 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igcmyOXhHCef"
      },
      "source": [
        "We aslso need the script that will evaluate our NER models, so to download it we will run the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdp0IdjmG4tl"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyLBFOCWHP2v"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4IjYYf7HOC9"
      },
      "source": [
        "import os, time\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from conlleval import evaluate\n",
        "from datasets import load_dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AAMsQPCHnk4"
      },
      "source": [
        "### Building a TransformerBlock layer\n",
        "\n",
        "We are going to then build the Transformer block layer using keras SubClassing API in the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7807e0f_Hj-0"
      },
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "  def __init__(self, embedding_dim, num_heads, ff_dim, rate=.1):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "\n",
        "    self.att = keras.layers.MultiHeadAttention(\n",
        "        num_heads, key_dim = embedding_dim\n",
        "    )\n",
        "    self.ffn = keras.Sequential([\n",
        "        keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        keras.layers.Dense(embedding_dim)\n",
        "    ])\n",
        "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = keras.layers.Dropout(rate)\n",
        "    self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    attn_output = self.att(inputs, inputs)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFyVmbTaItiP"
      },
      "source": [
        "### Building the `TokenAndPositionEmbedding`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Y0x438ItDr"
      },
      "source": [
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "    super(TokenAndPositionEmbedding, self).__init__()\n",
        "\n",
        "    self.token_emb = keras.layers.Embedding(\n",
        "        input_dim=vocab_size, output_dim=embed_dim\n",
        "    )\n",
        "    self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    maxlen = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "    position_embeddings = self.pos_emb(positions)\n",
        "    token_embeddings = self.token_emb(inputs)\n",
        "    return token_embeddings + position_embeddings\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fgxZhcXJBNn"
      },
      "source": [
        "### Building the `NER` model using the subclassing API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg3mgFS4I-nK"
      },
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "      super(NERModel, self).__init__()\n",
        "      self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "      self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "      self.dropout1 = keras.layers.Dropout(0.1)\n",
        "      self.ff = keras.layers.Dense(ff_dim, activation=\"relu\")\n",
        "      self.dropout2 = keras.layers.Dropout(0.1)\n",
        "      self.ff_final = keras.layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "      x = self.embedding_layer(inputs)\n",
        "      x = self.transformer_block(x)\n",
        "      x = self.dropout1(x, training=training)\n",
        "      x = self.ff(x)\n",
        "      x = self.dropout2(x, training=training)\n",
        "      x = self.ff_final(x)\n",
        "      return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHeDHL6mJJag"
      },
      "source": [
        "### Loading the `CoNLL 2003` dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "00db2c82b75944e0b81b3e1c2b9ca842",
            "ee4bb9d758064ece8d723cfeac50e9a7",
            "2a8537e8459043c193fd198f5684abe5",
            "31d1c438b8f14d82b9bc73c466189674",
            "999fc35a69834e8db74f7edb4f3b48fa",
            "1e3012102f9c464b9b803fdd3b9a24df",
            "01b6774eeb694baeab8f1704bd11aa4b",
            "adedd9f70c884be4a02ca840fb6e911c",
            "cfd150350cb5412799e39ffbfd673907",
            "e0b3340309bc43cb993622c730a428c2",
            "bcbb7950e60847b6bee57c46706e347c"
          ]
        },
        "id": "Sur7nuNGJH-i",
        "outputId": "2391a9e8-08c4-4c28-ef45-901eec3788af"
      },
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00db2c82b75944e0b81b3e1c2b9ca842",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viFrLy3rJbbR"
      },
      "source": [
        "We will export this data to a tab-separated file format which will be easy to read as a `tf.data.Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k68lm9TJOzT"
      },
      "source": [
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            f.write(\n",
        "                str(len(tokens))\n",
        "                + \"\\t\"\n",
        "                + \"\\t\".join(tokens)\n",
        "                + \"\\t\"\n",
        "                + \"\\t\".join(map(str, ner_tags))\n",
        "                + \"\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "if os.path.exists(\"data\") == False:\n",
        "  os.mkdir(\"data\")\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pJdA-ecJiQP"
      },
      "source": [
        "### Make the NER label lookup table\n",
        "\n",
        "NER labels are usually provided in IOB, IOB2 or IOBES formats. \n",
        "\n",
        "\n",
        "Note that we start our label numbering from 1 since 0 will be reserved for padding. We have a total of 10 labels: 9 from the NER dataset and one for padding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fggll9pJfaG",
        "outputId": "a4a2af34-1b2d-4411-daa9-5b22e64cc5db"
      },
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAHy2eqRJvbw"
      },
      "source": [
        "Get a list of all tokens in the training dataset. This will be used to create the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOkTLLO0JtQW",
        "outputId": "b4622158-008f-41dc-c1a9-e8efe1560b84"
      },
      "source": [
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "\n",
        "# We only take (vocab_size - 2) most commons words from the training data since\n",
        "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
        "# token and another one denoting a masking token\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "# The StringLook class will convert tokens to token IDs\n",
        "lookup_layer = keras.layers.StringLookup(\n",
        "    vocabulary=vocabulary\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW_kfbRPJ0tx"
      },
      "source": [
        "Create 2 new Dataset objects from the training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMg10j8zJw--"
      },
      "source": [
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdoAQRhlJ3bp",
        "outputId": "ea30e133-95a5-4f41-debe-bb59a14fdd92"
      },
      "source": [
        "print(list(train_data.take(1).as_numpy_iterator()))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N7THkMRJ60o"
      },
      "source": [
        "We will be using the following map function to transform the data in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqYqpz0iJ5B4"
      },
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "# We use `padded_batch` here because each record in the dataset has a\n",
        "# different length.\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h72_sIuiJ8e3"
      },
      "source": [
        "We will be using a custom loss function that will ignore the loss from padded tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWrEXIXaKQ-3"
      },
      "source": [
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfENVuCGKQ4Q",
        "outputId": "675a5d7b-cbee-4d0c-e358-fd06ffc833d0"
      },
      "source": [
        "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
        "ner_model.fit(train_dataset, epochs=10)\n",
        "\n",
        "\n",
        "def tokenize_and_convert_to_ids(text):\n",
        "    tokens = text.split()\n",
        "    return lowercase_and_convert_to_ids(tokens)\n",
        "\n",
        "\n",
        "# Sample inference using the trained model\n",
        "sample_input = tokenize_and_convert_to_ids(\n",
        "    \"eu rejects german call to boycott british lamb\"\n",
        ")\n",
        "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
        "print(sample_input)\n",
        "\n",
        "output = ner_model.predict(sample_input)\n",
        "prediction = np.argmax(output, axis=-1)[0]\n",
        "prediction = [mapping[i] for i in prediction]\n",
        "\n",
        "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
        "print(prediction)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "439/439 [==============================] - 13s 19ms/step - loss: 0.6409\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 8s 19ms/step - loss: 0.2478\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 8s 19ms/step - loss: 0.1531\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 9s 19ms/step - loss: 0.1202\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 8s 19ms/step - loss: 0.0959\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 8s 19ms/step - loss: 0.0782\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 8s 19ms/step - loss: 0.0654\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 9s 19ms/step - loss: 0.0560\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 9s 20ms/step - loss: 0.0496\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 9s 19ms/step - loss: 0.0436\n",
            "tf.Tensor([[  988 10950   204   628     6  3938   215  5773]], shape=(1, 8), dtype=int64)\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujGVZHpiLBCf"
      },
      "source": [
        "### Metrics calculation\n",
        "Here is a function to calculate the metrics. The function calculates F1 score for the overall NER dataset as well as individual scores for each NER tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KhvDwIIKQ03",
        "outputId": "4e0da556-6b0f-4d10-a774-9b4f2f01445d"
      },
      "source": [
        "def calculate_metrics(dataset):\n",
        "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
        "\n",
        "    for x, y in dataset:\n",
        "        output = ner_model.predict(x)\n",
        "        predictions = np.argmax(output, axis=-1)\n",
        "        predictions = np.reshape(predictions, [-1])\n",
        "\n",
        "        true_tag_ids = np.reshape(y, [-1])\n",
        "\n",
        "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
        "        true_tag_ids = true_tag_ids[mask]\n",
        "        predicted_tag_ids = predictions[mask]\n",
        "\n",
        "        all_true_tag_ids.append(true_tag_ids)\n",
        "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
        "\n",
        "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
        "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
        "\n",
        "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
        "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
        "\n",
        "    evaluate(real_tags, predicted_tags)\n",
        "\n",
        "\n",
        "calculate_metrics(val_dataset)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51362 tokens with 5942 phrases; found: 5594 phrases; correct: 4015.\n",
            "accuracy:  65.16%; (non-O)\n",
            "accuracy:  93.42%; precision:  71.77%; recall:  67.57%; FB1:  69.61\n",
            "              LOC: precision:  84.35%; recall:  78.61%; FB1:  81.38  1712\n",
            "             MISC: precision:  72.65%; recall:  65.40%; FB1:  68.84  830\n",
            "              ORG: precision:  60.55%; recall:  63.53%; FB1:  62.01  1407\n",
            "              PER: precision:  67.84%; recall:  60.59%; FB1:  64.01  1645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PV3n_OiLHnJ"
      },
      "source": [
        "### Conclusions\n",
        "In this exercise, we created a simple transformer based named entity recognition model. We trained it on the CoNLL 2003 shared task data and got an overall F1 score of around 70%. State of the art NER models fine-tuned on pretrained models such as BERT or ELECTRA can easily get much higher F1 score -between 90-95% on this dataset owing to the inherent knowledge of words as part of the pretraining process and the usage of subword tokenization.\n",
        "\n",
        "\n",
        "### Copied from:\n",
        "\n",
        "https://keras.io/examples/nlp/ner_transformers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4usQwMNtLFEy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}