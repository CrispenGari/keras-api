{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Image_Captioning_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBeaoUKVQoCm"
      },
      "source": [
        "### Image Captioning with visual attention\n",
        "\n",
        "This notebook is based on the [tensorflow tutorial](https://www.tensorflow.org/tutorials/text/image_captioning).\n",
        "\n",
        "Given an image, we should be able to generate caption for an image based on what appears on that image.\n",
        "\n",
        "We are going to use attention based model which enables us to see what parts of the image the models focuses on as it generates the caption.\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We are going to use the [MS-COCO](http://cocodataset.org/#home) dataset. We are going to use a relatively small dataset with 30000 captions for 20000 images (this is beacuse a single image can have multiple captions).\n",
        "\n",
        "\n",
        "### Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C3Urrt8mQdAc",
        "outputId": "4e25f8f4-9468-483b-ceeb-29c6c1e4d165"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import collections, os, json, random, time\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m4SxJNASvhv"
      },
      "source": [
        "### Downloading and dataset preparation.\n",
        "\n",
        "We are going to use the [MS-COCO dataset](https://cocodataset.org/#home) which contains over 82000 images and each image has 5 different captions.\n",
        "\n",
        "\n",
        "### Annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNNFPkDyQc9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b1cd3f-bfd5-41bc-ba3a-84c78dc45f9a"
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                           extract=True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 4s 0us/step\n",
            "252887040/252872794 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5knNCo3fRVeG"
      },
      "source": [
        "### Image files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk4qI_NXQc5m",
        "outputId": "248ce2fb-3f02-46d3-8b6b-c20b7fddbdc0"
      },
      "source": [
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract=True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 288s 0us/step\n",
            "13510582272/13510573713 [==============================] - 288s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nriXjTmRTtIk"
      },
      "source": [
        "### Training size\n",
        "\n",
        "For the sake of demostration we are going to limit the training images together with their captions, to have 30000 captions and their corresponding images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkTzzBPeQc3k"
      },
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "  annotations = json.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIcQkeBZQczC"
      },
      "source": [
        "# Group all captions together having the same image ID.\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for val in annotations['annotations']:\n",
        "  caption = f\"<start> {val['caption']} <end>\"\n",
        "  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
        "  image_path_to_caption[image_path].append(caption)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvZwHzgcU58x"
      },
      "source": [
        "### Trimming the dataset\n",
        "\n",
        "We are goig to seledt 6000 images from the shuffled images names. Since each image have 5 captions this will result in the dataset with 30, 000 examples in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfP6VD40QcxH",
        "outputId": "9a68de5a-cea6-405d-c19e-72a58fbd2db2"
      },
      "source": [
        "image_paths = list(image_path_to_caption.keys())\n",
        "random.shuffle(image_paths)\n",
        "train_image_paths = image_paths[:6000]\n",
        "print(len(train_image_paths))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6xpc9R9VaSe"
      },
      "source": [
        "train_captions = []\n",
        "img_name_vector = []\n",
        "\n",
        "for image_path in train_image_paths:\n",
        "  caption_list = image_path_to_caption[image_path]\n",
        "  train_captions.extend(caption_list)\n",
        "  img_name_vector.extend([image_path] * len(caption_list))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOW_OsMoXEIF"
      },
      "source": [
        "### InceptionV3\n",
        "\n",
        "Next we are going to process the image so that we will be using InceptionV3 model to classify images. We will extract features from the last convolutional layer.\n",
        "\n",
        "We are going to so the following.\n",
        "\n",
        "* Resize the images to be (299 x 299)\n",
        "\n",
        "* Normalize the image pixcls so that they will have pixels between -1 and 1 which matches the format InceptionV3 was trained on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjq1L3PQcov"
      },
      "source": [
        "def load_image(image_path):\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.image.resize(image, (299, 299))\n",
        "  image = keras.applications.inception_v3.preprocess_input(image)\n",
        "  return image, image_path"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW1MN8MYYUKE"
      },
      "source": [
        "### Initializing the IceptionV3 pretrained imagenet weights.\n",
        "Now you'll create a `tf.keras` model where the output layer is the last convolutional layer in the InceptionV3 architecture. The shape of the output of this layer is ``8x8x2048``.\n",
        "\n",
        ">  We use the last convolutional layer because we are using attention in this example. We don't perform this initialization during training because it could become a bottleneck. We are going to so the following:\n",
        "\n",
        "1. forward the image into  the network and store the resulting vector in a dictionary:\n",
        "\n",
        "```json\n",
        "{\n",
        "  image_name: feature_vector\n",
        "}\n",
        "```\n",
        "\n",
        "After we have passed all these images to the network we are going to save all these images to the disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFnntqvCQcmq",
        "outputId": "9f78f360-3e8a-4b8a-b888-5bbd9198b36c"
      },
      "source": [
        "\n",
        "image_model = keras.applications.InceptionV3(include_top=False,\n",
        "                                             weights=\"imagenet\")\n",
        "new_input = image_model.input\n",
        "\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input,\n",
        "                                              hidden_layer, \n",
        "                                              name=\"image_features_extractor\")\n",
        "\n",
        "image_features_extract_model.summary()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n",
            "Model: \"image_features_extractor\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 3 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 3 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 3 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 3 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, None, 3 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 6 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 6 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, None, None, 6 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 8 5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 8 240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, None, None, 8 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 1 138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 6 192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, None, None, 6 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 4 9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 9 55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 4 144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 9 288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, None, None, 4 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, None, None, 9 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, None, None, 1 0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 6 12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 6 76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 9 82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 3 6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 6 192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 6 192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 9 288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 3 96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, None, None, 6 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, None, None, 9 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, None, None, 3 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, None, None, 2 0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, None, None, 6 192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, None, None, 6 0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 4 12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 9 55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, None, None, 4 144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, None, None, 9 288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, None, None, 4 0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, None, None, 9 0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, None, None, 2 0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 6 16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 6 76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, None, None, 9 82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, None, None, 6 16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, None, None, 6 192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, None, None, 6 192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, None, None, 9 288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, None, None, 6 192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, None, None, 6 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, None, None, 6 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, None, None, 9 0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, None, None, 6 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, None, None, 2 0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, None, None, 6 192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, None, None, 6 0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, None, None, 4 13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, None, None, 9 55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, None, None, 4 144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, None, None, 9 288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, None, None, 4 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, None, None, 9 0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, None, None, 2 0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, None, None, 6 18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, None, None, 6 76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, None, None, 9 82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, None, None, 6 18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, None, None, 6 192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, None, None, 6 192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, None, None, 9 288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, None, None, 6 192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, None, None, 6 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, None, None, 6 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, None, None, 9 0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, None, None, 6 0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, None, None, 2 0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, None, None, 6 18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, None, None, 6 192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, None, None, 6 0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, None, None, 9 55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, None, None, 9 288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, None, None, 9 0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, None, None, 3 995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, None, None, 9 82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, None, None, 3 1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, None, None, 9 288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, None, None, 3 0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, None, None, 9 0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, None, None, 7 0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, None, None, 1 384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, None, None, 1 0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, None, None, 1 114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, None, None, 1 384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, None, None, 1 0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, None, None, 1 98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, None, None, 1 114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, None, None, 1 384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, None, None, 1 384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, None, None, 1 0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, None, None, 1 0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, None, None, 1 114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, None, None, 1 114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, None, None, 1 384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, None, None, 1 384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, None, None, 1 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, None, None, 1 0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, None, None, 7 0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, None, None, 1 147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, None, None, 1 172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, None, None, 1 172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, None, None, 1 576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, None, None, 1 576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, None, None, 1 576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, None, None, 1 576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, None, None, 1 0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, None, None, 1 0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, None, None, 1 0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, None, None, 1 0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, None, None, 7 0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, None, None, 1 480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, None, None, 1 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, None, None, 1 179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, None, None, 1 480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, None, None, 1 122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, None, None, 1 179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, None, None, 1 480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, None, None, 1 480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, None, None, 1 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, None, None, 1 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, None, None, 1 179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, None, None, 1 179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, None, None, 1 480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, None, None, 1 480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, None, None, 1 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, None, None, 7 0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, None, None, 1 147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, None, None, 1 215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, None, None, 1 215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, None, None, 1 576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, None, None, 1 576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, None, None, 1 576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, None, None, 1 576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, None, None, 1 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, None, None, 1 0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, None, None, 7 0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, None, None, 1 480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, None, None, 1 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, None, None, 1 179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, None, None, 1 480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, None, None, 1 122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, None, None, 1 179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, None, None, 1 480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, None, None, 1 480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, None, None, 1 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, None, None, 1 179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, None, None, 1 179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, None, None, 1 480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, None, None, 1 480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, None, None, 1 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, None, None, 7 0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, None, None, 1 147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, None, None, 1 215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, None, None, 1 215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, None, None, 1 576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, None, None, 1 576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, None, None, 1 576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, None, None, 1 576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, None, None, 1 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, None, None, 1 0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, None, None, 1 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, None, None, 7 0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, None, None, 1 576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, None, None, 1 0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, None, None, 1 258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, None, None, 1 576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, None, None, 1 258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, None, None, 1 576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, None, None, 1 576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, None, None, 1 0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, None, None, 1 258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, None, None, 1 258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, None, None, 1 576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, None, None, 1 576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, None, None, 1 0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, None, None, 1 0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, None, None, 7 0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, None, None, 1 147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, None, None, 1 258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, None, None, 1 258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, None, None, 1 147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, None, None, 1 576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, None, None, 1 576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, None, None, 1 576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, None, None, 1 576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, None, None, 1 0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, None, None, 1 0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, None, None, 1 0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, None, None, 7 0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, None, None, 1 576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, None, None, 1 0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, None, None, 1 258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, None, None, 1 576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, None, None, 1 0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, None, None, 1 147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, None, None, 1 258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, None, None, 1 576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, None, None, 1 576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, None, None, 1 0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, None, None, 3 552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, None, None, 1 331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, None, None, 3 960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, None, None, 1 576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, None, None, 3 0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, None, None, 1 0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 7 0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, None, None, 1 0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, None, None, 4 573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, None, None, 4 1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, None, None, 4 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, None, None, 3 491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, None, None, 3 1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, None, None, 3 1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, None, None, 3 1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, None, None, 3 0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, None, None, 3 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, None, None, 3 442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, None, None, 3 442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, None, None, 1 0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, None, None, 3 409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, None, None, 3 1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, None, None, 3 1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, None, None, 3 1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, None, None, 3 1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, None, None, 1 245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, None, None, 3 960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, None, None, 3 0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, None, None, 3 0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, None, None, 3 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, None, None, 3 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, None, None, 1 576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, None, None, 3 0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, None, None, 7 0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, None, 7 0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, None, None, 1 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, None, None, 2 0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, None, None, 4 917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, None, None, 4 1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, None, None, 4 0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, None, None, 3 786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, None, None, 3 1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, None, None, 3 1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, None, None, 3 1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, None, None, 3 0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, None, None, 3 0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, None, None, 3 442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, None, None, 3 442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, None, None, 2 0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, None, None, 3 655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, None, None, 3 1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, None, None, 3 1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, None, None, 3 1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, None, None, 3 1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, None, None, 1 393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, None, None, 3 960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, None, None, 3 0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, None, None, 3 0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, None, None, 3 0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, None, None, 3 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, None, None, 1 576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, None, None, 3 0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, None, None, 7 0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 7 0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, None, None, 1 0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, None, None, 2 0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 21,802,784\n",
            "Trainable params: 21,768,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHGmPM1WaXYc"
      },
      "source": [
        "### Caching the features extracted from Inceptionv3\n",
        "\n",
        "> We will pre-process each image with InceptionV3 and cache the output to disk. Caching the output in RAM would be faster but also memory intensive, requiring 8 * 8 * 2048 floats per image. At the time of writing, this exceeds the memory limitations of Colab (currently 12GB of memory).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn06uIv3ukoe",
        "outputId": "5702676c-e92d-4ba3-9dd8-92e1300c2844"
      },
      "source": [
        "img_name_vector[:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/train2014/COCO_train2014_000000062622.jpg',\n",
              " '/content/train2014/COCO_train2014_000000062622.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po_9q87EQciw"
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBpeZYy4xSlz",
        "outputId": "de7df50e-9063-4c9c-d3aa-1fbdc967855f"
      },
      "source": [
        "encode_train[:2]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/train2014/COCO_train2014_000000000077.jpg',\n",
              " '/content/train2014/COCO_train2014_000000000349.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKM7E4V_QcgH"
      },
      "source": [
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i59shBcxd2M",
        "outputId": "91cb6105-c716-480b-8534-3db3228f421d"
      },
      "source": [
        "next(iter(image_dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(16, 299, 299, 3), dtype=float32, numpy=\n",
              " array([[[[ 4.26761746e-01,  7.27791309e-01,  8.96102786e-01],\n",
              "          [ 3.84517670e-01,  7.06151843e-01,  8.78700852e-01],\n",
              "          [ 3.96950006e-01,  7.26361752e-01,  9.09586906e-01],\n",
              "          ...,\n",
              "          [ 2.49266863e-01,  3.11170578e-01, -4.58851695e-01],\n",
              "          [-2.17814982e-01, -1.29857600e-01, -6.98305249e-01],\n",
              "          [ 5.02256870e-01,  4.88277912e-01, -1.35714412e-02]],\n",
              " \n",
              "         [[ 4.05906796e-01,  7.29485750e-01,  8.85343313e-01],\n",
              "          [ 4.14492726e-01,  7.39092350e-01,  9.11641359e-01],\n",
              "          [ 3.84650111e-01,  7.20167518e-01,  8.97287011e-01],\n",
              "          ...,\n",
              "          [-2.04325318e-02,  6.48479462e-02, -5.89793503e-01],\n",
              "          [-2.34727323e-01, -9.74420905e-02, -6.96066141e-01],\n",
              "          [-9.43085551e-02,  2.42102146e-02, -5.20460606e-01]],\n",
              " \n",
              "         [[ 4.10845399e-01,  7.44303823e-01,  8.95221710e-01],\n",
              "          [ 4.09544349e-01,  7.39081144e-01,  9.11588430e-01],\n",
              "          [ 3.86080980e-01,  7.36282468e-01,  9.01901603e-01],\n",
              "          ...,\n",
              "          [-3.87420058e-02,  1.45578384e-02, -6.15442395e-01],\n",
              "          [ 1.09292030e-01,  1.77165151e-01, -3.43602419e-01],\n",
              "          [-4.59465921e-01, -3.84779215e-01, -8.57550502e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 7.10418344e-01,  6.91134810e-01,  5.80166936e-01],\n",
              "          [ 7.74173260e-01,  7.32915878e-01,  6.38798237e-01],\n",
              "          [ 6.98773742e-01,  6.78077340e-01,  5.78621626e-01],\n",
              "          ...,\n",
              "          [ 7.24043965e-01,  7.08357692e-01,  6.14240050e-01],\n",
              "          [ 7.38210559e-01,  7.22524285e-01,  6.28406644e-01],\n",
              "          [ 7.35969305e-01,  7.16486692e-01,  6.16424084e-01]],\n",
              " \n",
              "         [[ 7.41746306e-01,  7.20787525e-01,  6.21462941e-01],\n",
              "          [ 7.25669026e-01,  6.94296479e-01,  6.00178838e-01],\n",
              "          [ 6.97295308e-01,  6.86507106e-01,  5.83748698e-01],\n",
              "          ...,\n",
              "          [ 7.35883117e-01,  7.20196843e-01,  6.26079202e-01],\n",
              "          [ 7.18790889e-01,  7.03104615e-01,  6.08986974e-01],\n",
              "          [ 7.27854729e-01,  6.98492408e-01,  6.03369713e-01]],\n",
              " \n",
              "         [[ 6.79878831e-01,  6.77157521e-01,  5.70986748e-01],\n",
              "          [ 6.95106149e-01,  6.63848162e-01,  5.69673300e-01],\n",
              "          [ 6.99455738e-01,  6.99146867e-01,  5.92166305e-01],\n",
              "          ...,\n",
              "          [ 7.43372083e-01,  7.23312020e-01,  6.29194379e-01],\n",
              "          [ 7.23561764e-01,  6.94182992e-01,  6.00065351e-01],\n",
              "          [ 7.34245777e-01,  7.02873230e-01,  6.08755589e-01]]],\n",
              " \n",
              " \n",
              "        [[[ 9.37254906e-01,  9.45098042e-01,  9.84313726e-01],\n",
              "          [ 9.38942075e-01,  9.43410873e-01,  9.84313726e-01],\n",
              "          [ 9.47118640e-01,  9.39275503e-01,  9.86334324e-01],\n",
              "          ...,\n",
              "          [ 7.09911227e-01,  7.25597501e-01,  8.43244553e-01],\n",
              "          [ 5.61100841e-01,  5.76787114e-01,  6.78747892e-01],\n",
              "          [ 2.01554060e-01,  2.17240334e-01,  3.25942159e-01]],\n",
              " \n",
              "         [[ 9.51498508e-01,  9.45098042e-01,  9.88374829e-01],\n",
              "          [ 9.52011228e-01,  9.44585323e-01,  9.91435409e-01],\n",
              "          [ 9.58895564e-01,  9.44990635e-01,  9.85987663e-01],\n",
              "          ...,\n",
              "          [ 6.88987255e-01,  7.04673529e-01,  8.22320580e-01],\n",
              "          [ 4.86194968e-01,  5.01881242e-01,  6.03842020e-01],\n",
              "          [ 1.93605542e-01,  2.09291816e-01,  3.17993760e-01]],\n",
              " \n",
              "         [[ 9.60994244e-01,  9.49124575e-01,  9.88340259e-01],\n",
              "          [ 9.60994244e-01,  9.49124575e-01,  9.91052747e-01],\n",
              "          [ 9.71486688e-01,  9.56368327e-01,  9.95584011e-01],\n",
              "          ...,\n",
              "          [ 6.33244634e-01,  6.48930907e-01,  7.66577959e-01],\n",
              "          [ 4.58940625e-01,  4.74626899e-01,  5.76587677e-01],\n",
              "          [ 2.00528622e-01,  2.16214895e-01,  3.24916840e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-2.56988287e-01, -3.35629344e-01, -3.70818615e-01],\n",
              "          [-2.61431217e-01, -3.40072274e-01, -3.75261545e-01],\n",
              "          [-2.69810498e-01, -3.48451555e-01, -3.83640826e-01],\n",
              "          ...,\n",
              "          [-3.70005190e-01, -4.24907148e-01, -5.03338516e-01],\n",
              "          [-3.89977336e-01, -4.44879293e-01, -5.07624388e-01],\n",
              "          [-4.49340165e-01, -5.04242122e-01, -5.66987216e-01]],\n",
              " \n",
              "         [[-2.64187932e-01, -3.33333313e-01, -3.73270452e-01],\n",
              "          [-2.71309674e-01, -3.40455055e-01, -3.80392134e-01],\n",
              "          [-2.70588219e-01, -3.39733660e-01, -3.79670739e-01],\n",
              "          ...,\n",
              "          [-3.79946470e-01, -4.51256156e-01, -5.21844387e-01],\n",
              "          [-3.88840556e-01, -4.60871637e-01, -5.15052140e-01],\n",
              "          [-4.08639610e-01, -4.80670631e-01, -5.34851193e-01]],\n",
              " \n",
              "         [[-2.78431356e-01, -3.33333313e-01, -3.80392134e-01],\n",
              "          [-2.74544418e-01, -3.29446375e-01, -3.76505196e-01],\n",
              "          [-2.70588219e-01, -3.25490177e-01, -3.72548997e-01],\n",
              "          ...,\n",
              "          [-3.81598949e-01, -4.76438046e-01, -5.41557074e-01],\n",
              "          [-3.70140374e-01, -4.67353284e-01, -5.14412165e-01],\n",
              "          [-3.37213337e-01, -4.34426308e-01, -4.81485128e-01]]],\n",
              " \n",
              " \n",
              "        [[[-2.70644605e-01, -2.43737578e-01, -3.05044532e-01],\n",
              "          [-2.57177830e-01, -2.47798800e-01, -3.17405403e-01],\n",
              "          [-2.48050809e-01, -2.63671517e-01, -3.34325314e-01],\n",
              "          ...,\n",
              "          [-3.72219563e-01, -3.64482939e-01, -4.32812035e-01],\n",
              "          [-3.62429976e-01, -3.74077380e-01, -4.58219588e-01],\n",
              "          [-3.44759345e-01, -3.71722698e-01, -4.84222233e-01]],\n",
              " \n",
              "         [[-2.39773452e-01, -2.52385139e-01, -3.30035090e-01],\n",
              "          [-2.43157983e-01, -2.39047289e-01, -3.05910707e-01],\n",
              "          [-2.55706787e-01, -2.41253376e-01, -3.00211608e-01],\n",
              "          ...,\n",
              "          [-3.62151861e-01, -3.51760805e-01, -4.53864455e-01],\n",
              "          [-3.68668914e-01, -3.68648171e-01, -4.13937688e-01],\n",
              "          [-3.79382908e-01, -3.66974890e-01, -4.63468611e-01]],\n",
              " \n",
              "         [[-2.55196929e-01, -2.34941602e-01, -2.80347824e-01],\n",
              "          [-2.69420981e-01, -2.51675546e-01, -3.30326378e-01],\n",
              "          [-2.46791422e-01, -2.30380893e-01, -3.32703769e-01],\n",
              "          ...,\n",
              "          [-3.67307901e-01, -3.51621628e-01, -4.56837296e-01],\n",
              "          [-3.59472752e-01, -3.56305838e-01, -4.51509774e-01],\n",
              "          [-3.47773671e-01, -3.68453741e-01, -4.59428430e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-9.94084835e-01, -9.94084835e-01, -1.00000000e+00],\n",
              "          [-1.00000000e+00, -9.99139309e-01, -1.00000000e+00],\n",
              "          [-9.73144293e-01, -9.74941134e-01, -9.90627408e-01],\n",
              "          ...,\n",
              "          [-8.26016545e-01, -8.75676870e-01, -9.74555254e-01],\n",
              "          [-8.03143740e-01, -8.93948078e-01, -9.71236646e-01],\n",
              "          [-7.90378094e-01, -8.85093331e-01, -9.59537506e-01]],\n",
              " \n",
              "         [[-9.95238125e-01, -9.93867815e-01, -9.99442339e-01],\n",
              "          [-9.97731388e-01, -9.84699905e-01, -9.99404371e-01],\n",
              "          [-9.85305190e-01, -9.86582339e-01, -1.00000000e+00],\n",
              "          ...,\n",
              "          [-8.03407490e-01, -9.01540518e-01, -9.80493665e-01],\n",
              "          [-7.98861504e-01, -8.87430131e-01, -9.77701843e-01],\n",
              "          [-7.83226013e-01, -8.93100321e-01, -9.88603532e-01]],\n",
              " \n",
              "         [[-9.83699679e-01, -9.95530009e-01, -9.84530807e-01],\n",
              "          [-9.97940838e-01, -9.86372888e-01, -9.92156863e-01],\n",
              "          [-9.89587843e-01, -9.77495253e-01, -9.85590816e-01],\n",
              "          ...,\n",
              "          [-7.40830004e-01, -8.80380988e-01, -9.90660429e-01],\n",
              "          [-7.73016453e-01, -8.68620574e-01, -9.83284295e-01],\n",
              "          [-7.92698681e-01, -8.85597467e-01, -9.76743996e-01]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[-6.55968547e-01, -5.22635221e-01, -5.71148276e-01],\n",
              "          [-6.27849758e-01, -5.22828281e-01, -5.56693316e-01],\n",
              "          [-6.17197156e-01, -5.41142344e-01, -5.82461834e-01],\n",
              "          ...,\n",
              "          [-5.13802767e-01, -4.32356834e-01, -3.92442524e-01],\n",
              "          [-5.85831404e-01, -5.23173571e-01, -5.05271673e-01],\n",
              "          [-5.64868033e-01, -4.87300932e-01, -3.90065968e-01]],\n",
              " \n",
              "         [[-6.47947073e-01, -5.14613688e-01, -5.72776675e-01],\n",
              "          [-6.42886400e-01, -5.19363046e-01, -5.46826005e-01],\n",
              "          [-6.31055236e-01, -5.43613434e-01, -5.79697013e-01],\n",
              "          ...,\n",
              "          [-4.98944819e-01, -3.96619141e-01, -4.86143470e-01],\n",
              "          [-6.59619153e-01, -5.86891413e-01, -6.31941020e-01],\n",
              "          [-6.27614796e-01, -5.42639494e-01, -5.52883983e-01]],\n",
              " \n",
              "         [[-6.20072126e-01, -5.09166479e-01, -5.70809841e-01],\n",
              "          [-6.35357141e-01, -5.10483384e-01, -5.34012794e-01],\n",
              "          [-6.22453809e-01, -5.27811527e-01, -5.58607101e-01],\n",
              "          ...,\n",
              "          [-6.10545099e-01, -5.30697107e-01, -4.70193863e-01],\n",
              "          [-6.58915639e-01, -5.87290049e-01, -6.05613887e-01],\n",
              "          [-6.66766524e-01, -5.75865149e-01, -6.11531496e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 4.92934465e-01, -5.56886196e-03, -2.83496320e-01],\n",
              "          [ 5.40776253e-01,  2.48005390e-02, -2.46707797e-01],\n",
              "          [ 5.36113501e-01,  3.26182842e-02, -2.40632474e-01],\n",
              "          ...,\n",
              "          [ 1.72598481e-01, -1.46265388e-01, -2.92884529e-01],\n",
              "          [ 2.95397758e-01, -1.99655294e-02, -1.45420849e-01],\n",
              "          [ 9.24992561e-02, -9.96184945e-02, -2.49485195e-01]],\n",
              " \n",
              "         [[ 5.76099157e-01,  7.75090456e-02, -2.01473176e-01],\n",
              "          [ 5.39602637e-01,  2.19554901e-02, -2.52554297e-01],\n",
              "          [ 5.38692236e-01,  3.52206230e-02, -2.38533735e-01],\n",
              "          ...,\n",
              "          [ 4.49989080e-01,  2.44427919e-02, -1.59133017e-01],\n",
              "          [ 3.41550112e-01, -5.61815500e-03, -1.67021394e-01],\n",
              "          [-4.86338139e-03, -1.86640620e-01, -3.28704715e-01]],\n",
              " \n",
              "         [[ 5.63917279e-01,  6.53272867e-02, -2.13654935e-01],\n",
              "          [ 5.58798671e-01,  4.11515236e-02, -2.33358264e-01],\n",
              "          [ 5.38078308e-01,  3.61174345e-02, -2.38392293e-01],\n",
              "          ...,\n",
              "          [ 3.73308778e-01, -6.99545145e-02, -2.59956479e-01],\n",
              "          [ 3.82488728e-01, -5.36149740e-02, -2.46526897e-01],\n",
              "          [ 3.62195253e-01,  8.54694843e-03, -1.63910627e-01]]],\n",
              " \n",
              " \n",
              "        [[[-4.00322318e-01, -5.54787695e-01, -5.67045212e-01],\n",
              "          [-4.45885479e-01, -5.59540629e-01, -5.67175150e-01],\n",
              "          [-4.64470685e-01, -5.67598760e-01, -6.09527707e-01],\n",
              "          ...,\n",
              "          [-8.08954239e-03, -3.32345665e-01, -7.24243283e-01],\n",
              "          [-3.65572572e-02, -3.64497244e-01, -7.33404994e-01],\n",
              "          [-9.68338847e-02, -4.06970978e-01, -8.05230260e-01]],\n",
              " \n",
              "         [[-4.51874554e-01, -5.71685672e-01, -6.09357357e-01],\n",
              "          [-4.50476170e-01, -5.53367019e-01, -5.97236395e-01],\n",
              "          [-4.49100912e-01, -5.36925554e-01, -5.83662271e-01],\n",
              "          ...,\n",
              "          [ 1.92366838e-02, -3.11895132e-01, -6.50489867e-01],\n",
              "          [-6.38825297e-02, -3.62297356e-01, -7.26390421e-01],\n",
              "          [-1.62963867e-01, -5.07929802e-01, -8.46062958e-01]],\n",
              " \n",
              "         [[-4.62672889e-01, -5.37077785e-01, -6.07666016e-01],\n",
              "          [-4.68302727e-01, -5.58184028e-01, -6.24955654e-01],\n",
              "          [-5.19557238e-01, -5.84427059e-01, -6.30035043e-01],\n",
              "          ...,\n",
              "          [-1.51433289e-01, -4.55340385e-01, -7.59835839e-01],\n",
              "          [-1.23473465e-01, -4.36931729e-01, -7.60139406e-01],\n",
              "          [-4.56448674e-01, -6.67429805e-01, -9.38992918e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-6.95517421e-01, -6.10910177e-01, -7.74002433e-01],\n",
              "          [-8.09245944e-01, -7.12246418e-01, -8.95229816e-01],\n",
              "          [-8.41223478e-01, -7.87486613e-01, -9.09265995e-01],\n",
              "          ...,\n",
              "          [-1.54131830e-01, -1.17907882e-01,  4.67816591e-02],\n",
              "          [-1.96947753e-01, -1.84457183e-01, -3.44022512e-02],\n",
              "          [-1.72181547e-01, -1.74241126e-01, -3.73935103e-02]],\n",
              " \n",
              "         [[-7.83573508e-01, -6.74251199e-01, -8.32320452e-01],\n",
              "          [-7.71187901e-01, -6.81827664e-01, -8.21133196e-01],\n",
              "          [-8.82034421e-01, -8.14958692e-01, -9.36443985e-01],\n",
              "          ...,\n",
              "          [-3.59623432e-01, -2.31968999e-01, -2.39337087e-02],\n",
              "          [-3.63078117e-01, -2.39552379e-01, -5.40866256e-02],\n",
              "          [-3.57786894e-01, -2.57198393e-01, -1.52735531e-01]],\n",
              " \n",
              "         [[-8.21897924e-01, -7.09618807e-01, -8.52560282e-01],\n",
              "          [-8.47848177e-01, -7.51147985e-01, -8.97037745e-01],\n",
              "          [-9.17906165e-01, -8.15531254e-01, -9.85260487e-01],\n",
              "          ...,\n",
              "          [-3.10974300e-01, -1.61140919e-01,  7.41254091e-02],\n",
              "          [-3.27699125e-01, -1.95702374e-01, -9.26673412e-04],\n",
              "          [-3.80967140e-01, -3.09505165e-01, -1.35315895e-01]]],\n",
              " \n",
              " \n",
              "        [[[-6.45756841e-01, -5.94384134e-01, -6.07126534e-01],\n",
              "          [-6.42140508e-01, -5.97091913e-01, -5.98317623e-01],\n",
              "          [-6.45258367e-01, -5.81512928e-01, -6.31925702e-01],\n",
              "          ...,\n",
              "          [-6.36139154e-01, -5.77818573e-01, -6.31774127e-01],\n",
              "          [-6.24811411e-01, -5.72785735e-01, -6.21592879e-01],\n",
              "          [-6.27432346e-01, -5.88216662e-01, -6.33281946e-01]],\n",
              " \n",
              "         [[-6.66091919e-01, -5.92075706e-01, -6.21792138e-01],\n",
              "          [-6.10774040e-01, -5.85463405e-01, -5.93306541e-01],\n",
              "          [-6.03724837e-01, -5.72024405e-01, -6.02487326e-01],\n",
              "          ...,\n",
              "          [-6.31809592e-01, -5.86333990e-01, -6.31417155e-01],\n",
              "          [-6.28332376e-01, -6.02940559e-01, -6.16764426e-01],\n",
              "          [-6.35599375e-01, -5.96383691e-01, -6.27756238e-01]],\n",
              " \n",
              "         [[-6.31591916e-01, -5.98774314e-01, -6.25138998e-01],\n",
              "          [-6.20699584e-01, -5.94837070e-01, -6.02763534e-01],\n",
              "          [-6.24185205e-01, -5.60989261e-01, -5.94814479e-01],\n",
              "          ...,\n",
              "          [-6.18664742e-01, -5.74905276e-01, -6.15978241e-01],\n",
              "          [-6.23440266e-01, -5.92802167e-01, -6.36267304e-01],\n",
              "          [-6.29024625e-01, -5.89808941e-01, -6.36133313e-01]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-8.02394927e-01, -7.64327407e-01, -7.79072523e-01],\n",
              "          [-7.94955015e-01, -7.79274762e-01, -7.77149796e-01],\n",
              "          [-8.40874612e-01, -8.40417206e-01, -8.28115761e-01],\n",
              "          ...,\n",
              "          [-4.04834807e-01, -2.91079402e-01, -2.38123298e-01],\n",
              "          [-4.18772995e-01, -2.91240871e-01, -2.36338913e-01],\n",
              "          [-4.29552317e-01, -3.01937163e-01, -2.47035205e-01]],\n",
              " \n",
              "         [[-8.89353931e-01, -8.66163909e-01, -8.63567889e-01],\n",
              "          [-8.96834791e-01, -8.79625738e-01, -8.68139446e-01],\n",
              "          [-9.12540078e-01, -8.90760899e-01, -8.92201483e-01],\n",
              "          ...,\n",
              "          [-4.33587849e-01, -3.36321712e-01, -2.94912815e-01],\n",
              "          [-4.42848623e-01, -3.34906816e-01, -2.84857512e-01],\n",
              "          [-4.61585879e-01, -3.53644133e-01, -2.97151446e-01]],\n",
              " \n",
              "         [[-8.74674916e-01, -8.15052629e-01, -8.23510468e-01],\n",
              "          [-8.29428136e-01, -7.69805908e-01, -7.87485003e-01],\n",
              "          [-8.55498254e-01, -7.94519007e-01, -8.28431726e-01],\n",
              "          ...,\n",
              "          [-4.96633947e-01, -4.18755710e-01, -3.93878043e-01],\n",
              "          [-4.80489790e-01, -4.05913949e-01, -3.65701377e-01],\n",
              "          [-4.92097080e-01, -4.17521238e-01, -3.62292469e-01]]]],\n",
              "       dtype=float32)>, <tf.Tensor: shape=(16,), dtype=string, numpy=\n",
              " array([b'/content/train2014/COCO_train2014_000000000077.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000349.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000384.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000514.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000634.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000723.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000797.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000839.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000897.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000927.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000000943.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000001224.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000001366.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000001375.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000001424.jpg',\n",
              "        b'/content/train2014/COCO_train2014_000000001455.jpg'],\n",
              "       dtype=object)>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nudiYe7wQccZ"
      },
      "source": [
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQMCJED0bp1X"
      },
      "source": [
        "### Caption preprocessing\n",
        "\n",
        "* We are going to tokenize captions for images using keras Tokenizer class. We are going to use the vocabulary of `5_000` words to save memory.\n",
        "\n",
        "* We will create a word-index-mappings.\n",
        "* We are then going to pad all the sequences to have the same length as the longest one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB2B0ADjQcaV"
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIiI3wCTQcWV"
      },
      "source": [
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
        "tokenizer.fit_on_texts(train_captions)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJww21qwQcUc"
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFRYsT9QcQU"
      },
      "source": [
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frr7BvsnQcOd"
      },
      "source": [
        "# Pad each vector to the max_length of the captions\n",
        "# If we do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1X3U4cJQcKr",
        "outputId": "bb2d21aa-18d3-44e6-d4b1-a677f04e8c9f"
      },
      "source": [
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)\n",
        "max_length"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtHNj-MQy3Ht",
        "outputId": "56c3f95c-ff70-415e-f3b1-ea41cbc894c2"
      },
      "source": [
        "cap_vector.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30018, 45)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNfnH5Xmc2ZX"
      },
      "source": [
        "### Splitting the dataset into train and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yBhN6mezwaH",
        "outputId": "56fbf497-828b-4f93-be70-44158533f2df"
      },
      "source": [
        "len(img_name_vector)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30018"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG0uifaeQcIY"
      },
      "source": [
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(img_name_vector, cap_vector):\n",
        "  img_to_cap_vector[img].append(cap)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orEoduuQVBI"
      },
      "source": [
        "# Create training and validation sets using an 80-20 split randomly.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lhw50HCdPS7"
      },
      "source": [
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
        "\n",
        "img_name_train = []\n",
        "cap_train = []\n",
        "for image_name_train in img_name_train_keys:\n",
        "  capt_len = len(img_to_cap_vector[image_name_train])\n",
        "  img_name_train.extend([image_name_train] * capt_len)\n",
        "  cap_train.extend(img_to_cap_vector[image_name_train])\n",
        "\n",
        "img_name_val = []\n",
        "cap_val = []\n",
        "for imgv in img_name_val_keys:\n",
        "  capv_len = len(img_to_cap_vector[imgv])\n",
        "  img_name_val.extend([imgv] * capv_len)\n",
        "  cap_val.extend(img_to_cap_vector[imgv])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT2QRqsUdq9b"
      },
      "source": [
        "### Counting examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpDisnZedPK6",
        "outputId": "f3a015cb-5188-49be-ddcf-188914dcfcc7"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24013, 24013, 6005, 6005)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3mmTns6dxDk"
      },
      "source": [
        "### Creating a `tf.dataset` for trainning\n",
        "\n",
        "\n",
        "Our image and captions are now ready, let's create a `tf.data` to use for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfP5PnQCdPHz"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1 # unknown\n",
        "\n",
        "num_steps = len(img_name_train)\n",
        "\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X66xkzu7dPEg"
      },
      "source": [
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qJrfwLg2PUp"
      },
      "source": [
        "Creating the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70HSgVtf2cme"
      },
      "source": [
        "assert len(img_name_train) == len(cap_train)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv64c7nfdPBJ"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (img_name_train, cap_train)\n",
        "    )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzjRbTr8dO9S"
      },
      "source": [
        "dataset = dataset.map(lambda x, y: tf.numpy_function(\n",
        "    map_func, [x, y], [tf.float32, tf.int32]\n",
        "), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle the dataset\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE\n",
        "                          ).batch(BATCH_SIZE\n",
        "                                  ).prefetch(\n",
        "                                      buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1yQ8UuG3Oew"
      },
      "source": [
        "#### Model.\n",
        "\n",
        "The decoder that we will be using is adentical to the one for Nueral Machine Translation with Attention mechanism.\n",
        "\n",
        "The model achitecture is insired by [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf) paper.\n",
        "\n",
        "* We will extract the features from the lower convulutional layer of InceptionV3 giving us a vector of shape ``(8, 8, 2048)``\n",
        "\n",
        "* We will squeeze this to have the shape of `(64 * 2048)`\n",
        "\n",
        "* This vector is then passed down to the CNN Encoder which consist of a single Fully connected layer.\n",
        "* The RNN  attends over the image to predict the next word.\n",
        "\n",
        "\n",
        "### BahdanauAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2BxjOTV4o1v"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ7saJAq4ujH"
      },
      "source": [
        "### CNN Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awo-Pfe34oyx"
      },
      "source": [
        "class CNN_Encoder(keras.Model):\n",
        "  # Since you have already extracted the features and dumped it\n",
        "  # This encoder passes those features through a Fully connected layer\n",
        "  def __init__(self, embedding_dim):\n",
        "      super(CNN_Encoder, self).__init__()\n",
        "      # shape after fc == (batch_size, 64, embedding_dim)\n",
        "      self.fc = keras.layers.Dense(embedding_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "      x = self.fc(x)\n",
        "      x = tf.nn.relu(x)\n",
        "      return x"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jz3yDnZ5bHG"
      },
      "source": [
        "### RNN Decoder\n",
        "\n",
        "We are going to use the GRU as our RNN layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LIM-3Jl4ovA"
      },
      "source": [
        "class RNN_Decoder(keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = keras.layers.Dense(self.units)\n",
        "    self.fc2 = keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEr8g_Zz6CHB"
      },
      "source": [
        "### Model instances\n",
        "\n",
        "1. Encoder (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-WVtmOw4osv"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jNWO6wN6Tq6"
      },
      "source": [
        "2. Decoder (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBRDSEbT4opX"
      },
      "source": [
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytIGAuDp6eYt"
      },
      "source": [
        "### Optimizer and Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu5ucMQ54onV"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "criterion = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJJAHE4V6m8R"
      },
      "source": [
        "Custom loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yU5-JIL6mQE"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBx1KB-6wOI"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF7xxYne6mMV"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21LR9key640b"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiQj2Vtj6_-5"
      },
      "source": [
        "### Training\n",
        "\n",
        "* We need to extract the stored featuers in a respective `.npy` files and the pass those features to the encoder.\n",
        "\n",
        "* The encoder output hidden state(initialized to 0) and the decoder input (which is the start token) is passed to the decoder.\n",
        "\n",
        "* The decoder returns the predictions and the decoder hidden state.\n",
        "\n",
        "* Use the teacher forcing to decide the next input to the decoder.\n",
        "\n",
        "* Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
        "\n",
        "* The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRy0TTtz6mJ9"
      },
      "source": [
        "loss_plot = []"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQdkCz0I6l1o"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    features = encoder(img_tensor)\n",
        "    for i in range(1, target.shape[1]):\n",
        "      # passing the features through the decoder\n",
        "      predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "      loss += criterion(target[:, i], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NagKzmqJ6lxl"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        # if batch % 100 == 0:\n",
        "        #     average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "\n",
        "        #     print(average_batch_loss)\n",
        "        #     print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va2vdZnC-6ST"
      },
      "source": [
        "### Visualizing training loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "D5EIKVKX6lvv",
        "outputId": "07b8921f-d795-481b-b2f6-9106e40df751"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3neCxTlALPX"
      },
      "source": [
        "### Model evaluation\n",
        "The eveluation function is simmilar to the train step, it's only that we are not going to use the teacher forcing and the input of the decoder at each time step is the previous predictions along with the hidden start and the encoder output.\n",
        "\n",
        "* stop predicting when we reach the `end` token or if the we reach the ,aximum length.\n",
        "\n",
        "* store the attention weights for every time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgLeGy1e_VEF"
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YF6ftYW_U84"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(np.ceil(len_result/2), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv9N8ogrCYdA"
      },
      "source": [
        "# captions on the validation set\n",
        "rid = np.random.randint(0, len(img_name_val))\n",
        "image = img_name_val[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i]\n",
        "                        for i in cap_val[rid] if i not in [0]])\n",
        "result, attention_plot = evaluate(image)\n",
        "\n",
        "print('Real Caption:', real_caption)\n",
        "print('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image, result, attention_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtEGIj6kRYT0"
      },
      "source": [
        "### Refs\n",
        "\n",
        "1. [tensorflow tutorial](https://www.tensorflow.org/tutorials/text/image_captioning)\n",
        "\n",
        "2. [Neural Image Caption Generation with Visual Attention PAPER](https://arxiv.org/abs/1502.03044)"
      ]
    }
  ]
}